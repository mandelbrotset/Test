diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 483b732..dbfebd9 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -257,7 +257,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
     }
 
     /**
-     * Retruns <code>true</code> iff the given class is a registered for an exception to be read.
+     * Returns <code>true</code> iff the given class is a registered for an exception to be read.
      */
     public static boolean isRegistered(Class<? extends Throwable> exception) {
         return CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE.containsKey(exception);
@@ -372,7 +372,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
     }
 
     /**
-     * Returns the root cause of this exception or mupltiple if different shards caused different exceptions
+     * Returns the root cause of this exception or multiple if different shards caused different exceptions
      */
     public ElasticsearchException[] guessRootCauses() {
         final Throwable cause = getCause();
@@ -383,7 +383,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
     }
 
     /**
-     * Returns the root cause of this exception or mupltiple if different shards caused different exceptions.
+     * Returns the root cause of this exception or multiple if different shards caused different exceptions.
      * If the given exception is not an instance of {@link org.elasticsearch.ElasticsearchException} an empty array
      * is returned.
      */
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java
index 42d6979..d73b3d4 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java
@@ -40,7 +40,7 @@ public class NodesInfoRequestBuilder extends NodesOperationRequestBuilder<NodesI
     }
 
     /**
-     * Sets to reutrn all the data.
+     * Sets to return all the data.
      */
     public NodesInfoRequestBuilder all() {
         request.all();
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java
index 3fc2f4b..0c883cc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java
@@ -105,7 +105,7 @@ public class TransportClusterStatsAction extends TransportNodesAction<ClusterSta
             for (IndexShard indexShard : indexService) {
                 if (indexShard.routingEntry() != null && indexShard.routingEntry().active()) {
                     // only report on fully started shards
-                    shardsStats.add(new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indexShard, SHARD_STATS_FLAGS), indexShard.commitStats()));
+                    shardsStats.add(new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indicesService.getIndicesQueryCache(), indexShard, SHARD_STATS_FLAGS), indexShard.commitStats()));
                 }
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java
index d633c3a..16002d3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java
@@ -49,17 +49,14 @@ import java.util.List;
 public class TransportClearIndicesCacheAction extends TransportBroadcastByNodeAction<ClearIndicesCacheRequest, ClearIndicesCacheResponse, TransportBroadcastByNodeAction.EmptyResult> {
 
     private final IndicesService indicesService;
-    private final IndicesRequestCache indicesRequestCache;
 
     @Inject
     public TransportClearIndicesCacheAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-                                            TransportService transportService, IndicesService indicesService,
-                                            IndicesRequestCache indicesQueryCache, ActionFilters actionFilters,
+                                            TransportService transportService, IndicesService indicesService, ActionFilters actionFilters,
                                             IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ClearIndicesCacheAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver,
                 ClearIndicesCacheRequest::new, ThreadPool.Names.MANAGEMENT);
         this.indicesService = indicesService;
-        this.indicesRequestCache = indicesQueryCache;
     }
 
     @Override
@@ -101,7 +98,7 @@ public class TransportClearIndicesCacheAction extends TransportBroadcastByNodeAc
             }
             if (request.requestCache()) {
                 clearedAtLeastOne = true;
-                indicesRequestCache.clear(shard);
+                indicesService.getIndicesRequestCache().clear(shard);
             }
             if (request.recycler()) {
                 logger.debug("Clear CacheRecycler on index [{}]", service.index());
@@ -117,7 +114,7 @@ public class TransportClearIndicesCacheAction extends TransportBroadcastByNodeAc
                 } else {
                     service.cache().clear("api");
                     service.fieldData().clear();
-                    indicesRequestCache.clear(shard);
+                    indicesService.getIndicesRequestCache().clear(shard);
                 }
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
index 85644e8..47fb8d8 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
@@ -26,6 +26,7 @@ import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.cache.query.QueryCacheStats;
 import org.elasticsearch.index.cache.request.RequestCacheStats;
 import org.elasticsearch.index.engine.SegmentsStats;
@@ -40,10 +41,13 @@ import org.elasticsearch.index.refresh.RefreshStats;
 import org.elasticsearch.index.search.stats.SearchStats;
 import org.elasticsearch.index.shard.DocsStats;
 import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.StoreStats;
 import org.elasticsearch.index.suggest.stats.SuggestStats;
 import org.elasticsearch.index.translog.TranslogStats;
 import org.elasticsearch.index.warmer.WarmerStats;
+import org.elasticsearch.indices.IndicesService;
+import org.elasticsearch.indices.cache.query.IndicesQueryCache;
 import org.elasticsearch.search.suggest.completion.CompletionStats;
 
 import java.io.IOException;
@@ -122,7 +126,8 @@ public class CommonStats implements Streamable, ToXContent {
     }
 
 
-    public CommonStats(IndexShard indexShard, CommonStatsFlags flags) {
+    public CommonStats(IndicesQueryCache indicesQueryCache, IndexShard indexShard, CommonStatsFlags flags) {
+
         CommonStatsFlags.Flag[] setFlags = flags.getFlags();
 
         for (CommonStatsFlags.Flag flag : setFlags) {
@@ -155,7 +160,7 @@ public class CommonStats implements Streamable, ToXContent {
                     warmer = indexShard.warmerStats();
                     break;
                 case QueryCache:
-                    queryCache = indexShard.queryCacheStats();
+                    queryCache = indicesQueryCache.getStats(indexShard.shardId());
                     break;
                 case FieldData:
                     fieldData = indexShard.fieldDataStats(flags.fieldDataFields());
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java
index d5de67d..4bed5f9 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java
@@ -162,6 +162,6 @@ public class TransportIndicesStatsAction extends TransportBroadcastByNodeAction<
             flags.set(CommonStatsFlags.Flag.Recovery);
         }
 
-        return new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indexShard, flags), indexShard.commitStats());
+        return new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indicesService.getIndicesQueryCache(), indexShard, flags), indexShard.commitStats());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index fe02a15..00f34f4 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -49,7 +49,6 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -77,20 +76,17 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportValidateQueryAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                        TransportService transportService, IndicesService indicesService,
+                                        ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                        BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ValidateQueryAction.NAME, threadPool, clusterService, transportService, actionFilters,
                 indexNameExpressionResolver, ValidateQueryRequest::new, ShardValidateQueryRequest::new, ThreadPool.Names.SEARCH);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -177,9 +173,11 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
         Engine.Searcher searcher = indexShard.acquireSearcher("validate_query");
 
         DefaultSearchContext searchContext = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()), null, searcher,
-                indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(),
-                parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()),
+                null, searcher, indexService, indexShard,
+                scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(searchContext);
         try {
             searchContext.parsedQuery(queryShardContext.toQuery(request.query()));
diff --git a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
index 5969e20..3ebef7c 100644
--- a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
@@ -44,7 +44,6 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -69,20 +68,17 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportExplainAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                  TransportService transportService, IndicesService indicesService,
+                                  ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                  BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ExplainAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver,
                 ExplainRequest::new, ThreadPool.Names.GET);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -115,10 +111,13 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
             return new ExplainResponse(shardId.getIndexName(), request.type(), request.id(), false);
         }
 
-        SearchContext context = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(new String[] { request.type() }, request.nowInMillis, request.filteringAlias()), null,
-                result.searcher(), indexService, indexShard, scriptService, pageCacheRecycler, bigArrays,
-                threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+        SearchContext context = new DefaultSearchContext(
+                0, new ShardSearchLocalRequest(new String[]{request.type()}, request.nowInMillis, request.filteringAlias()),
+                null, result.searcher(), indexService, indexShard,
+                scriptService, pageCacheRecycler,
+                bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(context);
 
         try {
diff --git a/core/src/main/java/org/elasticsearch/action/fieldstats/IndexConstraint.java b/core/src/main/java/org/elasticsearch/action/fieldstats/IndexConstraint.java
index 19e274e..681b7d7 100644
--- a/core/src/main/java/org/elasticsearch/action/fieldstats/IndexConstraint.java
+++ b/core/src/main/java/org/elasticsearch/action/fieldstats/IndexConstraint.java
@@ -73,7 +73,7 @@ public class IndexConstraint {
     }
 
     /**
-     * @return On what property of a field the contraint is going to be applied on (min or max value)
+     * @return On what property of a field the constraint is going to be applied on (min or max value)
      */
     public Property getProperty() {
         return property;
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index f21b805..336782d 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -372,7 +372,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
     }
 
     /**
-     * The source of the document to index, recopied to a new array if it is unsage.
+     * The source of the document to index, recopied to a new array if it is unsafe.
      */
     public BytesReference source() {
         return source;
diff --git a/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java b/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java
index 82d0f9b..765331f 100644
--- a/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java
@@ -164,7 +164,7 @@ public class PutIndexedScriptRequest extends ActionRequest<PutIndexedScriptReque
     }
 
     /**
-     * The source of the document to index, recopied to a new array if it is unsage.
+     * The source of the document to index, recopied to a new array if it is unsafe.
      */
     public BytesReference source() {
         return source;
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java b/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
index 342e4bd..253fa73 100644
--- a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
+++ b/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
@@ -69,7 +69,9 @@ final class WriteableIngestDocument implements Writeable<WriteableIngestDocument
         builder.startObject("doc");
         Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
         for (Map.Entry<IngestDocument.MetaData, String> metadata : metadataMap.entrySet()) {
-            builder.field(metadata.getKey().getFieldName(), metadata.getValue());
+            if (metadata.getValue() != null) {
+                builder.field(metadata.getKey().getFieldName(), metadata.getValue());
+            }
         }
         builder.field("_source", ingestDocument.getSourceAndMetadata());
         builder.startObject("_ingest");
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
index 472938c..e4b3a04 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
@@ -26,8 +26,7 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
 
@@ -153,26 +152,16 @@ public class PercolateRequestBuilder extends BroadcastOperationRequestBuilder<Pe
     }
 
     /**
-     * Delegates to
-     * {@link PercolateSourceBuilder#addAggregation(AggregatorBuilder)}
+     * Delegates to {@link PercolateSourceBuilder#addAggregation(AbstractAggregationBuilder)}
      */
-    public PercolateRequestBuilder addAggregation(AggregatorBuilder<?> aggregationBuilder) {
+    public PercolateRequestBuilder addAggregation(AbstractAggregationBuilder aggregationBuilder) {
         sourceBuilder().addAggregation(aggregationBuilder);
         return this;
     }
 
     /**
-     * Delegates to
-     * {@link PercolateSourceBuilder#addAggregation(PipelineAggregatorBuilder)}
-     */
-    public PercolateRequestBuilder addAggregation(PipelineAggregatorBuilder aggregationBuilder) {
-        sourceBuilder().addAggregation(aggregationBuilder);
-        return this;
-    }
-
-    /**
-     * Sets the percolate request definition directly on the request. This will
-     * overwrite any definitions set by any of the delegate methods.
+     * Sets the percolate request definition directly on the request.
+     * This will overwrite any definitions set by any of the delegate methods.
      */
     public PercolateRequestBuilder setSource(PercolateSourceBuilder source) {
         sourceBuilder = source;
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
index 57e9409..b080039 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
@@ -29,8 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.ScoreSortBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
@@ -52,8 +51,7 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
     private List<SortBuilder> sorts;
     private Boolean trackScores;
     private HighlightBuilder highlightBuilder;
-    private List<AggregatorBuilder<?>> aggregationFactorys;
-    private List<PipelineAggregatorBuilder> pipelineAggregationFactorys;
+    private List<AbstractAggregationBuilder> aggregations;
 
     /**
      * Sets the document to run the percolate queries against.
@@ -125,22 +123,11 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
     /**
      * Add an aggregation definition.
      */
-    public PercolateSourceBuilder addAggregation(AggregatorBuilder<?> aggregationBuilder) {
-        if (aggregationFactorys == null) {
-            aggregationFactorys = new ArrayList<>();
+    public PercolateSourceBuilder addAggregation(AbstractAggregationBuilder aggregationBuilder) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
         }
-        aggregationFactorys.add(aggregationBuilder);
-        return this;
-    }
-
-    /**
-     * Add an aggregation definition.
-     */
-    public PercolateSourceBuilder addAggregation(PipelineAggregatorBuilder aggregationBuilder) {
-        if (pipelineAggregationFactorys == null) {
-            pipelineAggregationFactorys = new ArrayList<>();
-        }
-        pipelineAggregationFactorys.add(aggregationBuilder);
+        aggregations.add(aggregationBuilder);
         return this;
     }
 
@@ -172,18 +159,11 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
         if (highlightBuilder != null) {
             highlightBuilder.toXContent(builder, params);
         }
-        if (aggregationFactorys != null || pipelineAggregationFactorys != null) {
+        if (aggregations != null) {
             builder.field("aggregations");
             builder.startObject();
-            if (aggregationFactorys != null) {
-                for (AggregatorBuilder<?> aggregation : aggregationFactorys) {
-                    aggregation.toXContent(builder, params);
-                }
-            }
-            if (pipelineAggregationFactorys != null) {
-                for (PipelineAggregatorBuilder aggregation : pipelineAggregationFactorys) {
-                    aggregation.toXContent(builder, params);
-                }
+            for (AbstractAggregationBuilder aggregation : aggregations) {
+                aggregation.toXContent(builder, params);
             }
             builder.endObject();
         }
diff --git a/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java b/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
index 3540daa..2b3cd80 100644
--- a/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
@@ -55,7 +55,7 @@ public class ClearScrollResponse extends ActionResponse implements StatusToXCont
     }
 
     /**
-     * @return The number of seach contexts that were freed. If this is <code>0</code> the assumption can be made,
+     * @return The number of search contexts that were freed. If this is <code>0</code> the assumption can be made,
      * that the scroll id specified in the request did not exist. (never existed, was expired, or completely consumed)
      */
     public int getNumFreed() {
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 2577e6d..ca4a8e5 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -28,8 +28,7 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -224,7 +223,7 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Sets the boost a specific index will receive when the query is executeed against it.
+     * Sets the boost a specific index will receive when the query is executed against it.
      *
      * @param index      The index to apply the boost against
      * @param indexBoost The boost to apply to the index
@@ -372,17 +371,9 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Adds an aggregation to the search operation.
+     * Adds an get to the search operation.
      */
-    public SearchRequestBuilder addAggregation(AggregatorBuilder<?> aggregation) {
-        sourceBuilder().aggregation(aggregation);
-        return this;
-    }
-
-    /**
-     * Adds an aggregation to the search operation.
-     */
-    public SearchRequestBuilder addAggregation(PipelineAggregatorBuilder aggregation) {
+    public SearchRequestBuilder addAggregation(AbstractAggregationBuilder aggregation) {
         sourceBuilder().aggregation(aggregation);
         return this;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index b425768..1d22207 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -415,7 +415,11 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     public static class RetryOnPrimaryException extends ElasticsearchException {
         public RetryOnPrimaryException(ShardId shardId, String msg) {
-            super(msg);
+            this(shardId, msg, null);
+        }
+
+        public RetryOnPrimaryException(ShardId shardId, String msg, Throwable cause) {
+            super(msg, cause);
             setShard(shardId);
         }
 
@@ -801,6 +805,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
      * relocating copies
      */
     final class ReplicationPhase extends AbstractRunnable {
+
         private final ReplicationTask task;
         private final ReplicaRequest replicaRequest;
         private final Response finalResponse;
@@ -982,9 +987,17 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                                         }
 
                                         @Override
-                                        public void onFailure(Throwable t) {
-                                            // TODO: handle catastrophic non-channel failures
-                                            onReplicaFailure(nodeId, exp);
+                                        public void onFailure(Throwable shardFailedError) {
+                                            if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) {
+                                                ShardRouting primaryShard = indexShardReference.routingEntry();
+                                                String message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp);
+                                                // we are no longer the primary, fail ourselves and start over
+                                                indexShardReference.failShard(message, shardFailedError);
+                                                forceFinishAsFailed(new RetryOnPrimaryException(shardId, message, shardFailedError));
+                                            } else {
+                                                assert false : shardFailedError;
+                                                onReplicaFailure(nodeId, exp);
+                                            }
                                         }
                                     }
                                 );
@@ -1070,7 +1083,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     interface IndexShardReference extends Releasable {
         boolean isRelocated();
-
+        void failShard(String reason, @Nullable Throwable e);
         ShardRouting routingEntry();
     }
 
@@ -1099,6 +1112,11 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
 
         @Override
+        public void failShard(String reason, @Nullable Throwable e) {
+            indexShard.failShard(reason, e);
+        }
+
+        @Override
         public ShardRouting routingEntry() {
             return indexShard.routingEntry();
         }
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java
index 0b4152f..9c745ad 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java
@@ -486,7 +486,7 @@ public final class TermVectorsFields extends Fields {
 
     // read a vInt. this is used if the integer might be negative. In this case,
     // the writer writes a 0 for -1 or value +1 and accordingly we have to
-    // substract 1 again
+    // subtract 1 again
     // adds one to mock not existing term freq
     int readPotentiallyNegativeVInt(StreamInput stream) throws IOException {
         return stream.readVInt() - 1;
@@ -494,7 +494,7 @@ public final class TermVectorsFields extends Fields {
 
     // read a vLong. this is used if the integer might be negative. In this
     // case, the writer writes a 0 for -1 or value +1 and accordingly we have to
-    // substract 1 again
+    // subtract 1 again
     // adds one to mock not existing term freq
     long readPotentiallyNegativeVLong(StreamInput stream) throws IOException {
         return stream.readVLong() - 1;
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index a1468f4..70e9ca8 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -254,10 +254,6 @@ final class Bootstrap {
         INSTANCE = new Bootstrap();
 
         boolean foreground = !"false".equals(System.getProperty("es.foreground", System.getProperty("es-foreground")));
-        // handle the wrapper system property, if its a service, don't run as a service
-        if (System.getProperty("wrapper.service", "XXX").equalsIgnoreCase("true")) {
-            foreground = false;
-        }
 
         Environment environment = initialSettings(foreground);
         Settings settings = environment.settings();
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java b/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
index 78dbc00..64b685c 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
@@ -108,7 +108,7 @@ class JNANatives {
         if (value == JNACLibrary.RLIM_INFINITY) {
             return "unlimited";
         } else {
-            // TODO, on java 8 use Long.toUnsignedString, since thats what it is.
+            // TODO, on java 8 use Long.toUnsignedString, since that's what it is.
             return Long.toString(value);
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java b/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java
index 00f60a7..4325c5b 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java
@@ -104,7 +104,7 @@ final class Seccomp {
         int prctl(int option, NativeLong arg2, NativeLong arg3, NativeLong arg4, NativeLong arg5);
         /**
          * used to call seccomp(2), its too new...
-         * this is the only way, DONT use it on some other architecture unless you know wtf you are doing
+         * this is the only way, DON'T use it on some other architecture unless you know wtf you are doing
          */
         NativeLong syscall(NativeLong number, Object... args);
     };
diff --git a/core/src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java b/core/src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java
index 7016718..a293428 100644
--- a/core/src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java
+++ b/core/src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java
@@ -93,7 +93,7 @@ public class PageCacheRecycler extends AbstractComponent implements Releasable {
         // because they would make obtain/release too costly: we really need constant-time
         // operations.
         // Ultimately a better solution would be to only store one kind of data and have the
-        // ability to intepret it either as a source of bytes, doubles, longs, etc. eg. thanks
+        // ability to interpret it either as a source of bytes, doubles, longs, etc. eg. thanks
         // to direct ByteBuffers or sun.misc.Unsafe on a byte[] but this would have other issues
         // that would need to be addressed such as garbage collection of native memory or safety
         // of Unsafe writes.
diff --git a/core/src/main/java/org/elasticsearch/client/Requests.java b/core/src/main/java/org/elasticsearch/client/Requests.java
index 2f3e3f8..c3dd77a 100644
--- a/core/src/main/java/org/elasticsearch/client/Requests.java
+++ b/core/src/main/java/org/elasticsearch/client/Requests.java
@@ -107,7 +107,7 @@ public class Requests {
     }
 
     /**
-     * Creats a new bulk request.
+     * Creates a new bulk request.
      */
     public static BulkRequest bulkRequest() {
         return new BulkRequest();
diff --git a/core/src/main/java/org/elasticsearch/cluster/LocalNodeMasterListener.java b/core/src/main/java/org/elasticsearch/cluster/LocalNodeMasterListener.java
index ebea8b8..9f7fb00 100644
--- a/core/src/main/java/org/elasticsearch/cluster/LocalNodeMasterListener.java
+++ b/core/src/main/java/org/elasticsearch/cluster/LocalNodeMasterListener.java
@@ -38,9 +38,9 @@ public interface LocalNodeMasterListener {
      * The name of the executor that the implementation of the callbacks of this lister should be executed on. The thread
      * that is responsible for managing instances of this lister is the same thread handling the cluster state events. If
      * the work done is the callbacks above is inexpensive, this value may be {@link org.elasticsearch.threadpool.ThreadPool.Names#SAME SAME}
-     * (indicating that the callbaks will run on the same thread as the cluster state events are fired with). On the other hand,
+     * (indicating that the callbacks will run on the same thread as the cluster state events are fired with). On the other hand,
      * if the logic in the callbacks are heavier and take longer to process (or perhaps involve blocking due to IO operations),
-     * prefer to execute them on a separte more appropriate executor (eg. {@link org.elasticsearch.threadpool.ThreadPool.Names#GENERIC GENERIC}
+     * prefer to execute them on a separate more appropriate executor (eg. {@link org.elasticsearch.threadpool.ThreadPool.Names#GENERIC GENERIC}
      * or {@link org.elasticsearch.threadpool.ThreadPool.Names#MANAGEMENT MANAGEMENT}).
      *
      * @return The name of the executor that will run the callbacks of this listener.
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
index 9fb526b..8ba7897 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
@@ -959,7 +959,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
 
         public MetaData build() {
             // TODO: We should move these datastructures to IndexNameExpressionResolver, this will give the following benefits:
-            // 1) The datastructures will only be rebuilded when needed. Now during serailizing we rebuild these datastructures
+            // 1) The datastructures will only be rebuilded when needed. Now during serializing we rebuild these datastructures
             //    while these datastructures aren't even used.
             // 2) The aliasAndIndexLookup can be updated instead of rebuilding it all the time.
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java b/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java
index 528ed8b..a5e96e6 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java
@@ -97,6 +97,13 @@ public class AllocationId implements ToXContent {
     }
 
     /**
+     * Creates a new allocation id for initializing allocation based on an existing id.
+     */
+    public static AllocationId newInitializing(String existingAllocationId) {
+        return new AllocationId(existingAllocationId, null);
+    }
+
+    /**
      * Creates a new allocation id for the target initializing shard that is the result
      * of a relocation.
      */
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index 02bcea4..c1a5f3f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -26,6 +26,7 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.index.Index;
@@ -420,11 +421,13 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
     /**
      * Moves a shard from unassigned to initialize state
+     *
+     * @param existingAllocationId allocation id to use. If null, a fresh allocation id is generated.
      */
-    public void initialize(ShardRouting shard, String nodeId, long expectedSize) {
+    public void initialize(ShardRouting shard, String nodeId, @Nullable String existingAllocationId, long expectedSize) {
         ensureMutable();
         assert shard.unassigned() : shard;
-        shard.initialize(nodeId, expectedSize);
+        shard.initialize(nodeId, existingAllocationId, expectedSize);
         node(nodeId).add(shard);
         inactiveShardCount++;
         if (shard.primary()) {
@@ -692,10 +695,12 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
             /**
              * Initializes the current unassigned shard and moves it from the unassigned list.
+             *
+             * @param existingAllocationId allocation id to use. If null, a fresh allocation id is generated.
              */
-            public void initialize(String nodeId, long expectedShardSize) {
+            public void initialize(String nodeId, @Nullable String existingAllocationId, long expectedShardSize) {
                 innerRemove();
-                nodes.initialize(new ShardRouting(current), nodeId, expectedShardSize);
+                nodes.initialize(new ShardRouting(current), nodeId, existingAllocationId, expectedShardSize);
             }
 
             /**
@@ -711,7 +716,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
             /**
              * Unsupported operation, just there for the interface. Use {@link #removeAndIgnore()} or
-             * {@link #initialize(String, long)}.
+             * {@link #initialize(String, String, long)}.
              */
             @Override
             public void remove() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
index 7535aa1..336b654 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
@@ -410,14 +410,20 @@ public final class ShardRouting implements Streamable, ToXContent {
 
     /**
      * Initializes an unassigned shard on a node.
+     *
+     * @param existingAllocationId allocation id to use. If null, a fresh allocation id is generated.
      */
-    void initialize(String nodeId, long expectedShardSize) {
+    void initialize(String nodeId, @Nullable String existingAllocationId, long expectedShardSize) {
         ensureNotFrozen();
         assert state == ShardRoutingState.UNASSIGNED : this;
         assert relocatingNodeId == null : this;
         state = ShardRoutingState.INITIALIZING;
         currentNodeId = nodeId;
-        allocationId = AllocationId.newInitializing();
+        if (existingAllocationId == null) {
+            allocationId = AllocationId.newInitializing();
+        } else {
+            allocationId = AllocationId.newInitializing(existingAllocationId);
+        }
         this.expectedShardSize = expectedShardSize;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
index 99b1974..8b10c4a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
@@ -611,7 +611,7 @@ public class AllocationService extends AbstractComponent {
         return routingNodes;
     }
 
-    /** ovrride this to control time based decisions during allocation */
+    /** override this to control time based decisions during allocation */
     protected long currentNanoTime() {
         return System.nanoTime();
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java
index 835556a..7232e15 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java
@@ -30,7 +30,7 @@ import java.util.List;
 
 /**
  * This {@link RoutingAllocation} keeps a shard which routing
- * allocation has faild
+ * allocation has failed.
  */
 public class FailedRerouteAllocation extends RoutingAllocation {
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
index 574f122..e12020c 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
@@ -571,7 +571,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
         }
 
         /**
-         * Allocates all given shards on the minimal eligable node for the shards index
+         * Allocates all given shards on the minimal eligible node for the shards index
          * with respect to the weight function. All given shards must be unassigned.
          */
         private boolean allocateUnassigned(RoutingNodes.UnassignedShards unassigned) {
@@ -611,7 +611,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
              * The arrays are sorted by primaries first and then by index and shard ID so a 2 indices with 2 replica and 1 shard would look like:
              * [(0,P,IDX1), (0,P,IDX2), (0,R,IDX1), (0,R,IDX1), (0,R,IDX2), (0,R,IDX2)]
              * if we allocate for instance (0, R, IDX1) we move the second replica to the secondary array and proceed with
-             * the next replica. If we could not find a node to allocate (0,R,IDX1) we move all it's replicas to ingoreUnassigned.
+             * the next replica. If we could not find a node to allocate (0,R,IDX1) we move all it's replicas to ignoreUnassigned.
              */
             ShardRouting[] primary = unassigned.drain();
             ShardRouting[] secondary = new ShardRouting[primary.length];
@@ -702,7 +702,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                             if (logger.isTraceEnabled()) {
                                 logger.trace("Assigned shard [{}] to [{}]", shard, minNode.getNodeId());
                             }
-                            routingNodes.initialize(shard, minNode.getNodeId(), allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
+                            routingNodes.initialize(shard, minNode.getNodeId(), null, allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                             changed = true;
                             continue; // don't add to ignoreUnassigned
                         } else {
@@ -733,7 +733,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 secondary = tmp;
                 secondaryLength = 0;
             } while (primaryLength > 0);
-            // clear everything we have either added it or moved to ingoreUnassigned
+            // clear everything we have either added it or moved to ignoreUnassigned
             return changed;
         }
 
@@ -790,7 +790,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                             routingNodes.relocate(candidate, minNode.getNodeId(), allocation.clusterInfo().getShardSize(candidate, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
 
                         } else {
-                            routingNodes.initialize(candidate, minNode.getNodeId(), allocation.clusterInfo().getShardSize(candidate, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
+                            routingNodes.initialize(candidate, minNode.getNodeId(), null, allocation.clusterInfo().getShardSize(candidate, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                         }
                         return true;
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java
index 5a13b3b..3a89507 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AbstractAllocateAllocationCommand.java
@@ -242,7 +242,7 @@ public abstract class AbstractAllocateAllocationCommand implements AllocationCom
             if (shardRoutingChanges != null) {
                 shardRoutingChanges.accept(unassigned);
             }
-            it.initialize(routingNode.nodeId(), allocation.clusterInfo().getShardSize(unassigned, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
+            it.initialize(routingNode.nodeId(), null, allocation.clusterInfo().getShardSize(unassigned, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
             return;
         }
         assert false : "shard to initialize not found in list of unassigned shards";
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
index 19047bc..9859a9b 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
@@ -54,7 +54,7 @@ import java.util.Map;
  * <p>
  * Awareness can also be used to prevent over-allocation in the case of node or
  * even "zone" failure. For example in cloud-computing infrastructures like
- * Amazone AWS a cluster might span over multiple "zones". Awareness can be used
+ * Amazon AWS a cluster might span over multiple "zones". Awareness can be used
  * to distribute replicas to individual zones by setting:
  * <pre>
  * cluster.routing.allocation.awareness.attributes: zone
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
index a9ad35f..d39b960 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
@@ -30,7 +30,7 @@ import org.elasticsearch.common.settings.Settings;
  * Similar to the {@link ClusterRebalanceAllocationDecider} this
  * {@link AllocationDecider} controls the number of currently in-progress
  * re-balance (relocation) operations and restricts node allocations if the
- * configured threashold is reached. The default number of concurrent rebalance
+ * configured threshold is reached. The default number of concurrent rebalance
  * operations is set to <tt>2</tt>
  * <p>
  * Re-balance operations can be controlled in real-time via the cluster update API using
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
index c012017..821fa55 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
@@ -282,7 +282,7 @@ public class DiskThresholdDecider extends AllocationDecider {
 
     /**
      * Returns the size of all shards that are currently being relocated to
-     * the node, but may not be finished transfering yet.
+     * the node, but may not be finished transferring yet.
      *
      * If subtractShardsMovingAway is set then the size of shards moving away is subtracted from the total size
      * of all shards
diff --git a/core/src/main/java/org/elasticsearch/common/Base64.java b/core/src/main/java/org/elasticsearch/common/Base64.java
index 1bc6fd0..37d4526 100644
--- a/core/src/main/java/org/elasticsearch/common/Base64.java
+++ b/core/src/main/java/org/elasticsearch/common/Base64.java
@@ -93,7 +93,7 @@ import java.util.Locale;
  * <a href="http://www.faqs.org/rfcs/rfc3548.html">RFC3548</a>.</li>
  * <li><em>Throws exceptions instead of returning null values.</em> Because some operations
  * (especially those that may permit the GZIP option) use IO streams, there
- * is a possiblity of an java.io.IOException being thrown. After some discussion and
+ * is a possibility of an java.io.IOException being thrown. After some discussion and
  * thought, I've changed the behavior of the methods to throw java.io.IOExceptions
  * rather than return null if ever there's an error. I think this is more
  * appropriate, though it will require some changes to your code. Sorry,
@@ -1511,7 +1511,7 @@ public class Base64 {
             if (suspendEncoding) {
                 this.out.write(theByte);
                 return;
-            }   // end if: supsended
+            }   // end if: suspended
 
             // Encode?
             if (encode) {
@@ -1565,7 +1565,7 @@ public class Base64 {
             if (suspendEncoding) {
                 this.out.write(theBytes, off, len);
                 return;
-            }   // end if: supsended
+            }   // end if: suspended
 
             for (int i = 0; i < len; i++) {
                 write(theBytes[off + i]);
diff --git a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
index 1477179..a41b038 100644
--- a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
+++ b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
@@ -147,7 +147,7 @@ public class PagedBytesReference implements BytesReference {
         bytearray.get(offset, length, ref);
 
         // undo the single-page optimization by ByteArray.get(), otherwise
-        // a materialized stream will contain traling garbage/zeros
+        // a materialized stream will contain trailing garbage/zeros
         byte[] result = ref.bytes;
         if (result.length != length || ref.offset != 0) {
             result = Arrays.copyOfRange(result, ref.offset, ref.offset + length);
@@ -403,7 +403,7 @@ public class PagedBytesReference implements BytesReference {
                 return -1;
             }
 
-            final int numBytesToCopy = Math.min(len, length - pos); // copy the full lenth or the remaining part
+            final int numBytesToCopy = Math.min(len, length - pos); // copy the full length or the remaining part
 
             // current offset into the underlying ByteArray
             long byteArrayOffset = offset + pos;
diff --git a/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java b/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java
index 7afccb3..479496d 100644
--- a/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java
+++ b/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java
@@ -33,7 +33,7 @@ package org.elasticsearch.common.component;
  * following logic can be applied:
  * <pre>
  * public void stop() {
- *  if (!lifeccycleState.moveToStopped()) {
+ *  if (!lifecycleState.moveToStopped()) {
  *      return;
  *  }
  * // continue with stop logic
@@ -50,7 +50,7 @@ package org.elasticsearch.common.component;
  *  if (!lifecycleState.moveToClosed()) {
  *      return;
  *  }
- *  // perofrm close logic here
+ *  // perform close logic here
  * }
  * </pre>
  */
diff --git a/core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java b/core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java
index 1209410..599eaea 100644
--- a/core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java
+++ b/core/src/main/java/org/elasticsearch/common/compress/CompressedIndexInput.java
@@ -124,7 +124,7 @@ public abstract class CompressedIndexInput extends IndexInput {
 
     @Override
     public void readBytes(byte[] b, int offset, int len) throws IOException {
-        int result = read(b, offset, len, true /* we want to have full reads, thats the contract... */);
+        int result = read(b, offset, len, true /* we want to have full reads, that's the contract... */);
         if (result < len) {
             throw new EOFException();
         }
diff --git a/core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java b/core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java
index 44690e6..bdacbd4 100644
--- a/core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java
@@ -121,7 +121,7 @@ public abstract class CompressedStreamInput extends StreamInput {
 
     @Override
     public void readBytes(byte[] b, int offset, int len) throws IOException {
-        int result = read(b, offset, len, true /* we want to have full reads, thats the contract... */);
+        int result = read(b, offset, len, true /* we want to have full reads, that's the contract... */);
         if (result < len) {
             throw new EOFException();
         }
diff --git a/core/src/main/java/org/elasticsearch/common/compress/deflate/DeflateCompressor.java b/core/src/main/java/org/elasticsearch/common/compress/deflate/DeflateCompressor.java
index b2aea1f..be39632 100644
--- a/core/src/main/java/org/elasticsearch/common/compress/deflate/DeflateCompressor.java
+++ b/core/src/main/java/org/elasticsearch/common/compress/deflate/DeflateCompressor.java
@@ -52,7 +52,7 @@ public class DeflateCompressor implements Compressor {
     private static final byte[] HEADER = new byte[] { 'D', 'F', 'L', '\0' };
     // 3 is a good trade-off between speed and compression ratio
     private static final int LEVEL = 3;
-    // We use buffering on the input and ouput of in/def-laters in order to
+    // We use buffering on the input and output of in/def-laters in order to
     // limit the number of JNI calls
     private static final int BUFFER_SIZE = 4096;
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Exposed.java b/core/src/main/java/org/elasticsearch/common/inject/Exposed.java
index 8e0598a..4f55721 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Exposed.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Exposed.java
@@ -24,7 +24,7 @@ import java.lang.annotation.Target;
 import static java.lang.annotation.RetentionPolicy.RUNTIME;
 
 /**
- * Acccompanies a {@literal @}{@link org.elasticsearch.common.inject.Provides Provides} method annotation in a
+ * Accompanies a {@literal @}{@link org.elasticsearch.common.inject.Provides Provides} method annotation in a
  * private module to indicate that the provided binding is exposed.
  *
  * @author jessewilson@google.com (Jesse Wilson)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java b/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
index 7ad62af..c8f80ef 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
@@ -489,7 +489,7 @@ class InjectorImpl implements Injector, Lookups {
         ParameterizedType parameterizedType = (ParameterizedType) typeLiteralType;
         Type innerType = parameterizedType.getActualTypeArguments()[0];
 
-        // this is unforunate. We don't support building TypeLiterals for type variable like 'T'. If
+        // this is unfortunate. We don't support building TypeLiterals for type variable like 'T'. If
         // this proves problematic, we can probably fix TypeLiteral to support type variables
         if (!(innerType instanceof Class)
                 && !(innerType instanceof GenericArrayType)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Key.java b/core/src/main/java/org/elasticsearch/common/inject/Key.java
index 7344dfe..83ab440 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Key.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Key.java
@@ -41,7 +41,7 @@ import java.util.Objects;
  * TypeLiteral}.
  * <p>
  * Keys do not differentiate between primitive types (int, char, etc.) and
- * their correpsonding wrapper types (Integer, Character, etc.). Primitive
+ * their corresponding wrapper types (Integer, Character, etc.). Primitive
  * types will be replaced with their wrapper types when keys are created.
  *
  * @author crazybob@google.com (Bob Lee)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
index edceb3f..0716653 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
@@ -30,7 +30,7 @@ import java.util.List;
 import java.util.Set;
 
 /**
- * Internal respresentation of a constructor annotated with
+ * Internal representation of a constructor annotated with
  * {@link AssistedInject}
  *
  * @author jmourits@google.com (Jerome Mourits)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java
index fea6a62..fc2a96e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java
@@ -24,7 +24,7 @@ import java.util.Arrays;
 import java.util.List;
 
 /**
- * A list of {@link TypeLiteral}s to match an injectable Constructor's assited
+ * A list of {@link TypeLiteral}s to match an injectable Constructor's assisted
  * parameter types to the corresponding factory method.
  *
  * @author jmourits@google.com (Jerome Mourits)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java
index f339f4a..0f8f6ed 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java
@@ -91,7 +91,7 @@ public final class Dependency<T> {
 
     /**
      * Returns the index of this dependency in the injection point's parameter list, or {@code -1} if
-     * this dependency does not belong to a parameter list. Only method and constuctor dependencies
+     * this dependency does not belong to a parameter list. Only method and constructor dependencies
      * are elements in a parameter list.
      */
     public int getParameterIndex() {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
index 286635b..3339388 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
@@ -125,7 +125,7 @@ public final class InjectionPoint {
         return Collections.unmodifiableList(dependencies);
     }
 
-    // This metohd is necessary to create a Dependency<T> with proper generic type information
+    // This method is necessary to create a Dependency<T> with proper generic type information
     private <T> Dependency<T> newDependency(Key<T> key, boolean allowsNull, int parameterIndex) {
         return new Dependency<>(this, key, allowsNull, parameterIndex);
     }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java
index 9b925cd..7113855 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java
@@ -34,7 +34,7 @@ import java.util.Objects;
  */
 public final class ProviderLookup<T> implements Element {
 
-    // NOTE: this class is not part of guice and was added so the provder lookup's key can be acessible for tests
+    // NOTE: this class is not part of guice and was added so the provider lookup's key can be accessible for tests
     public static class ProviderImpl<T> implements Provider<T> {
         private ProviderLookup<T> lookup;
 
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java b/core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java
index ec83546..64cbf7d 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/NotSerializableExceptionWrapper.java
@@ -28,7 +28,7 @@ import java.io.IOException;
 /**
  * This exception can be used to wrap a given, not serializable exception
  * to serialize via {@link StreamOutput#writeThrowable(Throwable)}.
- * This class will perserve the stacktrace as well as the suppressed exceptions of
+ * This class will preserve the stacktrace as well as the suppressed exceptions of
  * the throwable it was created with instead of it's own. The stacktrace has no indication
  * of where this exception was created.
  */
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 8688802..6d10cac 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -39,8 +39,6 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.tasks.Task;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -666,20 +664,6 @@ public abstract class StreamInput extends InputStream {
     }
 
     /**
-     * Reads a {@link AggregatorBuilder} from the current stream
-     */
-    public AggregatorBuilder readAggregatorFactory() throws IOException {
-        return readNamedWriteable(AggregatorBuilder.class);
-    }
-
-    /**
-     * Reads a {@link PipelineAggregatorBuilder} from the current stream
-     */
-    public PipelineAggregatorBuilder readPipelineAggregatorFactory() throws IOException {
-        return readNamedWriteable(PipelineAggregatorBuilder.class);
-    }
-
-    /**
      * Reads a {@link QueryBuilder} from the current stream
      */
     public QueryBuilder readQuery() throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 0cda18c..77d4005 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -38,8 +38,6 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.tasks.Task;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -643,20 +641,6 @@ public abstract class StreamOutput extends OutputStream {
     }
 
     /**
-     * Writes a {@link AggregatorBuilder} to the current stream
-     */
-    public void writeAggregatorFactory(AggregatorBuilder factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
-     * Writes a {@link PipelineAggregatorBuilder} to the current stream
-     */
-    public void writePipelineAggregatorFactory(PipelineAggregatorBuilder factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
      * Writes a {@link QueryBuilder} to the current stream
      */
     public void writeQuery(QueryBuilder queryBuilder) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
index b693af1..43051f9 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
@@ -627,7 +627,7 @@ public class Lucene {
     }
 
     /**
-     * Parses the version string lenient and returns the the default value if the given string is null or emtpy
+     * Parses the version string lenient and returns the default value if the given string is null or emtpy
      */
     public static Version parseVersionLenient(String toParse, Version defaultValue) {
         return LenientParser.parse(toParse, defaultValue);
diff --git a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
index f0bd4fb..d055724 100644
--- a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
+++ b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
@@ -113,8 +113,4 @@ public final class Cidrs {
         assert octets.length == 4;
         return octetsToString(octets) + "/" + networkMask;
     }
-
-    public static String createCIDR(long ipAddress, int networkMask) {
-        return octetsToCIDR(longToOctets(ipAddress), networkMask);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 1debc69..5e8dbc4 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -104,7 +104,7 @@ public class NetworkService extends AbstractComponent {
      */
     public InetAddress[] resolveBindHostAddresses(String bindHosts[]) throws IOException {
         // first check settings
-        if (bindHosts == null) {
+        if (bindHosts == null || bindHosts.length == 0) {
             if (GLOBAL_NETWORK_BINDHOST_SETTING.exists(settings) || GLOBAL_NETWORK_HOST_SETTING.exists(settings)) {
                 // if we have settings use them (we have a fallback to GLOBAL_NETWORK_HOST_SETTING inline
                 bindHosts = GLOBAL_NETWORK_BINDHOST_SETTING.get(settings).toArray(Strings.EMPTY_ARRAY);
diff --git a/core/src/main/java/org/elasticsearch/common/recycler/FilterRecycler.java b/core/src/main/java/org/elasticsearch/common/recycler/FilterRecycler.java
index 9bb6b86..8646234 100644
--- a/core/src/main/java/org/elasticsearch/common/recycler/FilterRecycler.java
+++ b/core/src/main/java/org/elasticsearch/common/recycler/FilterRecycler.java
@@ -21,7 +21,7 @@ package org.elasticsearch.common.recycler;
 
 abstract class FilterRecycler<T> implements Recycler<T> {
 
-    /** Get the delegate instance to foward calls to. */
+    /** Get the delegate instance to forward calls to. */
     protected abstract Recycler<T> getDelegate();
 
     /** Wrap a recycled reference. */
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
index 7e94ebb..89a2679 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
@@ -19,13 +19,11 @@
 package org.elasticsearch.common.rounding;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A strategy for rounding long values.
@@ -63,12 +61,6 @@ public abstract class Rounding implements Streamable {
      */
     public abstract long nextRoundingValue(long value);
 
-    @Override
-    public abstract boolean equals(Object obj);
-
-    @Override
-    public abstract int hashCode();
-
     /**
      * Rounding strategy which is based on an interval
      *
@@ -78,8 +70,6 @@ public abstract class Rounding implements Streamable {
 
         final static byte ID = 0;
 
-        public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-
         private long interval;
 
         public Interval() { // for serialization
@@ -136,31 +126,12 @@ public abstract class Rounding implements Streamable {
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVLong(interval);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Interval other = (Interval) obj;
-            return Objects.equals(interval, other.interval);
-        }
     }
 
     public static class FactorRounding extends Rounding {
 
         final static byte ID = 7;
 
-        public static final ParseField FACTOR_FIELD = new ParseField("factor");
-
         private Rounding rounding;
 
         private float factor;
@@ -195,7 +166,7 @@ public abstract class Rounding implements Streamable {
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
-            rounding = Rounding.Streams.read(in);
+            rounding = (TimeZoneRounding) Rounding.Streams.read(in);
             factor = in.readFloat();
         }
 
@@ -204,32 +175,12 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeFloat(factor);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, factor);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            FactorRounding other = (FactorRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(factor, other.factor);
-        }
     }
 
     public static class OffsetRounding extends Rounding {
 
         final static byte ID = 8;
 
-        public static final ParseField OFFSET_FIELD = new ParseField("offset");
-
         private Rounding rounding;
 
         private long offset;
@@ -273,24 +224,6 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeLong(offset);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, offset);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            OffsetRounding other = (OffsetRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(offset, other.offset);
-        }
     }
 
     public static class Streams {
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
index 4189e41..1e6bbb6 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.rounding;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
@@ -28,13 +27,10 @@ import org.joda.time.DateTimeZone;
 import org.joda.time.DurationField;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  */
 public abstract class TimeZoneRounding extends Rounding {
-    public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-    public static final ParseField TIME_ZONE_FIELD = new ParseField("time_zone");
 
     public static Builder builder(DateTimeUnit unit) {
         return new Builder(unit);
@@ -161,24 +157,6 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeByte(unit.id());
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(unit, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeUnitRounding other = (TimeUnitRounding) obj;
-            return Objects.equals(unit, other.unit)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 
     static class TimeIntervalRounding extends TimeZoneRounding {
@@ -236,23 +214,5 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeVLong(interval);
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeIntervalRounding other = (TimeIntervalRounding) obj;
-            return Objects.equals(interval, other.interval)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
index 033423b..142fbac 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
@@ -22,6 +22,7 @@ import org.elasticsearch.action.admin.indices.close.TransportCloseIndexAction;
 import org.elasticsearch.action.support.AutoCreateIndex;
 import org.elasticsearch.action.support.DestructiveOperations;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
+import org.elasticsearch.bootstrap.BootstrapSettings;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClientNodesService;
@@ -58,6 +59,7 @@ import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.gateway.PrimaryShardAllocator;
 import org.elasticsearch.http.HttpTransportSettings;
+import org.elasticsearch.http.netty.NettyHttpServerTransport;
 import org.elasticsearch.index.IndexSettings;
 import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.indices.IndicesService;
@@ -88,7 +90,6 @@ import org.elasticsearch.transport.TransportService;
 import org.elasticsearch.transport.TransportSettings;
 import org.elasticsearch.transport.netty.NettyTransport;
 import org.elasticsearch.tribe.TribeService;
-import org.elasticsearch.bootstrap.BootstrapSettings;
 
 import java.util.Arrays;
 import java.util.Collections;
@@ -141,7 +142,8 @@ public final class ClusterSettings extends AbstractScopedSettings {
                 String component = key.substring("logger.".length());
                 if ("_root".equals(component)) {
                     final String rootLevel = value.get(key);
-                    ESLoggerFactory.getRootLogger().setLevel(rootLevel == null ? ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING.get(settings).name() : rootLevel);
+                    ESLoggerFactory.getRootLogger().setLevel(rootLevel == null ? ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING.get(settings)
+                            .name() : rootLevel);
                 } else {
                     ESLoggerFactory.getLogger(component).setLevel(value.get(key));
                 }
@@ -150,240 +152,253 @@ public final class ClusterSettings extends AbstractScopedSettings {
     }
 
     public static Set<Setting<?>> BUILT_IN_CLUSTER_SETTINGS = Collections.unmodifiableSet(new HashSet<>(
-        Arrays.asList(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING,
-            TransportClientNodesService.CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL, // TODO these transport client settings are kind of odd here and should only be valid if we are a transport client
-            TransportClientNodesService.CLIENT_TRANSPORT_PING_TIMEOUT,
-            TransportClientNodesService.CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME,
-            TransportClientNodesService.CLIENT_TRANSPORT_SNIFF,
-            AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING,
-            BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING,
-            BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING,
-            BalancedShardsAllocator.THRESHOLD_SETTING,
-            ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING,
-            ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING,
-            EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING,
-            EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING,
-            FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP_SETTING,
-            FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING,
-            FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING,
-            FsRepository.REPOSITORIES_CHUNK_SIZE_SETTING,
-            FsRepository.REPOSITORIES_COMPRESS_SETTING,
-            FsRepository.REPOSITORIES_LOCATION_SETTING,
-            IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING,
-            IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING,
-            IndicesQueryCache.INDICES_CACHE_QUERY_SIZE_SETTING,
-            IndicesQueryCache.INDICES_CACHE_QUERY_COUNT_SETTING,
-            IndicesTTLService.INDICES_TTL_INTERVAL_SETTING,
-            MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING,
-            MetaData.SETTING_READ_ONLY_SETTING,
-            RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING,
-            RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING,
-            RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING,
-            RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING,
-            RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING,
-            RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING,
-            ThreadPool.THREADPOOL_GROUP_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_OUTGOING_RECOVERIES_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING,
-            InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING,
-            InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING,
-            SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING,
-            DestructiveOperations.REQUIRES_NAME_SETTING,
-            DiscoverySettings.PUBLISH_TIMEOUT_SETTING,
-            DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING,
-            DiscoverySettings.COMMIT_TIMEOUT_SETTING,
-            DiscoverySettings.NO_MASTER_BLOCK_SETTING,
-            GatewayService.EXPECTED_DATA_NODES_SETTING,
-            GatewayService.EXPECTED_MASTER_NODES_SETTING,
-            GatewayService.EXPECTED_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_DATA_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_TIME_SETTING,
-            NetworkModule.HTTP_ENABLED,
-            NetworkModule.HTTP_TYPE_SETTING,
-            NetworkModule.TRANSPORT_SERVICE_TYPE_SETTING,
-            NetworkModule.TRANSPORT_TYPE_SETTING,
-            HttpTransportSettings.SETTING_CORS_ALLOW_CREDENTIALS,
-            HttpTransportSettings.SETTING_CORS_ENABLED,
-            HttpTransportSettings.SETTING_CORS_MAX_AGE,
-            HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
-            HttpTransportSettings.SETTING_PIPELINING,
-            HttpTransportSettings.SETTING_CORS_ALLOW_ORIGIN,
-            HttpTransportSettings.SETTING_HTTP_PORT,
-            HttpTransportSettings.SETTING_HTTP_PUBLISH_PORT,
-            HttpTransportSettings.SETTING_PIPELINING_MAX_EVENTS,
-            HttpTransportSettings.SETTING_HTTP_COMPRESSION,
-            HttpTransportSettings.SETTING_HTTP_COMPRESSION_LEVEL,
-            HttpTransportSettings.SETTING_CORS_ALLOW_METHODS,
-            HttpTransportSettings.SETTING_CORS_ALLOW_HEADERS,
-            HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
-            HttpTransportSettings.SETTING_HTTP_MAX_CONTENT_LENGTH,
-            HttpTransportSettings.SETTING_HTTP_MAX_CHUNK_SIZE,
-            HttpTransportSettings.SETTING_HTTP_MAX_HEADER_SIZE,
-            HttpTransportSettings.SETTING_HTTP_MAX_INITIAL_LINE_LENGTH,
-            HttpTransportSettings.SETTING_HTTP_RESET_COOKIES,
-            HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
-            HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
-            HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
-            HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING,
-            HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING,
-            InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING,
-            SearchService.DEFAULT_SEARCH_TIMEOUT_SETTING,
-            ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING,
-            TransportService.TRACE_LOG_EXCLUDE_SETTING,
-            TransportService.TRACE_LOG_INCLUDE_SETTING,
-            TransportCloseIndexAction.CLUSTER_INDICES_CLOSE_ENABLE_SETTING,
-            ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING,
-            InternalClusterService.CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING,
-            HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING,
-            HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING,
-            Transport.TRANSPORT_TCP_COMPRESS,
-            TransportSettings.TRANSPORT_PROFILES_SETTING,
-            TransportSettings.HOST,
-            TransportSettings.PUBLISH_HOST,
-            TransportSettings.BIND_HOST,
-            TransportSettings.PUBLISH_PORT,
-            TransportSettings.PORT,
-            NettyTransport.WORKER_COUNT,
-            NettyTransport.CONNECTIONS_PER_NODE_RECOVERY,
-            NettyTransport.CONNECTIONS_PER_NODE_BULK,
-            NettyTransport.CONNECTIONS_PER_NODE_REG,
-            NettyTransport.CONNECTIONS_PER_NODE_STATE,
-            NettyTransport.CONNECTIONS_PER_NODE_PING,
-            NettyTransport.PING_SCHEDULE,
-            NettyTransport.TCP_BLOCKING_CLIENT,
-            NettyTransport.TCP_CONNECT_TIMEOUT,
-            NettyTransport.NETTY_MAX_CUMULATION_BUFFER_CAPACITY,
-            NettyTransport.NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS,
-            NettyTransport.NETTY_RECEIVE_PREDICTOR_SIZE,
-            NettyTransport.NETTY_RECEIVE_PREDICTOR_MIN,
-            NettyTransport.NETTY_RECEIVE_PREDICTOR_MAX,
-            NetworkService.NETWORK_SERVER,
-            NettyTransport.NETTY_BOSS_COUNT,
-            NettyTransport.TCP_NO_DELAY,
-            NettyTransport.TCP_KEEP_ALIVE,
-            NettyTransport.TCP_REUSE_ADDRESS,
-            NettyTransport.TCP_SEND_BUFFER_SIZE,
-            NettyTransport.TCP_RECEIVE_BUFFER_SIZE,
-            NettyTransport.TCP_BLOCKING_SERVER,
-            NetworkService.GLOBAL_NETWORK_HOST_SETTING,
-            NetworkService.GLOBAL_NETWORK_BINDHOST_SETTING,
-            NetworkService.GLOBAL_NETWORK_PUBLISHHOST_SETTING,
-            NetworkService.TcpSettings.TCP_NO_DELAY,
-            NetworkService.TcpSettings.TCP_KEEP_ALIVE,
-            NetworkService.TcpSettings.TCP_REUSE_ADDRESS,
-            NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE,
-            NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE,
-            NetworkService.TcpSettings.TCP_BLOCKING,
-            NetworkService.TcpSettings.TCP_BLOCKING_SERVER,
-            NetworkService.TcpSettings.TCP_BLOCKING_CLIENT,
-            NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT,
-            IndexSettings.QUERY_STRING_ANALYZE_WILDCARD,
-            IndexSettings.QUERY_STRING_ALLOW_LEADING_WILDCARD,
-            PrimaryShardAllocator.NODE_INITIAL_SHARDS_SETTING,
-            ScriptService.SCRIPT_CACHE_SIZE_SETTING,
-            ScriptService.SCRIPT_CACHE_EXPIRE_SETTING,
-            ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING,
-            IndicesService.INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING,
-            IndicesFieldDataCache.INDICES_FIELDDATA_CACHE_SIZE_KEY,
-            IndicesRequestCache.INDICES_CACHE_QUERY_SIZE,
-            IndicesRequestCache.INDICES_CACHE_QUERY_EXPIRE,
-            IndicesRequestCache.INDICES_CACHE_REQUEST_CLEAN_INTERVAL,
-            HunspellService.HUNSPELL_LAZY_LOAD,
-            HunspellService.HUNSPELL_IGNORE_CASE,
-            HunspellService.HUNSPELL_DICTIONARY_OPTIONS,
-            IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT,
-            Environment.PATH_CONF_SETTING,
-            Environment.PATH_DATA_SETTING,
-            Environment.PATH_HOME_SETTING,
-            Environment.PATH_LOGS_SETTING,
-            Environment.PATH_PLUGINS_SETTING,
-            Environment.PATH_REPO_SETTING,
-            Environment.PATH_SCRIPTS_SETTING,
-            Environment.PATH_SHARED_DATA_SETTING,
-            Environment.PIDFILE_SETTING,
-            DiscoveryService.DISCOVERY_SEED_SETTING,
-            DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING,
-            DiscoveryModule.DISCOVERY_TYPE_SETTING,
-            DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING,
-            FaultDetection.PING_RETRIES_SETTING,
-            FaultDetection.PING_TIMEOUT_SETTING,
-            FaultDetection.REGISTER_CONNECTION_LISTENER_SETTING,
-            FaultDetection.PING_INTERVAL_SETTING,
-            FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING,
-            ZenDiscovery.PING_TIMEOUT_SETTING,
-            ZenDiscovery.JOIN_TIMEOUT_SETTING,
-            ZenDiscovery.JOIN_RETRY_ATTEMPTS_SETTING,
-            ZenDiscovery.JOIN_RETRY_DELAY_SETTING,
-            ZenDiscovery.MAX_PINGS_FROM_ANOTHER_MASTER_SETTING,
-            ZenDiscovery.SEND_LEAVE_REQUEST_SETTING,
-            ZenDiscovery.MASTER_ELECTION_FILTER_CLIENT_SETTING,
-            ZenDiscovery.MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING,
-            ZenDiscovery.MASTER_ELECTION_FILTER_DATA_SETTING,
-            UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING,
-            UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING,
-            SearchService.DEFAULT_KEEPALIVE_SETTING,
-            SearchService.KEEPALIVE_INTERVAL_SETTING,
-            Node.WRITE_PORTS_FIELD_SETTING,
-            Node.NODE_NAME_SETTING,
-            Node.NODE_CLIENT_SETTING,
-            Node.NODE_DATA_SETTING,
-            Node.NODE_MASTER_SETTING,
-            Node.NODE_LOCAL_SETTING,
-            Node.NODE_MODE_SETTING,
-            Node.NODE_INGEST_SETTING,
-            Node.NODE_ATTRIBUTES,
-            URLRepository.ALLOWED_URLS_SETTING,
-            URLRepository.REPOSITORIES_LIST_DIRECTORIES_SETTING,
-            URLRepository.REPOSITORIES_URL_SETTING,
-            URLRepository.SUPPORTED_PROTOCOLS_SETTING,
-            TransportMasterNodeReadAction.FORCE_LOCAL_SETTING,
-            AutoCreateIndex.AUTO_CREATE_INDEX_SETTING,
-            BaseRestHandler.MULTI_ALLOW_EXPLICIT_INDEX,
-            ClusterName.CLUSTER_NAME_SETTING,
-            Client.CLIENT_TYPE_SETTING_S,
-            InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING,
-            ClusterModule.SHARDS_ALLOCATOR_TYPE_SETTING,
-            EsExecutors.PROCESSORS_SETTING,
-            ThreadContext.DEFAULT_HEADERS_SETTING,
-            ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING,
-            ESLoggerFactory.LOG_LEVEL_SETTING,
-            TribeService.BLOCKS_METADATA_SETTING,
-            TribeService.BLOCKS_WRITE_SETTING,
-            TribeService.BLOCKS_WRITE_INDICES_SETTING,
-            TribeService.BLOCKS_READ_INDICES_SETTING,
-            TribeService.BLOCKS_METADATA_INDICES_SETTING,
-            TribeService.ON_CONFLICT_SETTING,
-            TribeService.TRIBE_NAME_SETTING,
-            NodeEnvironment.MAX_LOCAL_STORAGE_NODES_SETTING,
-            NodeEnvironment.ENABLE_LUCENE_SEGMENT_INFOS_TRACE_SETTING,
-            NodeEnvironment.ADD_NODE_ID_TO_CUSTOM_PATH,
-            OsService.REFRESH_INTERVAL_SETTING,
-            ProcessService.REFRESH_INTERVAL_SETTING,
-            JvmService.REFRESH_INTERVAL_SETTING,
-            FsService.REFRESH_INTERVAL_SETTING,
-            JvmGcMonitorService.ENABLED_SETTING,
-            JvmGcMonitorService.REFRESH_INTERVAL_SETTING,
-            JvmGcMonitorService.GC_SETTING,
-            PageCacheRecycler.LIMIT_HEAP_SETTING,
-            PageCacheRecycler.WEIGHT_BYTES_SETTING,
-            PageCacheRecycler.WEIGHT_INT_SETTING,
-            PageCacheRecycler.WEIGHT_LONG_SETTING,
-            PageCacheRecycler.WEIGHT_OBJECTS_SETTING,
-            PageCacheRecycler.TYPE_SETTING,
-            PluginsService.MANDATORY_SETTING,
-            BootstrapSettings.SECURITY_MANAGER_ENABLED_SETTING,
-            BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING,
-            BootstrapSettings.MLOCKALL_SETTING,
-            BootstrapSettings.SECCOMP_SETTING,
-            BootstrapSettings.CTRLHANDLER_SETTING
-        )));
+            Arrays.asList(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING,
+                    TransportClientNodesService.CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL, // TODO these transport client settings are kind
+                    // of odd here and should only be valid if we are a transport client
+                    TransportClientNodesService.CLIENT_TRANSPORT_PING_TIMEOUT,
+                    TransportClientNodesService.CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME,
+                    TransportClientNodesService.CLIENT_TRANSPORT_SNIFF,
+                    AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING,
+                    BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING,
+                    BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING,
+                    BalancedShardsAllocator.THRESHOLD_SETTING,
+                    ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING,
+                    ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING,
+                    EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING,
+                    EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING,
+                    FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP_SETTING,
+                    FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING,
+                    FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING,
+                    FsRepository.REPOSITORIES_CHUNK_SIZE_SETTING,
+                    FsRepository.REPOSITORIES_COMPRESS_SETTING,
+                    FsRepository.REPOSITORIES_LOCATION_SETTING,
+                    IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING,
+                    IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING,
+                    IndicesQueryCache.INDICES_CACHE_QUERY_SIZE_SETTING,
+                    IndicesQueryCache.INDICES_CACHE_QUERY_COUNT_SETTING,
+                    IndicesTTLService.INDICES_TTL_INTERVAL_SETTING,
+                    MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING,
+                    MetaData.SETTING_READ_ONLY_SETTING,
+                    RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING,
+                    RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING,
+                    RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING,
+                    RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING,
+                    RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING,
+                    RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING,
+                    ThreadPool.THREADPOOL_GROUP_SETTING,
+                    ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING,
+                    ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING,
+                    ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_OUTGOING_RECOVERIES_SETTING,
+                    ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING,
+                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING,
+                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING,
+                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING,
+                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING,
+                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING,
+                    InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING,
+                    InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING,
+                    SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING,
+                    DestructiveOperations.REQUIRES_NAME_SETTING,
+                    DiscoverySettings.PUBLISH_TIMEOUT_SETTING,
+                    DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING,
+                    DiscoverySettings.COMMIT_TIMEOUT_SETTING,
+                    DiscoverySettings.NO_MASTER_BLOCK_SETTING,
+                    GatewayService.EXPECTED_DATA_NODES_SETTING,
+                    GatewayService.EXPECTED_MASTER_NODES_SETTING,
+                    GatewayService.EXPECTED_NODES_SETTING,
+                    GatewayService.RECOVER_AFTER_DATA_NODES_SETTING,
+                    GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING,
+                    GatewayService.RECOVER_AFTER_NODES_SETTING,
+                    GatewayService.RECOVER_AFTER_TIME_SETTING,
+                    NetworkModule.HTTP_ENABLED,
+                    NetworkModule.HTTP_TYPE_SETTING,
+                    NetworkModule.TRANSPORT_SERVICE_TYPE_SETTING,
+                    NetworkModule.TRANSPORT_TYPE_SETTING,
+                    HttpTransportSettings.SETTING_CORS_ALLOW_CREDENTIALS,
+                    HttpTransportSettings.SETTING_CORS_ENABLED,
+                    HttpTransportSettings.SETTING_CORS_MAX_AGE,
+                    HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
+                    HttpTransportSettings.SETTING_PIPELINING,
+                    HttpTransportSettings.SETTING_CORS_ALLOW_ORIGIN,
+                    HttpTransportSettings.SETTING_HTTP_PORT,
+                    HttpTransportSettings.SETTING_HTTP_PUBLISH_PORT,
+                    HttpTransportSettings.SETTING_PIPELINING_MAX_EVENTS,
+                    HttpTransportSettings.SETTING_HTTP_COMPRESSION,
+                    HttpTransportSettings.SETTING_HTTP_COMPRESSION_LEVEL,
+                    HttpTransportSettings.SETTING_CORS_ALLOW_METHODS,
+                    HttpTransportSettings.SETTING_CORS_ALLOW_HEADERS,
+                    HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
+                    HttpTransportSettings.SETTING_HTTP_MAX_CONTENT_LENGTH,
+                    HttpTransportSettings.SETTING_HTTP_MAX_CHUNK_SIZE,
+                    HttpTransportSettings.SETTING_HTTP_MAX_HEADER_SIZE,
+                    HttpTransportSettings.SETTING_HTTP_MAX_INITIAL_LINE_LENGTH,
+                    HttpTransportSettings.SETTING_HTTP_RESET_COOKIES,
+                    NettyHttpServerTransport.SETTING_HTTP_NETTY_MAX_CUMULATION_BUFFER_CAPACITY,
+                    NettyHttpServerTransport.SETTING_HTTP_NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS,
+                    NettyHttpServerTransport.SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_SIZE,
+                    NettyHttpServerTransport.SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_MIN,
+                    NettyHttpServerTransport.SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_MAX,
+                    NettyHttpServerTransport.SETTING_HTTP_WORKER_COUNT,
+                    NettyHttpServerTransport.SETTING_HTTP_TCP_NO_DELAY,
+                    NettyHttpServerTransport.SETTING_HTTP_TCP_KEEP_ALIVE,
+                    NettyHttpServerTransport.SETTING_HTTP_TCP_BLOCKING_SERVER,
+                    NettyHttpServerTransport.SETTING_HTTP_TCP_REUSE_ADDRESS,
+                    NettyHttpServerTransport.SETTING_HTTP_TCP_SEND_BUFFER_SIZE,
+                    NettyHttpServerTransport.SETTING_HTTP_TCP_RECEIVE_BUFFER_SIZE,
+                    HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
+                    HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
+                    HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
+                    HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING,
+                    HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING,
+                    InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING,
+                    SearchService.DEFAULT_SEARCH_TIMEOUT_SETTING,
+                    ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING,
+                    TransportService.TRACE_LOG_EXCLUDE_SETTING,
+                    TransportService.TRACE_LOG_INCLUDE_SETTING,
+                    TransportCloseIndexAction.CLUSTER_INDICES_CLOSE_ENABLE_SETTING,
+                    ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING,
+                    InternalClusterService.CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING,
+                    HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING,
+                    HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING,
+                    Transport.TRANSPORT_TCP_COMPRESS,
+                    TransportSettings.TRANSPORT_PROFILES_SETTING,
+                    TransportSettings.HOST,
+                    TransportSettings.PUBLISH_HOST,
+                    TransportSettings.BIND_HOST,
+                    TransportSettings.PUBLISH_PORT,
+                    TransportSettings.PORT,
+                    NettyTransport.WORKER_COUNT,
+                    NettyTransport.CONNECTIONS_PER_NODE_RECOVERY,
+                    NettyTransport.CONNECTIONS_PER_NODE_BULK,
+                    NettyTransport.CONNECTIONS_PER_NODE_REG,
+                    NettyTransport.CONNECTIONS_PER_NODE_STATE,
+                    NettyTransport.CONNECTIONS_PER_NODE_PING,
+                    NettyTransport.PING_SCHEDULE,
+                    NettyTransport.TCP_BLOCKING_CLIENT,
+                    NettyTransport.TCP_CONNECT_TIMEOUT,
+                    NettyTransport.NETTY_MAX_CUMULATION_BUFFER_CAPACITY,
+                    NettyTransport.NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS,
+                    NettyTransport.NETTY_RECEIVE_PREDICTOR_SIZE,
+                    NettyTransport.NETTY_RECEIVE_PREDICTOR_MIN,
+                    NettyTransport.NETTY_RECEIVE_PREDICTOR_MAX,
+                    NetworkService.NETWORK_SERVER,
+                    NettyTransport.NETTY_BOSS_COUNT,
+                    NettyTransport.TCP_NO_DELAY,
+                    NettyTransport.TCP_KEEP_ALIVE,
+                    NettyTransport.TCP_REUSE_ADDRESS,
+                    NettyTransport.TCP_SEND_BUFFER_SIZE,
+                    NettyTransport.TCP_RECEIVE_BUFFER_SIZE,
+                    NettyTransport.TCP_BLOCKING_SERVER,
+                    NetworkService.GLOBAL_NETWORK_HOST_SETTING,
+                    NetworkService.GLOBAL_NETWORK_BINDHOST_SETTING,
+                    NetworkService.GLOBAL_NETWORK_PUBLISHHOST_SETTING,
+                    NetworkService.TcpSettings.TCP_NO_DELAY,
+                    NetworkService.TcpSettings.TCP_KEEP_ALIVE,
+                    NetworkService.TcpSettings.TCP_REUSE_ADDRESS,
+                    NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE,
+                    NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE,
+                    NetworkService.TcpSettings.TCP_BLOCKING,
+                    NetworkService.TcpSettings.TCP_BLOCKING_SERVER,
+                    NetworkService.TcpSettings.TCP_BLOCKING_CLIENT,
+                    NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT,
+                    IndexSettings.QUERY_STRING_ANALYZE_WILDCARD,
+                    IndexSettings.QUERY_STRING_ALLOW_LEADING_WILDCARD,
+                    PrimaryShardAllocator.NODE_INITIAL_SHARDS_SETTING,
+                    ScriptService.SCRIPT_CACHE_SIZE_SETTING,
+                    ScriptService.SCRIPT_CACHE_EXPIRE_SETTING,
+                    ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING,
+                    IndicesService.INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING,
+                    IndicesFieldDataCache.INDICES_FIELDDATA_CACHE_SIZE_KEY,
+                    IndicesRequestCache.INDICES_CACHE_QUERY_SIZE,
+                    IndicesRequestCache.INDICES_CACHE_QUERY_EXPIRE,
+                    IndicesRequestCache.INDICES_CACHE_REQUEST_CLEAN_INTERVAL,
+                    HunspellService.HUNSPELL_LAZY_LOAD,
+                    HunspellService.HUNSPELL_IGNORE_CASE,
+                    HunspellService.HUNSPELL_DICTIONARY_OPTIONS,
+                    IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT,
+                    Environment.PATH_CONF_SETTING,
+                    Environment.PATH_DATA_SETTING,
+                    Environment.PATH_HOME_SETTING,
+                    Environment.PATH_LOGS_SETTING,
+                    Environment.PATH_PLUGINS_SETTING,
+                    Environment.PATH_REPO_SETTING,
+                    Environment.PATH_SCRIPTS_SETTING,
+                    Environment.PATH_SHARED_DATA_SETTING,
+                    Environment.PIDFILE_SETTING,
+                    DiscoveryService.DISCOVERY_SEED_SETTING,
+                    DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING,
+                    DiscoveryModule.DISCOVERY_TYPE_SETTING,
+                    DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING,
+                    FaultDetection.PING_RETRIES_SETTING,
+                    FaultDetection.PING_TIMEOUT_SETTING,
+                    FaultDetection.REGISTER_CONNECTION_LISTENER_SETTING,
+                    FaultDetection.PING_INTERVAL_SETTING,
+                    FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING,
+                    ZenDiscovery.PING_TIMEOUT_SETTING,
+                    ZenDiscovery.JOIN_TIMEOUT_SETTING,
+                    ZenDiscovery.JOIN_RETRY_ATTEMPTS_SETTING,
+                    ZenDiscovery.JOIN_RETRY_DELAY_SETTING,
+                    ZenDiscovery.MAX_PINGS_FROM_ANOTHER_MASTER_SETTING,
+                    ZenDiscovery.SEND_LEAVE_REQUEST_SETTING,
+                    ZenDiscovery.MASTER_ELECTION_FILTER_CLIENT_SETTING,
+                    ZenDiscovery.MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING,
+                    ZenDiscovery.MASTER_ELECTION_FILTER_DATA_SETTING,
+                    UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING,
+                    UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING,
+                    SearchService.DEFAULT_KEEPALIVE_SETTING,
+                    SearchService.KEEPALIVE_INTERVAL_SETTING,
+                    Node.WRITE_PORTS_FIELD_SETTING,
+                    Node.NODE_NAME_SETTING,
+                    Node.NODE_CLIENT_SETTING,
+                    Node.NODE_DATA_SETTING,
+                    Node.NODE_MASTER_SETTING,
+                    Node.NODE_LOCAL_SETTING,
+                    Node.NODE_MODE_SETTING,
+                    Node.NODE_INGEST_SETTING,
+                    Node.NODE_ATTRIBUTES,
+                    URLRepository.ALLOWED_URLS_SETTING,
+                    URLRepository.REPOSITORIES_LIST_DIRECTORIES_SETTING,
+                    URLRepository.REPOSITORIES_URL_SETTING,
+                    URLRepository.SUPPORTED_PROTOCOLS_SETTING,
+                    TransportMasterNodeReadAction.FORCE_LOCAL_SETTING,
+                    AutoCreateIndex.AUTO_CREATE_INDEX_SETTING,
+                    BaseRestHandler.MULTI_ALLOW_EXPLICIT_INDEX,
+                    ClusterName.CLUSTER_NAME_SETTING,
+                    Client.CLIENT_TYPE_SETTING_S,
+                    InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING,
+                    ClusterModule.SHARDS_ALLOCATOR_TYPE_SETTING,
+                    EsExecutors.PROCESSORS_SETTING,
+                    ThreadContext.DEFAULT_HEADERS_SETTING,
+                    ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING,
+                    ESLoggerFactory.LOG_LEVEL_SETTING,
+                    TribeService.BLOCKS_METADATA_SETTING,
+                    TribeService.BLOCKS_WRITE_SETTING,
+                    TribeService.BLOCKS_WRITE_INDICES_SETTING,
+                    TribeService.BLOCKS_READ_INDICES_SETTING,
+                    TribeService.BLOCKS_METADATA_INDICES_SETTING,
+                    TribeService.ON_CONFLICT_SETTING,
+                    TribeService.TRIBE_NAME_SETTING,
+                    NodeEnvironment.MAX_LOCAL_STORAGE_NODES_SETTING,
+                    NodeEnvironment.ENABLE_LUCENE_SEGMENT_INFOS_TRACE_SETTING,
+                    NodeEnvironment.ADD_NODE_ID_TO_CUSTOM_PATH,
+                    OsService.REFRESH_INTERVAL_SETTING,
+                    ProcessService.REFRESH_INTERVAL_SETTING,
+                    JvmService.REFRESH_INTERVAL_SETTING,
+                    FsService.REFRESH_INTERVAL_SETTING,
+                    JvmGcMonitorService.ENABLED_SETTING,
+                    JvmGcMonitorService.REFRESH_INTERVAL_SETTING,
+                    JvmGcMonitorService.GC_SETTING,
+                    PageCacheRecycler.LIMIT_HEAP_SETTING,
+                    PageCacheRecycler.WEIGHT_BYTES_SETTING,
+                    PageCacheRecycler.WEIGHT_INT_SETTING,
+                    PageCacheRecycler.WEIGHT_LONG_SETTING,
+                    PageCacheRecycler.WEIGHT_OBJECTS_SETTING,
+                    PageCacheRecycler.TYPE_SETTING,
+                    PluginsService.MANDATORY_SETTING,
+                    BootstrapSettings.SECURITY_MANAGER_ENABLED_SETTING,
+                    BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING,
+                    BootstrapSettings.MLOCKALL_SETTING,
+                    BootstrapSettings.SECCOMP_SETTING,
+                    BootstrapSettings.CTRLHANDLER_SETTING
+            )));
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Setting.java b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
index 22485ec..7f64c01 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Setting.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
@@ -47,7 +47,7 @@ import java.util.stream.Collectors;
  * A setting. Encapsulates typical stuff like default value, parsing, and scope.
  * Some (dynamic=true) can by modified at run time using the API.
  * All settings inside elasticsearch or in any of the plugins should use this type-safe and generic settings infrastructure
- * together with {@link AbstractScopedSettings}. This class contains several untility methods that makes it straight forward
+ * together with {@link AbstractScopedSettings}. This class contains several utility methods that makes it straight forward
  * to add settings for the majority of the cases. For instance a simple boolean settings can be defined like this:
  * <pre>{@code
  * public static final Setting<Boolean>; MY_BOOLEAN = Setting.boolSetting("my.bool.setting", true, false, Scope.CLUSTER);}
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 8be01a6..aafaff3 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -471,7 +471,7 @@ public final class Settings implements ToXContent {
 
     /**
      * Returns the setting value (as size) associated with the setting key. Provided values can either be
-     * absolute values (intepreted as a number of bytes), byte sizes (eg. 1mb) or percentage of the heap size
+     * absolute values (interpreted as a number of bytes), byte sizes (eg. 1mb) or percentage of the heap size
      * (eg. 12%). If it does not exists, parses the default value provided.
      */
     public ByteSizeValue getAsMemory(String setting, String defaultValue) throws SettingsException {
@@ -480,7 +480,7 @@ public final class Settings implements ToXContent {
 
     /**
      * Returns the setting value (as size) associated with the setting key. Provided values can either be
-     * absolute values (intepreted as a number of bytes), byte sizes (eg. 1mb) or percentage of the heap size
+     * absolute values (interpreted as a number of bytes), byte sizes (eg. 1mb) or percentage of the heap size
      * (eg. 12%). If it does not exists, parses the default value provided.
      */
     public ByteSizeValue getAsMemory(String[] settings, String defaultValue) throws SettingsException {
diff --git a/core/src/main/java/org/elasticsearch/common/util/AbstractPagedHashMap.java b/core/src/main/java/org/elasticsearch/common/util/AbstractPagedHashMap.java
index 2368a5d..b075721 100644
--- a/core/src/main/java/org/elasticsearch/common/util/AbstractPagedHashMap.java
+++ b/core/src/main/java/org/elasticsearch/common/util/AbstractPagedHashMap.java
@@ -112,7 +112,7 @@ abstract class AbstractPagedHashMap implements Releasable {
             }
         }
         // The only entries which have not been put in their final position in the previous loop are those that were stored in a slot that
-        // is < slot(key, mask). This only happens when slot(key, mask) returned a slot that was close to the end of the array and colision
+        // is < slot(key, mask). This only happens when slot(key, mask) returned a slot that was close to the end of the array and collision
         // resolution has put it back in the first slots. This time, collision resolution will have put them at the beginning of the newly
         // allocated slots. Let's re-add them to make sure they are in the right slot. This 2nd loop will typically exit very early.
         for (long i = buckets; i < newBuckets; ++i) {
diff --git a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
index 17ae43b..db167c5 100644
--- a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
@@ -78,7 +78,7 @@ public abstract class ExtensionPoint {
         /**
          * Creates a new {@link ClassMap}
          *
-         * @param name           the human readable underscore case name of the extension poing. This is used in error messages etc.
+         * @param name           the human readable underscore case name of the extension point. This is used in error messages etc.
          * @param extensionClass the base class that should be extended
          * @param singletons     a list of singletons to bind with this extension point - these are bound in {@link #bind(Binder)}
          * @param reservedKeys   a set of reserved keys by internal implementations
@@ -120,7 +120,7 @@ public abstract class ExtensionPoint {
     }
 
     /**
-     * A Type extension point which basically allows to registerd keyed extensions like {@link ClassMap}
+     * A Type extension point which basically allows to registered keyed extensions like {@link ClassMap}
      * but doesn't instantiate and bind all the registered key value pairs but instead replace a singleton based on a given setting via {@link #bindType(Binder, Settings, String, String)}
      * Note: {@link #bind(Binder)} is not supported by this class
      */
@@ -169,7 +169,7 @@ public abstract class ExtensionPoint {
         /**
          * Creates a new {@link ClassSet}
          *
-         * @param name           the human readable underscore case name of the extension poing. This is used in error messages etc.
+         * @param name           the human readable underscore case name of the extension point. This is used in error messages etc.
          * @param extensionClass the base class that should be extended
          * @param singletons     a list of singletons to bind with this extension point - these are bound in {@link #bind(Binder)}
          */
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java b/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
index 6689d9c..dec8569 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
@@ -41,7 +41,7 @@ public class DiscoverySettings extends AbstractComponent {
     public final static ClusterBlock NO_MASTER_BLOCK_WRITES = new ClusterBlock(NO_MASTER_BLOCK_ID, "no master", true, false, RestStatus.SERVICE_UNAVAILABLE, EnumSet.of(ClusterBlockLevel.WRITE, ClusterBlockLevel.METADATA_WRITE));
     /**
      * sets the timeout for a complete publishing cycle, including both sending and committing. the master
-     * will continute to process the next cluster state update after this time has elapsed
+     * will continue to process the next cluster state update after this time has elapsed
      **/
     public static final Setting<TimeValue> PUBLISH_TIMEOUT_SETTING = Setting.positiveTimeSetting("discovery.zen.publish_timeout", TimeValue.timeValueSeconds(30), true, Setting.Scope.CLUSTER);
 
diff --git a/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
index cd4a6a5..10fe432 100644
--- a/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
@@ -328,7 +328,7 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
                 ClusterState newNodeSpecificClusterState = null;
                 synchronized (this) {
                     // we do the marshaling intentionally, to check it works well...
-                    // check if we publsihed cluster state at least once and node was in the cluster when we published cluster state the last time
+                    // check if we published cluster state at least once and node was in the cluster when we published cluster state the last time
                     if (discovery.lastProcessedClusterState != null && clusterChangedEvent.previousState().nodes().nodeExists(discovery.localNode().id())) {
                         // both conditions are true - which means we can try sending cluster state as diffs
                         if (clusterStateDiffBytes == null) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
index 9cca1ed..1482fb9 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
@@ -79,7 +79,7 @@ public class ElectMasterService extends AbstractComponent {
     }
 
     /**
-     * Returns the given nodes sorted by likelyhood of being elected as master, most likely first.
+     * Returns the given nodes sorted by likelihood of being elected as master, most likely first.
      * Non-master nodes are not removed but are rather put in the end
      */
     public List<DiscoveryNode> sortByMasterLikelihood(Iterable<DiscoveryNode> nodes) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
index e661002..427abca 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
@@ -342,7 +342,7 @@ public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implemen
         // sort the nodes by likelihood of being an active master
         List<DiscoveryNode> sortedNodesToPing = electMasterService.sortByMasterLikelihood(nodesToPingSet);
 
-        // new add the the unicast targets first
+        // new add the unicast targets first
         List<DiscoveryNode> nodesToPing = CollectionUtils.arrayAsArrayList(configuredTargetNodes);
         nodesToPing.addAll(sortedNodesToPing);
 
diff --git a/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java b/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java
index 4524222..757a78c 100644
--- a/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java
+++ b/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java
@@ -166,7 +166,7 @@ public abstract class AsyncShardFetch<T extends BaseNodeResponse> implements Rel
     /**
      * Called by the response handler of the async action to fetch data. Verifies that its still working
      * on the same cache generation, otherwise the results are discarded. It then goes and fills the relevant data for
-     * the shard (response + failures), issueing a reroute at the end of it to make sure there will be another round
+     * the shard (response + failures), issuing a reroute at the end of it to make sure there will be another round
      * of allocations taking this new data into account.
      */
     protected synchronized void processAsyncFetch(ShardId shardId, T[] responses, FailedNodeException[] failures) {
diff --git a/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java b/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java
index 5c7b31d..f714c64 100644
--- a/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java
+++ b/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java
@@ -255,7 +255,7 @@ public abstract class MetaDataStateFormat<T> {
         List<PathAndStateId> files = new ArrayList<>();
         long maxStateId = -1;
         boolean maxStateIdIsLegacy = true;
-        if (dataLocations != null) { // select all eligable files first
+        if (dataLocations != null) { // select all eligible files first
             for (Path dataLocation : dataLocations) {
                 final Path stateDir = dataLocation.resolve(STATE_DIR_NAME);
                 // now, iterate over the current versions, and find latest one
diff --git a/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java b/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java
index 012b33d..8809f68 100644
--- a/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java
+++ b/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java
@@ -32,15 +32,14 @@ import org.elasticsearch.cluster.routing.allocation.decider.Decision;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.NodeGatewayStartedShards;
 import org.elasticsearch.index.shard.ShardStateMetaData;
 
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Comparator;
-import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
-import java.util.Map;
 import java.util.Set;
 import java.util.function.Function;
 import java.util.stream.Collectors;
@@ -98,7 +97,7 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
                 continue;
             }
 
-            final AsyncShardFetch.FetchResult<TransportNodesListGatewayStartedShards.NodeGatewayStartedShards> shardState = fetchData(shard, allocation);
+            final AsyncShardFetch.FetchResult<NodeGatewayStartedShards> shardState = fetchData(shard, allocation);
             if (shardState.hasData() == false) {
                 logger.trace("{}: ignoring allocation, still fetching shard started state", shard);
                 allocation.setHasPendingAsyncFetch();
@@ -110,7 +109,7 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
             final boolean snapshotRestore = shard.restoreSource() != null;
             final boolean recoverOnAnyNode = recoverOnAnyNode(indexMetaData);
 
-            final NodesResult nodesResult;
+            final NodeShardsResult nodeShardsResult;
             final boolean enoughAllocationsFound;
 
             if (lastActiveAllocationIds.isEmpty()) {
@@ -118,20 +117,20 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
                 // when we load an old index (after upgrading cluster) or restore a snapshot of an old index
                 // fall back to old version-based allocation mode
                 // Note that once the shard has been active, lastActiveAllocationIds will be non-empty
-                nodesResult = buildVersionBasedNodes(shard, snapshotRestore || recoverOnAnyNode, allocation.getIgnoreNodes(shard.shardId()), shardState);
+                nodeShardsResult = buildVersionBasedNodeShardsResult(shard, snapshotRestore || recoverOnAnyNode, allocation.getIgnoreNodes(shard.shardId()), shardState);
                 if (snapshotRestore || recoverOnAnyNode) {
-                    enoughAllocationsFound = nodesResult.allocationsFound > 0;
+                    enoughAllocationsFound = nodeShardsResult.allocationsFound > 0;
                 } else {
-                    enoughAllocationsFound = isEnoughVersionBasedAllocationsFound(shard, indexMetaData, nodesResult);
+                    enoughAllocationsFound = isEnoughVersionBasedAllocationsFound(indexMetaData, nodeShardsResult);
                 }
-                logger.debug("[{}][{}]: version-based allocation for pre-{} index found {} allocations of {}", shard.index(), shard.id(), Version.V_3_0_0, nodesResult.allocationsFound, shard);
+                logger.debug("[{}][{}]: version-based allocation for pre-{} index found {} allocations of {}", shard.index(), shard.id(), Version.V_3_0_0, nodeShardsResult.allocationsFound, shard);
             } else {
                 assert lastActiveAllocationIds.isEmpty() == false;
                 // use allocation ids to select nodes
-                nodesResult = buildAllocationIdBasedNodes(shard, snapshotRestore || recoverOnAnyNode,
+                nodeShardsResult = buildAllocationIdBasedNodeShardsResult(shard, snapshotRestore || recoverOnAnyNode,
                         allocation.getIgnoreNodes(shard.shardId()), lastActiveAllocationIds, shardState);
-                enoughAllocationsFound = nodesResult.allocationsFound > 0;
-                logger.debug("[{}][{}]: found {} allocations of {} based on allocation ids: [{}]", shard.index(), shard.id(), nodesResult.allocationsFound, shard, lastActiveAllocationIds);
+                enoughAllocationsFound = nodeShardsResult.orderedAllocationCandidates.size() > 0;
+                logger.debug("[{}][{}]: found {} allocation candidates of {} based on allocation ids: [{}]", shard.index(), shard.id(), nodeShardsResult.orderedAllocationCandidates.size(), shard, lastActiveAllocationIds);
             }
 
             if (enoughAllocationsFound == false){
@@ -144,25 +143,25 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
                 } else {
                     // we can't really allocate, so ignore it and continue
                     unassignedIterator.removeAndIgnore();
-                    logger.debug("[{}][{}]: not allocating, number_of_allocated_shards_found [{}]", shard.index(), shard.id(), nodesResult.allocationsFound);
+                    logger.debug("[{}][{}]: not allocating, number_of_allocated_shards_found [{}]", shard.index(), shard.id(), nodeShardsResult.allocationsFound);
                 }
                 continue;
             }
 
-            final NodesToAllocate nodesToAllocate = buildNodesToAllocate(shard, allocation, nodesResult.nodes);
-            if (nodesToAllocate.yesNodes.isEmpty() == false) {
-                DiscoveryNode node = nodesToAllocate.yesNodes.get(0);
-                logger.debug("[{}][{}]: allocating [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, node);
+            final NodesToAllocate nodesToAllocate = buildNodesToAllocate(shard, allocation, nodeShardsResult.orderedAllocationCandidates);
+            if (nodesToAllocate.yesNodeShards.isEmpty() == false) {
+                NodeGatewayStartedShards nodeShardState = nodesToAllocate.yesNodeShards.get(0);
+                logger.debug("[{}][{}]: allocating [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, nodeShardState.getNode());
                 changed = true;
-                unassignedIterator.initialize(node.id(), ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
-            } else if (nodesToAllocate.throttleNodes.isEmpty() == true && nodesToAllocate.noNodes.isEmpty() == false) {
-                DiscoveryNode node = nodesToAllocate.noNodes.get(0);
-                logger.debug("[{}][{}]: forcing allocating [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, node);
+                unassignedIterator.initialize(nodeShardState.getNode().id(), nodeShardState.allocationId(), ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
+            } else if (nodesToAllocate.throttleNodeShards.isEmpty() == true && nodesToAllocate.noNodeShards.isEmpty() == false) {
+                NodeGatewayStartedShards nodeShardState = nodesToAllocate.noNodeShards.get(0);
+                logger.debug("[{}][{}]: forcing allocating [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, nodeShardState.getNode());
                 changed = true;
-                unassignedIterator.initialize(node.id(), ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
+                unassignedIterator.initialize(nodeShardState.getNode().id(), nodeShardState.allocationId(), ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
             } else {
                 // we are throttling this, but we have enough to allocate to this node, ignore it for now
-                logger.debug("[{}][{}]: throttling allocation [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, nodesToAllocate.throttleNodes);
+                logger.debug("[{}][{}]: throttling allocation [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, nodesToAllocate.throttleNodeShards);
                 unassignedIterator.removeAndIgnore();
             }
         }
@@ -174,11 +173,12 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
      * lastActiveAllocationIds are added to the list. Otherwise, any node that has a shard is added to the list, but
      * entries with matching allocation id are always at the front of the list.
      */
-    protected NodesResult buildAllocationIdBasedNodes(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes,
-                                                      Set<String> lastActiveAllocationIds, AsyncShardFetch.FetchResult<TransportNodesListGatewayStartedShards.NodeGatewayStartedShards> shardState) {
-        LinkedList<DiscoveryNode> matchingNodes = new LinkedList<>();
-        LinkedList<DiscoveryNode> nonMatchingNodes = new LinkedList<>();
-        for (TransportNodesListGatewayStartedShards.NodeGatewayStartedShards nodeShardState : shardState.getData().values()) {
+    protected NodeShardsResult buildAllocationIdBasedNodeShardsResult(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes,
+                                                                      Set<String> lastActiveAllocationIds, AsyncShardFetch.FetchResult<NodeGatewayStartedShards> shardState) {
+        LinkedList<NodeGatewayStartedShards> matchingNodeShardStates = new LinkedList<>();
+        LinkedList<NodeGatewayStartedShards> nonMatchingNodeShardStates = new LinkedList<>();
+        int numberOfAllocationsFound = 0;
+        for (NodeGatewayStartedShards nodeShardState : shardState.getData().values()) {
             DiscoveryNode node = nodeShardState.getNode();
             String allocationId = nodeShardState.allocationId();
 
@@ -199,36 +199,37 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
             }
 
             if (allocationId != null) {
+                numberOfAllocationsFound++;
                 if (lastActiveAllocationIds.contains(allocationId)) {
                     if (nodeShardState.primary()) {
-                        matchingNodes.addFirst(node);
+                        matchingNodeShardStates.addFirst(nodeShardState);
                     } else {
-                        matchingNodes.addLast(node);
+                        matchingNodeShardStates.addLast(nodeShardState);
                     }
                 } else if (matchAnyShard) {
                     if (nodeShardState.primary()) {
-                        nonMatchingNodes.addFirst(node);
+                        nonMatchingNodeShardStates.addFirst(nodeShardState);
                     } else {
-                        nonMatchingNodes.addLast(node);
+                        nonMatchingNodeShardStates.addLast(nodeShardState);
                     }
                 }
             }
         }
 
-        List<DiscoveryNode> nodes = new ArrayList<>();
-        nodes.addAll(matchingNodes);
-        nodes.addAll(nonMatchingNodes);
+        List<NodeGatewayStartedShards> nodeShardStates = new ArrayList<>();
+        nodeShardStates.addAll(matchingNodeShardStates);
+        nodeShardStates.addAll(nonMatchingNodeShardStates);
 
         if (logger.isTraceEnabled()) {
-            logger.trace("{} candidates for allocation: {}", shard, nodes.stream().map(DiscoveryNode::name).collect(Collectors.joining(", ")));
+            logger.trace("{} candidates for allocation: {}", shard, nodeShardStates.stream().map(s -> s.getNode().getName()).collect(Collectors.joining(", ")));
         }
-        return new NodesResult(nodes, nodes.size());
+        return new NodeShardsResult(nodeShardStates, numberOfAllocationsFound);
     }
 
     /**
      * used by old version-based allocation
      */
-    private boolean isEnoughVersionBasedAllocationsFound(ShardRouting shard, IndexMetaData indexMetaData, NodesResult nodesAndVersions) {
+    private boolean isEnoughVersionBasedAllocationsFound(IndexMetaData indexMetaData, NodeShardsResult nodeShardsResult) {
         // check if the counts meets the minimum set
         int requiredAllocation = 1;
         // if we restore from a repository one copy is more then enough
@@ -253,45 +254,44 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
             requiredAllocation = Integer.parseInt(initialShards);
         }
 
-        return nodesAndVersions.allocationsFound >= requiredAllocation;
+        return nodeShardsResult.allocationsFound >= requiredAllocation;
     }
 
     /**
-     * Split the list of nodes to lists of yes/no/throttle nodes based on allocation deciders
+     * Split the list of node shard states into groups yes/no/throttle based on allocation deciders
      */
-    private NodesToAllocate buildNodesToAllocate(ShardRouting shard, RoutingAllocation allocation, List<DiscoveryNode> nodes) {
-        List<DiscoveryNode> yesNodes = new ArrayList<>();
-        List<DiscoveryNode> throttledNodes = new ArrayList<>();
-        List<DiscoveryNode> noNodes = new ArrayList<>();
-        for (DiscoveryNode discoNode : nodes) {
-            RoutingNode node = allocation.routingNodes().node(discoNode.id());
+    private NodesToAllocate buildNodesToAllocate(ShardRouting shard, RoutingAllocation allocation, List<NodeGatewayStartedShards> nodeShardStates) {
+        List<NodeGatewayStartedShards> yesNodeShards = new ArrayList<>();
+        List<NodeGatewayStartedShards> throttledNodeShards = new ArrayList<>();
+        List<NodeGatewayStartedShards> noNodeShards = new ArrayList<>();
+        for (NodeGatewayStartedShards nodeShardState : nodeShardStates) {
+            RoutingNode node = allocation.routingNodes().node(nodeShardState.getNode().id());
             if (node == null) {
                 continue;
             }
 
             Decision decision = allocation.deciders().canAllocate(shard, node, allocation);
             if (decision.type() == Decision.Type.THROTTLE) {
-                throttledNodes.add(discoNode);
+                throttledNodeShards.add(nodeShardState);
             } else if (decision.type() == Decision.Type.NO) {
-                noNodes.add(discoNode);
+                noNodeShards.add(nodeShardState);
             } else {
-                yesNodes.add(discoNode);
+                yesNodeShards.add(nodeShardState);
             }
         }
-        return new NodesToAllocate(Collections.unmodifiableList(yesNodes), Collections.unmodifiableList(throttledNodes), Collections.unmodifiableList(noNodes));
+        return new NodesToAllocate(Collections.unmodifiableList(yesNodeShards), Collections.unmodifiableList(throttledNodeShards), Collections.unmodifiableList(noNodeShards));
     }
 
     /**
-     * Builds a list of nodes. If matchAnyShard is set to false, only nodes that have the highest shard version
-     * are added to the list. Otherwise, any node that has a shard is added to the list, but entries with highest
-     * version are always at the front of the list.
+     * Builds a list of previously started shards. If matchAnyShard is set to false, only shards with the highest shard version are added to
+     * the list. Otherwise, any existing shard is added to the list, but entries with highest version are always at the front of the list.
      */
-    NodesResult buildVersionBasedNodes(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes,
-                                       AsyncShardFetch.FetchResult<TransportNodesListGatewayStartedShards.NodeGatewayStartedShards> shardState) {
-        final Map<DiscoveryNode, Long> nodesWithVersion = new HashMap<>();
+    NodeShardsResult buildVersionBasedNodeShardsResult(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes,
+                                                       AsyncShardFetch.FetchResult<NodeGatewayStartedShards> shardState) {
+        final List<NodeGatewayStartedShards> allocationCandidates = new ArrayList<>();
         int numberOfAllocationsFound = 0;
         long highestVersion = ShardStateMetaData.NO_VERSION;
-        for (TransportNodesListGatewayStartedShards.NodeGatewayStartedShards nodeShardState : shardState.getData().values()) {
+        for (NodeGatewayStartedShards nodeShardState : shardState.getData().values()) {
             long version = nodeShardState.legacyVersion();
             DiscoveryNode node = nodeShardState.getNode();
 
@@ -315,38 +315,29 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
                 if (version > highestVersion) {
                     highestVersion = version;
                     if (matchAnyShard == false) {
-                        nodesWithVersion.clear();
+                        allocationCandidates.clear();
                     }
-                    nodesWithVersion.put(node, version);
+                    allocationCandidates.add(nodeShardState);
                 } else if (version == highestVersion) {
                     // If the candidate is the same, add it to the
                     // list, but keep the current candidate
-                    nodesWithVersion.put(node, version);
+                    allocationCandidates.add(nodeShardState);
                 }
             }
         }
-        // Now that we have a map of nodes to versions along with the
-        // number of allocations found (and not ignored), we need to sort
-        // it so the node with the highest version is at the beginning
-        List<DiscoveryNode> nodesWithHighestVersion = new ArrayList<>();
-        nodesWithHighestVersion.addAll(nodesWithVersion.keySet());
-        CollectionUtil.timSort(nodesWithHighestVersion, new Comparator<DiscoveryNode>() {
-            @Override
-            public int compare(DiscoveryNode o1, DiscoveryNode o2) {
-                return Long.compare(nodesWithVersion.get(o2), nodesWithVersion.get(o1));
-            }
-        });
+        // sort array so the node with the highest version is at the beginning
+        CollectionUtil.timSort(allocationCandidates, Comparator.comparing(NodeGatewayStartedShards::legacyVersion).reversed());
 
         if (logger.isTraceEnabled()) {
             StringBuilder sb = new StringBuilder("[");
-            for (DiscoveryNode n : nodesWithVersion.keySet()) {
-                sb.append("[").append(n.getName()).append("]").append(" -> ").append(nodesWithVersion.get(n)).append(", ");
+            for (NodeGatewayStartedShards n : allocationCandidates) {
+                sb.append("[").append(n.getNode().getName()).append("]").append(" -> ").append(n.legacyVersion()).append(", ");
             }
             sb.append("]");
             logger.trace("{} candidates for allocation: {}", shard, sb.toString());
         }
 
-        return new NodesResult(Collections.unmodifiableList(nodesWithHighestVersion), numberOfAllocationsFound);
+        return new NodeShardsResult(Collections.unmodifiableList(allocationCandidates), numberOfAllocationsFound);
     }
 
     /**
@@ -358,27 +349,28 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
             && IndexMetaData.INDEX_SHARED_FS_ALLOW_RECOVERY_ON_ANY_NODE_SETTING.get(metaData.getSettings(), this.settings);
     }
 
-    protected abstract AsyncShardFetch.FetchResult<TransportNodesListGatewayStartedShards.NodeGatewayStartedShards> fetchData(ShardRouting shard, RoutingAllocation allocation);
+    protected abstract AsyncShardFetch.FetchResult<NodeGatewayStartedShards> fetchData(ShardRouting shard, RoutingAllocation allocation);
 
-    static class NodesResult {
-        public final List<DiscoveryNode> nodes;
+    static class NodeShardsResult {
+        public final List<NodeGatewayStartedShards> orderedAllocationCandidates;
         public final int allocationsFound;
 
-        public NodesResult(List<DiscoveryNode> nodes, int allocationsFound) {
-            this.nodes = nodes;
+        public NodeShardsResult(List<NodeGatewayStartedShards> orderedAllocationCandidates, int allocationsFound) {
+            this.orderedAllocationCandidates = orderedAllocationCandidates;
             this.allocationsFound = allocationsFound;
         }
     }
 
     static class NodesToAllocate {
-        final List<DiscoveryNode> yesNodes;
-        final List<DiscoveryNode> throttleNodes;
-        final List<DiscoveryNode> noNodes;
-
-        public NodesToAllocate(List<DiscoveryNode> yesNodes, List<DiscoveryNode> throttleNodes, List<DiscoveryNode> noNodes) {
-            this.yesNodes = yesNodes;
-            this.throttleNodes = throttleNodes;
-            this.noNodes = noNodes;
+        final List<NodeGatewayStartedShards> yesNodeShards;
+        final List<NodeGatewayStartedShards> throttleNodeShards;
+        final List<NodeGatewayStartedShards> noNodeShards;
+
+        public NodesToAllocate(List<NodeGatewayStartedShards> yesNodeShards, List<NodeGatewayStartedShards> throttleNodeShards,
+                               List<NodeGatewayStartedShards> noNodeShards) {
+            this.yesNodeShards = yesNodeShards;
+            this.throttleNodeShards = throttleNodeShards;
+            this.noNodeShards = noNodeShards;
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java b/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
index 2c25ce5..e2b6f0d 100644
--- a/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
+++ b/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
@@ -173,7 +173,7 @@ public abstract class ReplicaShardAllocator extends AbstractComponent {
                     logger.debug("[{}][{}]: allocating [{}] to [{}] in order to reuse its unallocated persistent store", shard.index(), shard.id(), shard, nodeWithHighestMatch.node());
                     // we found a match
                     changed = true;
-                    unassignedIterator.initialize(nodeWithHighestMatch.nodeId(), allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
+                    unassignedIterator.initialize(nodeWithHighestMatch.nodeId(), null, allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                 }
             } else if (matchingNodes.hasAnyData() == false) {
                 // if we didn't manage to find *any* data (regardless of matching sizes), check if the allocation of the replica shard needs to be delayed
diff --git a/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java b/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java
index 79e9c53..03f8dc8 100644
--- a/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java
+++ b/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java
@@ -335,5 +335,28 @@ public class TransportNodesListGatewayStartedShards extends TransportNodesAction
                 out.writeBoolean(false);
             }
         }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            NodeGatewayStartedShards that = (NodeGatewayStartedShards) o;
+
+            if (legacyVersion != that.legacyVersion) return false;
+            if (primary != that.primary) return false;
+            if (allocationId != null ? !allocationId.equals(that.allocationId) : that.allocationId != null) return false;
+            return storeException != null ? storeException.equals(that.storeException) : that.storeException == null;
+
+        }
+
+        @Override
+        public int hashCode() {
+            int result = Long.hashCode(legacyVersion);
+            result = 31 * result + (allocationId != null ? allocationId.hashCode() : 0);
+            result = 31 * result + (primary ? 1 : 0);
+            result = 31 * result + (storeException != null ? storeException.hashCode() : 0);
+            return result;
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java b/core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java
index c5a1844..0e36261 100644
--- a/core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java
+++ b/core/src/main/java/org/elasticsearch/http/HttpTransportSettings.java
@@ -25,6 +25,11 @@ import org.elasticsearch.common.transport.PortsRange;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 
+import java.util.List;
+
+import static java.util.Collections.emptyList;
+import static org.elasticsearch.common.settings.Setting.listSetting;
+
 public final class HttpTransportSettings {
 
     public static final Setting<Boolean> SETTING_CORS_ENABLED = Setting.boolSetting("http.cors.enabled", false, false, Scope.CLUSTER);
@@ -37,6 +42,10 @@ public final class HttpTransportSettings {
     public static final Setting<Integer> SETTING_PIPELINING_MAX_EVENTS = Setting.intSetting("http.pipelining.max_events", 10000, false, Scope.CLUSTER);
     public static final Setting<Boolean> SETTING_HTTP_COMPRESSION = Setting.boolSetting("http.compression", false, false, Scope.CLUSTER);
     public static final Setting<Integer> SETTING_HTTP_COMPRESSION_LEVEL = Setting.intSetting("http.compression_level", 6, false, Scope.CLUSTER);
+    public static final Setting<List<String>> SETTING_HTTP_HOST = listSetting("http.host", emptyList(), s -> s, false, Scope.CLUSTER);
+    public static final Setting<List<String>> SETTING_HTTP_PUBLISH_HOST = listSetting("http.publish_host", SETTING_HTTP_HOST, s -> s, false, Scope.CLUSTER);
+    public static final Setting<List<String>> SETTING_HTTP_BIND_HOST = listSetting("http.bind_host", SETTING_HTTP_HOST, s -> s, false, Scope.CLUSTER);
+
     public static final Setting<PortsRange> SETTING_HTTP_PORT = new Setting<PortsRange>("http.port", "9200-9300", PortsRange::new, false, Scope.CLUSTER);
     public static final Setting<Integer> SETTING_HTTP_PUBLISH_PORT = Setting.intSetting("http.publish_port", 0, 0, false, Scope.CLUSTER);
     public static final Setting<Boolean> SETTING_HTTP_DETAILED_ERRORS_ENABLED = Setting.boolSetting("http.detailed_errors.enabled", true, false, Scope.CLUSTER);
diff --git a/core/src/main/java/org/elasticsearch/http/netty/ESHttpContentDecompressor.java b/core/src/main/java/org/elasticsearch/http/netty/ESHttpContentDecompressor.java
index 79e8456..0d1e46c 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/ESHttpContentDecompressor.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/ESHttpContentDecompressor.java
@@ -39,7 +39,7 @@ public class ESHttpContentDecompressor extends HttpContentDecompressor {
             // compression is enabled so handle the request according to the headers (compressed and uncompressed)
             return super.newContentDecoder(contentEncoding);
         } else {
-            // if compression is disabled only allow "indentity" (uncompressed) requests
+            // if compression is disabled only allow "identity" (uncompressed) requests
             if (HttpHeaders.Values.IDENTITY.equals(contentEncoding)) {
                 // nothing to handle here
                 return null;
@@ -48,4 +48,4 @@ public class ESHttpContentDecompressor extends HttpContentDecompressor {
             }
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
index 280ef71..79927c2 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
@@ -20,13 +20,13 @@
 package org.elasticsearch.http.netty;
 
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.netty.NettyUtils;
 import org.elasticsearch.common.netty.OpenChannelsHandler;
 import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
@@ -50,8 +50,8 @@ import org.elasticsearch.http.netty.cors.CorsConfigBuilder;
 import org.elasticsearch.http.netty.cors.CorsHandler;
 import org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler;
 import org.elasticsearch.monitor.jvm.JvmInfo;
-import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.rest.support.RestUtils;
+import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.BindTransportException;
 import org.jboss.netty.bootstrap.ServerBootstrap;
 import org.jboss.netty.channel.AdaptiveReceiveBufferSizePredictorFactory;
@@ -81,19 +81,16 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.regex.Pattern;
 
+import static org.elasticsearch.common.settings.Setting.boolSetting;
+import static org.elasticsearch.common.settings.Setting.byteSizeSetting;
 import static org.elasticsearch.common.util.concurrent.EsExecutors.daemonThreadFactory;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_KEEP_ALIVE;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_NO_DELAY;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_REUSE_ADDRESS;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_CORS_ALLOW_CREDENTIALS;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_CORS_ALLOW_HEADERS;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_CORS_ALLOW_METHODS;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_CORS_ALLOW_ORIGIN;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_CORS_ENABLED;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_CORS_MAX_AGE;
+import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_BIND_HOST;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_COMPRESSION;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_COMPRESSION_LEVEL;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED;
@@ -102,6 +99,7 @@ import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_MAX_CONT
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_MAX_HEADER_SIZE;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_MAX_INITIAL_LINE_LENGTH;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_PORT;
+import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_PUBLISH_HOST;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_PUBLISH_PORT;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_HTTP_RESET_COOKIES;
 import static org.elasticsearch.http.HttpTransportSettings.SETTING_PIPELINING;
@@ -117,6 +115,52 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         NettyUtils.setup();
     }
 
+    public static Setting<ByteSizeValue> SETTING_HTTP_NETTY_MAX_CUMULATION_BUFFER_CAPACITY =
+            Setting.byteSizeSetting("http.netty.max_cumulation_buffer_capacity", new ByteSizeValue(-1), false, Setting.Scope.CLUSTER);
+    public static Setting<Integer> SETTING_HTTP_NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS =
+            Setting.intSetting("http.netty.max_composite_buffer_components", -1, false, Setting.Scope.CLUSTER);
+
+    public static final Setting<Integer> SETTING_HTTP_WORKER_COUNT = new Setting<>("http.netty.worker_count",
+            (s) -> Integer.toString(EsExecutors.boundedNumberOfProcessors(s) * 2),
+            (s) -> Setting.parseInt(s, 1, "http.netty.worker_count"), false, Setting.Scope.CLUSTER);
+
+    public static final Setting<Boolean> SETTING_HTTP_TCP_NO_DELAY = boolSetting("http.tcp_no_delay", NetworkService.TcpSettings
+            .TCP_NO_DELAY, false,
+            Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> SETTING_HTTP_TCP_KEEP_ALIVE = boolSetting("http.tcp.keep_alive", NetworkService.TcpSettings
+            .TCP_KEEP_ALIVE, false,
+            Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> SETTING_HTTP_TCP_BLOCKING_SERVER = boolSetting("http.tcp.blocking_server", NetworkService
+            .TcpSettings.TCP_BLOCKING_SERVER,
+            false, Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> SETTING_HTTP_TCP_REUSE_ADDRESS = boolSetting("http.tcp.reuse_address", NetworkService
+            .TcpSettings.TCP_REUSE_ADDRESS,
+            false, Setting.Scope.CLUSTER);
+
+    public static final Setting<ByteSizeValue> SETTING_HTTP_TCP_SEND_BUFFER_SIZE = Setting.byteSizeSetting("http.tcp.send_buffer_size",
+            NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE, false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> SETTING_HTTP_TCP_RECEIVE_BUFFER_SIZE = Setting.byteSizeSetting("http.tcp" +
+            ".receive_buffer_size", NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE, false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_SIZE = Setting.byteSizeSetting(
+            "transport.netty.receive_predictor_size",
+            settings -> {
+                long defaultReceiverPredictor = 512 * 1024;
+                if (JvmInfo.jvmInfo().getMem().getDirectMemoryMax().bytes() > 0) {
+                    // we can guess a better default...
+                    long l = (long) ((0.3 * JvmInfo.jvmInfo().getMem().getDirectMemoryMax().bytes()) / SETTING_HTTP_WORKER_COUNT.get
+                            (settings));
+                    defaultReceiverPredictor = Math.min(defaultReceiverPredictor, Math.max(l, 64 * 1024));
+                }
+                return new ByteSizeValue(defaultReceiverPredictor).toString();
+            }, false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_MIN = byteSizeSetting("http.netty" +
+            ".receive_predictor_min",
+            SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_SIZE, false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_MAX = byteSizeSetting("http.netty" +
+            ".receive_predictor_max",
+            SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_SIZE, false, Setting.Scope.CLUSTER);
+
+
     protected final NetworkService networkService;
     protected final BigArrays bigArrays;
 
@@ -175,47 +219,36 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
     private final CorsConfig corsConfig;
 
     @Inject
-    @SuppressForbidden(reason = "sets org.jboss.netty.epollBugWorkaround based on netty.epollBugWorkaround")
-    // TODO: why be confusing like this? just let the user do it with the netty parameter instead!
     public NettyHttpServerTransport(Settings settings, NetworkService networkService, BigArrays bigArrays, ThreadPool threadPool) {
         super(settings);
         this.networkService = networkService;
         this.bigArrays = bigArrays;
         this.threadPool = threadPool;
 
-        if (settings.getAsBoolean("netty.epollBugWorkaround", false)) {
-            System.setProperty("org.jboss.netty.epollBugWorkaround", "true");
-        }
         ByteSizeValue maxContentLength = SETTING_HTTP_MAX_CONTENT_LENGTH.get(settings);
         this.maxChunkSize = SETTING_HTTP_MAX_CHUNK_SIZE.get(settings);
         this.maxHeaderSize = SETTING_HTTP_MAX_HEADER_SIZE.get(settings);
         this.maxInitialLineLength = SETTING_HTTP_MAX_INITIAL_LINE_LENGTH.get(settings);
         this.resetCookies = SETTING_HTTP_RESET_COOKIES.get(settings);
-        this.maxCumulationBufferCapacity = settings.getAsBytesSize("http.netty.max_cumulation_buffer_capacity", null);
-        this.maxCompositeBufferComponents = settings.getAsInt("http.netty.max_composite_buffer_components", -1);
-        this.workerCount = settings.getAsInt("http.netty.worker_count", EsExecutors.boundedNumberOfProcessors(settings) * 2);
-        this.blockingServer = settings.getAsBoolean("http.netty.http.blocking_server", TCP_BLOCKING.get(settings));
+        this.maxCumulationBufferCapacity = SETTING_HTTP_NETTY_MAX_CUMULATION_BUFFER_CAPACITY.get(settings);
+        this.maxCompositeBufferComponents = SETTING_HTTP_NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS.get(settings);
+        this.workerCount = SETTING_HTTP_WORKER_COUNT.get(settings);
+        this.blockingServer = SETTING_HTTP_TCP_BLOCKING_SERVER.get(settings);
         this.port = SETTING_HTTP_PORT.get(settings);
-        this.bindHosts = settings.getAsArray("http.netty.bind_host", settings.getAsArray("http.bind_host", settings.getAsArray("http.host", null)));
-        this.publishHosts = settings.getAsArray("http.netty.publish_host", settings.getAsArray("http.publish_host", settings.getAsArray("http.host", null)));
+        this.bindHosts = SETTING_HTTP_BIND_HOST.get(settings).toArray(Strings.EMPTY_ARRAY);
+        this.publishHosts = SETTING_HTTP_PUBLISH_HOST.get(settings).toArray(Strings.EMPTY_ARRAY);
         this.publishPort = SETTING_HTTP_PUBLISH_PORT.get(settings);
-        this.tcpNoDelay = settings.getAsBoolean("http.netty.tcp_no_delay", TCP_NO_DELAY.get(settings));
-        this.tcpKeepAlive = settings.getAsBoolean("http.netty.tcp_keep_alive", TCP_KEEP_ALIVE.get(settings));
-        this.reuseAddress = settings.getAsBoolean("http.netty.reuse_address", TCP_REUSE_ADDRESS.get(settings));
-        this.tcpSendBufferSize = settings.getAsBytesSize("http.netty.tcp_send_buffer_size", TCP_SEND_BUFFER_SIZE.get(settings));
-        this.tcpReceiveBufferSize = settings.getAsBytesSize("http.netty.tcp_receive_buffer_size", TCP_RECEIVE_BUFFER_SIZE.get(settings));
+        this.tcpNoDelay = SETTING_HTTP_TCP_NO_DELAY.get(settings);
+        this.tcpKeepAlive = SETTING_HTTP_TCP_KEEP_ALIVE.get(settings);
+        this.reuseAddress = SETTING_HTTP_TCP_REUSE_ADDRESS.get(settings);
+        this.tcpSendBufferSize = SETTING_HTTP_TCP_SEND_BUFFER_SIZE.get(settings);
+        this.tcpReceiveBufferSize = SETTING_HTTP_TCP_RECEIVE_BUFFER_SIZE.get(settings);
         this.detailedErrorsEnabled = SETTING_HTTP_DETAILED_ERRORS_ENABLED.get(settings);
 
-        long defaultReceiverPredictor = 512 * 1024;
-        if (JvmInfo.jvmInfo().getMem().getDirectMemoryMax().bytes() > 0) {
-            // we can guess a better default...
-            long l = (long) ((0.3 * JvmInfo.jvmInfo().getMem().getDirectMemoryMax().bytes()) / workerCount);
-            defaultReceiverPredictor = Math.min(defaultReceiverPredictor, Math.max(l, 64 * 1024));
-        }
 
         // See AdaptiveReceiveBufferSizePredictor#DEFAULT_XXX for default values in netty..., we can use higher ones for us, even fixed one
-        ByteSizeValue receivePredictorMin = settings.getAsBytesSize("http.netty.receive_predictor_min", settings.getAsBytesSize("http.netty.receive_predictor_size", new ByteSizeValue(defaultReceiverPredictor)));
-        ByteSizeValue receivePredictorMax = settings.getAsBytesSize("http.netty.receive_predictor_max", settings.getAsBytesSize("http.netty.receive_predictor_size", new ByteSizeValue(defaultReceiverPredictor)));
+        ByteSizeValue receivePredictorMin = SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_MIN.get(settings);
+        ByteSizeValue receivePredictorMax = SETTING_HTTP_NETTY_RECEIVE_PREDICTOR_MAX.get(settings);
         if (receivePredictorMax.bytes() == receivePredictorMin.bytes()) {
             receiveBufferSizePredictorFactory = new FixedReceiveBufferSizePredictorFactory((int) receivePredictorMax.bytes());
         } else {
@@ -479,7 +512,7 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
                     (int) transport.maxHeaderSize.bytes(),
                     (int) transport.maxChunkSize.bytes()
             );
-            if (transport.maxCumulationBufferCapacity != null) {
+            if (transport.maxCumulationBufferCapacity.bytes() >= 0) {
                 if (transport.maxCumulationBufferCapacity.bytes() > Integer.MAX_VALUE) {
                     requestDecoder.setMaxCumulationBufferCapacity(Integer.MAX_VALUE);
                 } else {
diff --git a/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfig.java b/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfig.java
index 33a0653..bd1c2ec 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfig.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfig.java
@@ -132,7 +132,7 @@ public final class CorsConfig {
      * xhr.withCredentials = true;
      * </pre>
      * The default value for 'withCredentials' is false in which case no cookies are sent.
-     * Settning this to true will included cookies in cross origin requests.
+     * Setting this to true will included cookies in cross origin requests.
      *
      * @return {@code true} if cookies are supported.
      */
@@ -205,7 +205,7 @@ public final class CorsConfig {
      * and this setting will check that the Origin is valid and if it is not valid no
      * further processing will take place, and a error will be returned to the calling client.
      *
-     * @return {@code true} if a CORS request should short-curcuit upon receiving an invalid Origin header.
+     * @return {@code true} if a CORS request should short-circuit upon receiving an invalid Origin header.
      */
     public boolean isShortCircuit() {
         return shortCircuit;
diff --git a/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfigBuilder.java b/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfigBuilder.java
index 7331ed9..05aa1e6 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfigBuilder.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/cors/CorsConfigBuilder.java
@@ -133,7 +133,7 @@ public final class CorsConfigBuilder {
     /**
      * Web browsers may set the 'Origin' request header to 'null' if a resource is loaded
      * from the local file system. Calling this method will enable a successful CORS response
-     * with a wildcard for the the CORS response header 'Access-Control-Allow-Origin'.
+     * with a wildcard for the CORS response header 'Access-Control-Allow-Origin'.
      *
      * @return {@link CorsConfigBuilder} to support method chaining.
      */
diff --git a/core/src/main/java/org/elasticsearch/index/CompositeIndexEventListener.java b/core/src/main/java/org/elasticsearch/index/CompositeIndexEventListener.java
index 1ee1f1c..acc0d3f8 100644
--- a/core/src/main/java/org/elasticsearch/index/CompositeIndexEventListener.java
+++ b/core/src/main/java/org/elasticsearch/index/CompositeIndexEventListener.java
@@ -258,4 +258,16 @@ final class CompositeIndexEventListener implements IndexEventListener {
             }
         }
     }
+
+    @Override
+    public void onStoreClosed(ShardId shardId) {
+        for (IndexEventListener listener  : listeners) {
+            try {
+                listener.onStoreClosed(shardId);
+            } catch (Throwable t) {
+                logger.warn("failed to invoke on store closed", t);
+                throw t;
+            }
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/IndexModule.java b/core/src/main/java/org/elasticsearch/index/IndexModule.java
index fada5f4..f23441f 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexModule.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexModule.java
@@ -143,7 +143,7 @@ public final class IndexModule {
     }
 
     /**
-     * Adds an {@link IndexStore} type to this index module. Typically stores are registered with a refrence to
+     * Adds an {@link IndexStore} type to this index module. Typically stores are registered with a reference to
      * it's constructor:
      * <pre>
      *     indexModule.addIndexStore("my_store_type", MyStore::new);
@@ -241,7 +241,7 @@ public final class IndexModule {
         IndexSearcherWrapper newWrapper(final IndexService indexService);
     }
 
-    public IndexService newIndexService(NodeEnvironment environment, IndexService.ShardStoreDeleter shardStoreDeleter, NodeServicesProvider servicesProvider, MapperRegistry mapperRegistry, IndicesFieldDataCache indicesFieldDataCache,
+    public IndexService newIndexService(NodeEnvironment environment, IndexService.ShardStoreDeleter shardStoreDeleter, NodeServicesProvider servicesProvider, IndicesQueryCache indicesQueryCache, MapperRegistry mapperRegistry, IndicesFieldDataCache indicesFieldDataCache,
                                         IndexingOperationListener... listeners) throws IOException {
         IndexSearcherWrapperFactory searcherWrapperFactory = indexSearcherWrapper.get() == null ? (shard) -> null : indexSearcherWrapper.get();
         IndexEventListener eventListener = freeze();
@@ -263,7 +263,7 @@ public final class IndexModule {
         indexSettings.getScopedSettings().addSettingsUpdateConsumer(IndexStore.INDEX_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING, store::setMaxRate);
         final String queryCacheType = indexSettings.getValue(INDEX_QUERY_CACHE_TYPE_SETTING);
         final BiFunction<IndexSettings, IndicesQueryCache, QueryCache> queryCacheProvider = queryCaches.get(queryCacheType);
-        final QueryCache queryCache = queryCacheProvider.apply(indexSettings, servicesProvider.getIndicesQueryCache());
+        final QueryCache queryCache = queryCacheProvider.apply(indexSettings, indicesQueryCache);
         return new IndexService(indexSettings, environment, new SimilarityService(indexSettings, similarities), shardStoreDeleter, analysisRegistry, engineFactory.get(),
                 servicesProvider, queryCache, store, eventListener, searcherWrapperFactory, mapperRegistry, indicesFieldDataCache, listeners);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index 543a5fc..ffac5d2 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -321,8 +321,7 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
                     warmer.warm(searcher, shard, IndexService.this.indexSettings, toLevel);
                 }
             };
-
-            store = new Store(shardId, this.indexSettings, indexStore.newDirectoryService(path), lock, new StoreCloseListener(shardId, canDeleteShardContent, () -> nodeServicesProvider.getIndicesQueryCache().onClose(shardId)));
+            store = new Store(shardId, this.indexSettings, indexStore.newDirectoryService(path), lock, new StoreCloseListener(shardId, canDeleteShardContent, () -> eventListener.onStoreClosed(shardId)));
             if (useShadowEngine(primary, indexSettings)) {
                 indexShard = new ShadowIndexShard(shardId, this.indexSettings, path, store, indexCache, mapperService, similarityService, indexFieldData, engineFactory, eventListener, searcherWrapper, nodeServicesProvider, searchSlowLog, engineWarmer); // no indexing listeners - shadow  engines don't index
             } else {
@@ -691,7 +690,7 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
         }
 
         boolean mustReschedule() {
-            // don't re-schedule if its closed or if we dont' have a single shard here..., we are done
+            // don't re-schedule if its closed or if we don't have a single shard here..., we are done
             return indexService.closed.get() == false
                 && closed.get() == false && interval.millis() > 0;
         }
diff --git a/core/src/main/java/org/elasticsearch/index/IndexSettings.java b/core/src/main/java/org/elasticsearch/index/IndexSettings.java
index da15d29..b17b8ab 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexSettings.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexSettings.java
@@ -329,7 +329,7 @@ public final class IndexSettings {
     public boolean isShadowReplicaIndex() { return isShadowReplicaIndex; }
 
     /**
-     * Returns the node settings. The settings retured from {@link #getSettings()} are a merged version of the
+     * Returns the node settings. The settings returned from {@link #getSettings()} are a merged version of the
      * index settings and the node settings where node settings are overwritten by index settings.
      */
     public Settings getNodeSettings() {
diff --git a/core/src/main/java/org/elasticsearch/index/NodeServicesProvider.java b/core/src/main/java/org/elasticsearch/index/NodeServicesProvider.java
index 4bb2521..fa24535 100644
--- a/core/src/main/java/org/elasticsearch/index/NodeServicesProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/NodeServicesProvider.java
@@ -23,7 +23,6 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.cache.query.IndicesQueryCache;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -36,7 +35,6 @@ import org.elasticsearch.threadpool.ThreadPool;
 public final class NodeServicesProvider {
 
     private final ThreadPool threadPool;
-    private final IndicesQueryCache indicesQueryCache;
     private final BigArrays bigArrays;
     private final Client client;
     private final IndicesQueriesRegistry indicesQueriesRegistry;
@@ -44,9 +42,8 @@ public final class NodeServicesProvider {
     private final CircuitBreakerService circuitBreakerService;
 
     @Inject
-    public NodeServicesProvider(ThreadPool threadPool, IndicesQueryCache indicesQueryCache, BigArrays bigArrays, Client client, ScriptService scriptService, IndicesQueriesRegistry indicesQueriesRegistry, CircuitBreakerService circuitBreakerService) {
+    public NodeServicesProvider(ThreadPool threadPool, BigArrays bigArrays, Client client, ScriptService scriptService, IndicesQueriesRegistry indicesQueriesRegistry, CircuitBreakerService circuitBreakerService) {
         this.threadPool = threadPool;
-        this.indicesQueryCache = indicesQueryCache;
         this.bigArrays = bigArrays;
         this.client = client;
         this.indicesQueriesRegistry = indicesQueriesRegistry;
@@ -58,10 +55,6 @@ public final class NodeServicesProvider {
         return threadPool;
     }
 
-    public IndicesQueryCache getIndicesQueryCache() {
-        return indicesQueryCache;
-    }
-
     public BigArrays getBigArrays() { return bigArrays; }
 
     public Client getClient() {
diff --git a/core/src/main/java/org/elasticsearch/index/engine/DeleteVersionValue.java b/core/src/main/java/org/elasticsearch/index/engine/DeleteVersionValue.java
index 1bb75a2..5a7f481 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/DeleteVersionValue.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/DeleteVersionValue.java
@@ -22,7 +22,7 @@ package org.elasticsearch.index.engine;
 import org.apache.lucene.util.RamUsageEstimator;
 import org.elasticsearch.index.translog.Translog;
 
-/** Holds a deleted version, which just adds a timestmap to {@link VersionValue} so we know when we can expire the deletion. */
+/** Holds a deleted version, which just adds a timestamp to {@link VersionValue} so we know when we can expire the deletion. */
 
 class DeleteVersionValue extends VersionValue {
     private final long time;
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java b/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java
index 062e79e..3ecae76 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java
@@ -29,7 +29,7 @@ import java.io.IOException;
  * An engine is already closed.
  * <p>
  * Note, the relationship between shard and engine indicates that engine closed is shard closed, and
- * we might get something slipping through the the shard and into the engine while the shard is closing.
+ * we might get something slipping through the shard and into the engine while the shard is closing.
  *
  *
  */
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java b/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java
index d17a9fd..04e1ff1 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java
@@ -321,7 +321,7 @@ public final class OrdinalsBuilder implements Closeable {
     }
 
     /**
-     * Retruns the current ordinal or <tt>0</tt> if this build has not been advanced via
+     * Returns the current ordinal or <tt>0</tt> if this build has not been advanced via
      * {@link #nextOrdinal()}.
      */
     public long currentOrdinal() {
@@ -457,7 +457,7 @@ public final class OrdinalsBuilder implements Closeable {
      * This method iterates all terms in the given {@link TermsEnum} and
      * associates each terms ordinal with the terms documents. The caller must
      * exhaust the returned {@link BytesRefIterator} which returns all values
-     * where the first returned value is associted with the ordinal <tt>1</tt>
+     * where the first returned value is associated with the ordinal <tt>1</tt>
      * etc.
      * <p>
      * If the {@link TermsEnum} contains prefix coded numerical values the terms
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/ExtractQueryTermsService.java b/core/src/main/java/org/elasticsearch/index/percolator/ExtractQueryTermsService.java
index 7dc6e51..2dfc940 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/ExtractQueryTermsService.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/ExtractQueryTermsService.java
@@ -85,7 +85,7 @@ public final class ExtractQueryTermsService {
     /**
      * Extracts all query terms from the provided query and adds it to specified list.
      *
-     * From boolean query with no should clauses or phrase queries only the the longest term are selected,
+     * From boolean query with no should clauses or phrase queries only the longest term are selected,
      * since that those terms are likely to be the rarest. Boolean query's must_not clauses are always ignored.
      *
      * If from part of the query, no query terms can be extracted then term extraction is stopped and
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java
index f927a42..f4c899d 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java
@@ -41,7 +41,7 @@ public class PercolateStats implements Streamable, ToXContent {
     private long numQueries;
 
     /**
-     * Noop constructor for serialazation purposes.
+     * Noop constructor for serialization purposes.
      */
     public PercolateStats() {
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
index 550ffe8..a0fc9ee 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
@@ -165,7 +165,7 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
 
     /**
      * Sets the minimum number of high frequent query terms that need to match in order to
-     * produce a hit when there are no low frequen terms.
+     * produce a hit when there are no low frequent terms.
      */
     public CommonTermsQueryBuilder highFreqMinimumShouldMatch(String highFreqMinimumShouldMatch) {
         this.highFreqMinimumShouldMatch = highFreqMinimumShouldMatch;
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
index 2031abc..4b24278 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
@@ -154,7 +154,7 @@ public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> i
     }
 
     private FuzzyQueryBuilder() {
-        // for protoype
+        // for prototype
         this.fieldName = null;
         this.value = null;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index 9f1187e..a0246f1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -251,7 +251,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         GeoPoint luceneBottomRight = new GeoPoint(bottomRight);
         if (GeoValidationMethod.isCoerce(validationMethod)) {
             // Special case: if the difference between the left and right is 360 and the right is greater than the left, we are asking for
-            // the complete longitude range so need to set longitude to the complete longditude range
+            // the complete longitude range so need to set longitude to the complete longitude range
             double right = luceneBottomRight.getLon();
             double left = luceneTopLeft.getLon();
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index ff5b9d9..43f55dc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -197,7 +197,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         return this.optimizeBbox;
     }
 
-    /** Set validaton method for geo coordinates. */
+    /** Set validation method for geo coordinates. */
     public void setValidationMethod(GeoValidationMethod method) {
         this.validationMethod = method;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
index 8b4af09..0e98b20 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
@@ -634,7 +634,7 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
     }
 
     /**
-     * The analyzer that will be used to analyze the text. Defaults to the analyzer associated with the fied.
+     * The analyzer that will be used to analyze the text. Defaults to the analyzer associated with the field.
      */
     public MoreLikeThisQueryBuilder analyzer(String analyzer) {
         this.analyzer = analyzer;
@@ -703,7 +703,7 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
      * Converts an array of String ids to and Item[].
      * @param ids the ids to convert
      * @return the new items array
-     * @deprecated construct the items array externaly and use it in the constructor / setter
+     * @deprecated construct the items array externally and use it in the constructor / setter
      */
     @Deprecated
     public static Item[] ids(String... ids) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
index f7c1945..bc629d8 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
@@ -29,7 +29,7 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for the The More Like This Query (MLT Query) which finds documents that are "like" a given set of documents.
+ * Parser for the More Like This Query (MLT Query) which finds documents that are "like" a given set of documents.
  *
  * The documents are provided as a set of strings and/or a list of {@link Item}.
  */
diff --git a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
index dbe4de5..17ba74a 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
@@ -284,7 +284,7 @@ public class MultiMatchQuery extends MatchQuery {
             } catch (RuntimeException ex) {
                 // we can't parse it just use the incoming value -- it will
                 // just have a DF of 0 at the end of the day and will be ignored
-                // Note that this is like lenient = true allways
+                // Note that this is like lenient = true always
             }
             return new Term(field, value);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexEventListener.java b/core/src/main/java/org/elasticsearch/index/shard/IndexEventListener.java
index 8d3523a..f5c6dca 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexEventListener.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexEventListener.java
@@ -177,4 +177,12 @@ public interface IndexEventListener {
      */
     default void beforeIndexAddedToCluster(Index index, Settings indexSettings) {
     }
+
+    /**
+     * Called when the given shards store is closed. The store is closed once all resource have been released on the store.
+     * This implies that all index readers are closed and no recoveries are running.
+     *
+     * @param shardId the shard ID the store belongs to
+     */
+    default void onStoreClosed(ShardId shardId) {}
 }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index f2f1add..0f47eec 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -58,7 +58,6 @@ import org.elasticsearch.index.SearchSlowLog;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.cache.bitset.ShardBitsetFilterCache;
-import org.elasticsearch.index.cache.query.QueryCacheStats;
 import org.elasticsearch.index.cache.request.ShardRequestCache;
 import org.elasticsearch.index.codec.CodecService;
 import org.elasticsearch.index.engine.CommitStats;
@@ -105,7 +104,6 @@ import org.elasticsearch.index.translog.TranslogStats;
 import org.elasticsearch.index.warmer.ShardIndexWarmerService;
 import org.elasticsearch.index.warmer.WarmerStats;
 import org.elasticsearch.indices.IndexingMemoryController;
-import org.elasticsearch.indices.cache.query.IndicesQueryCache;
 import org.elasticsearch.indices.recovery.RecoveryFailedException;
 import org.elasticsearch.indices.recovery.RecoveryState;
 import org.elasticsearch.percolator.PercolatorService;
@@ -154,7 +152,6 @@ public class IndexShard extends AbstractIndexShardComponent {
     private final SimilarityService similarityService;
     private final EngineConfig engineConfig;
     private final TranslogConfig translogConfig;
-    private final IndicesQueryCache indicesQueryCache;
     private final IndexEventListener indexEventListener;
     private final IndexSettings idxSettings;
 
@@ -227,7 +224,6 @@ public class IndexShard extends AbstractIndexShardComponent {
         this.getService = new ShardGetService(indexSettings, this, mapperService);
         this.searchService = new ShardSearchStats(slowLog);
         this.shardWarmerService = new ShardIndexWarmerService(shardId, indexSettings);
-        this.indicesQueryCache = provider.getIndicesQueryCache();
         this.shardQueryCache = new ShardRequestCache(shardId, indexSettings);
         this.shardFieldData = new ShardFieldData();
         this.indexFieldDataService = indexFieldDataService;
@@ -652,10 +648,6 @@ public class IndexShard extends AbstractIndexShardComponent {
         return shardWarmerService.stats();
     }
 
-    public QueryCacheStats queryCacheStats() {
-        return indicesQueryCache.getStats(shardId);
-    }
-
     public FieldDataStats fieldDataStats(String... fields) {
         return shardFieldData.stats(fields);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/ShardPath.java b/core/src/main/java/org/elasticsearch/index/shard/ShardPath.java
index e870057..3d6fbf0 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/ShardPath.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/ShardPath.java
@@ -176,7 +176,7 @@ public final class ShardPath {
             totFreeSpace += nodePath.fileStore.getUsableSpace();
         }
 
-        // Very rough heurisic of how much disk space we expect the shard will use over its lifetime, the max of current average
+        // Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average
         // shard size across the cluster and 5% of the total available free space on this node:
         long estShardSizeInBytes = Math.max(avgShardSizeInBytes, (long) (totFreeSpace/20.0));
 
@@ -215,7 +215,7 @@ public final class ShardPath {
             // TODO: this is a hack!!  We should instead keep track of incoming (relocated) shards since we know
             // how large they will be once they're done copying, instead of a silly guess for such cases:
 
-            // Very rough heurisic of how much disk space we expect the shard will use over its lifetime, the max of current average
+            // Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average
             // shard size across the cluster and 5% of the total available free space on this node:
             long estShardSizeInBytes = Math.max(avgShardSizeInBytes, (long) (totFreeSpace/20.0));
 
diff --git a/core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java b/core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java
index aaa30c1..e057349 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/StoreRecovery.java
@@ -64,7 +64,7 @@ final class StoreRecovery {
      * @param indexShard the index shard instance to recovery the shard into
      * @param indexShouldExists <code>true</code> iff the index should exist on disk ie. has the shard been allocated previously on the shards store.
      * @param localNode the reference to the local node
-     * @return  <code>true</code> if the the shard has been recovered successfully, <code>false</code> if the recovery
+     * @return  <code>true</code> if the shard has been recovered successfully, <code>false</code> if the recovery
      * has been ignored due to a concurrent modification of if the clusters state has changed due to async updates.
      * @see Store
      */
@@ -86,7 +86,7 @@ final class StoreRecovery {
      * previously created index snapshot into an existing initializing shard.
      * @param indexShard the index shard instance to recovery the snapshot from
      * @param repository the repository holding the physical files the shard should be recovered from
-     * @return <code>true</code> if the the shard has been recovered successfully, <code>false</code> if the recovery
+     * @return <code>true</code> if the shard has been recovered successfully, <code>false</code> if the recovery
      * has been ignored due to a concurrent modification of if the clusters state has changed due to async updates.
      */
     boolean recoverFromRepository(final IndexShard indexShard, IndexShardRepository repository, DiscoveryNode localNode) {
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index c2a134b..604ac1b 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -728,7 +728,7 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
     }
 
     /**
-     * This is a BWC layer to ensure we update the snapshots metdata with the corresponding hashes before we compare them.
+     * This is a BWC layer to ensure we update the snapshots metadata with the corresponding hashes before we compare them.
      * The new logic for StoreFileMetaData reads the entire <tt>.si</tt> and <tt>segments.n</tt> files to strengthen the
      * comparison of the files on a per-segment / per-commit level.
      */
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
index 8f05572..1e0d55f 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
@@ -42,7 +42,7 @@ import static java.util.Collections.unmodifiableMap;
 /**
  * Contains information about all snapshot for the given shard in repository
  * <p>
- * This class is used to find files that were already snapshoted and clear out files that no longer referenced by any
+ * This class is used to find files that were already snapshotted and clear out files that no longer referenced by any
  * snapshots
  */
 public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, ToXContent, FromXContentBuilder<BlobStoreIndexShardSnapshots> {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index 7579f55..5a4438f 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -97,7 +97,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     /*
      * TODO
      *  - we might need something like a deletion policy to hold on to more than one translog eventually (I think sequence IDs needs this) but we can refactor as we go
-     *  - use a simple BufferedOuputStream to write stuff and fold BufferedTranslogWriter into it's super class... the tricky bit is we need to be able to do random access reads even from the buffer
+     *  - use a simple BufferedOutputStream to write stuff and fold BufferedTranslogWriter into it's super class... the tricky bit is we need to be able to do random access reads even from the buffer
      *  - we need random exception on the FileSystem API tests for all this.
      *  - we need to page align the last write before we sync, we can take advantage of ensureSynced for this since we might have already fsynced far enough
      */
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
index f14d708..b94ef19 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
@@ -55,8 +55,6 @@ import org.elasticsearch.index.mapper.internal.VersionFieldMapper;
 import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.percolator.PercolatorFieldMapper;
-import org.elasticsearch.indices.cache.query.IndicesQueryCache;
-import org.elasticsearch.indices.cache.request.IndicesRequestCache;
 import org.elasticsearch.indices.cluster.IndicesClusterStateService;
 import org.elasticsearch.indices.flush.SyncedFlushService;
 import org.elasticsearch.indices.mapper.MapperRegistry;
@@ -160,8 +158,6 @@ public class IndicesModule extends AbstractModule {
         bind(IndicesStore.class).asEagerSingleton();
         bind(IndicesClusterStateService.class).asEagerSingleton();
         bind(SyncedFlushService.class).asEagerSingleton();
-        bind(IndicesQueryCache.class).asEagerSingleton();
-        bind(IndicesRequestCache.class).asEagerSingleton();
         bind(TransportNodesListShardStoreMetaData.class).asEagerSingleton();
         bind(IndicesTTLService.class).asEagerSingleton();
         bind(UpdateHelper.class).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index 8764119..7b2bc89 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -71,6 +71,8 @@ import org.elasticsearch.index.shard.IndexingStats;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
+import org.elasticsearch.indices.cache.query.IndicesQueryCache;
+import org.elasticsearch.indices.cache.request.IndicesRequestCache;
 import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
 import org.elasticsearch.indices.mapper.MapperRegistry;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
@@ -124,6 +126,8 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
     private final MapperRegistry mapperRegistry;
     private final IndexingMemoryController indexingMemoryController;
     private final TimeValue cleanInterval;
+    private final IndicesRequestCache indicesRequestCache;
+    private final IndicesQueryCache indicesQueryCache;
 
     @Override
     protected void doStart() {
@@ -146,6 +150,8 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         this.indicesQueriesRegistry = indicesQueriesRegistry;
         this.clusterService = clusterService;
         this.indexNameExpressionResolver = indexNameExpressionResolver;
+        this.indicesRequestCache = new IndicesRequestCache(settings, threadPool);
+        this.indicesQueryCache = new IndicesQueryCache(settings);
         this.mapperRegistry = mapperRegistry;
         clusterSettings.addSettingsUpdateConsumer(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING, indexStoreConfig::setRateLimitingType);
         clusterSettings.addSettingsUpdateConsumer(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING, indexStoreConfig::setRateLimitingThrottle);
@@ -196,7 +202,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
 
     @Override
     protected void doClose() {
-        IOUtils.closeWhileHandlingException(analysisRegistry, indexingMemoryController, indicesFieldDataCache, fieldDataCacheCleaner);
+        IOUtils.closeWhileHandlingException(analysisRegistry, indexingMemoryController, indicesFieldDataCache, fieldDataCacheCleaner, indicesRequestCache, indicesQueryCache);
     }
 
     /**
@@ -247,7 +253,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
                     if (indexShard.routingEntry() == null) {
                         continue;
                     }
-                    IndexShardStats indexShardStats = new IndexShardStats(indexShard.shardId(), new ShardStats[] { new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indexShard, flags), indexShard.commitStats()) });
+                    IndexShardStats indexShardStats = new IndexShardStats(indexShard.shardId(), new ShardStats[] { new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indicesQueryCache, indexShard, flags), indexShard.commitStats()) });
                     if (!statsByShard.containsKey(indexService.index())) {
                         statsByShard.put(indexService.index(), arrayAsArrayList(indexShardStats));
                     } else {
@@ -348,10 +354,17 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         for (IndexEventListener listener : builtInListeners) {
             indexModule.addIndexEventListener(listener);
         }
+        final IndexEventListener onStoreClose = new IndexEventListener() {
+            @Override
+            public void onStoreClosed(ShardId shardId) {
+                indicesQueryCache.onClose(shardId);
+            }
+        };
+        indexModule.addIndexEventListener(onStoreClose);
         indexModule.addIndexEventListener(oldShardsStats);
         final IndexEventListener listener = indexModule.freeze();
         listener.beforeIndexCreated(index, idxSettings.getSettings());
-        final IndexService indexService = indexModule.newIndexService(nodeEnv, this, nodeServicesProvider, mapperRegistry, indicesFieldDataCache, indexingMemoryController);
+        final IndexService indexService = indexModule.newIndexService(nodeEnv, this, nodeServicesProvider, indicesQueryCache, mapperRegistry, indicesFieldDataCache, indexingMemoryController);
         boolean success = false;
         try {
             assert indexService.getIndexEventListener() == listener;
@@ -420,6 +433,14 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         return circuitBreakerService;
     }
 
+    public IndicesRequestCache getIndicesRequestCache() {
+        return indicesRequestCache;
+    }
+
+    public IndicesQueryCache getIndicesQueryCache() {
+        return indicesQueryCache;
+    }
+
     static class OldShardsStats implements IndexEventListener {
 
         final SearchStats searchStats = new SearchStats();
@@ -515,7 +536,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
             if (success == false) {
                 addPendingDelete(index, indexSettings);
             }
-            // this is a pure protection to make sure this index doesn't get re-imported as a dangeling index.
+            // this is a pure protection to make sure this index doesn't get re-imported as a dangling index.
             // we should in the future rather write a tombstone rather than wiping the metadata.
             MetaDataStateFormat.deleteMetaState(nodeEnv.indexPaths(index.getName()));
         }
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java b/core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java
index 58c2cd5..718f4db 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java
@@ -64,7 +64,6 @@ public class IndicesQueryCache extends AbstractComponent implements QueryCache,
     // See onDocIdSetEviction for more info
     private final Map<Object, StatsAndCount> stats2 = new IdentityHashMap<>();
 
-    @Inject
     public IndicesQueryCache(Settings settings) {
         super(settings);
         final ByteSizeValue size = INDICES_CACHE_QUERY_SIZE_SETTING.get(settings);
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index 36ac787..d58c1c1 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -45,6 +45,8 @@ import org.elasticsearch.common.unit.MemorySizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.IndexSettings;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.search.internal.SearchContext;
@@ -53,6 +55,7 @@ import org.elasticsearch.search.query.QueryPhase;
 import org.elasticsearch.search.query.QuerySearchResult;
 import org.elasticsearch.threadpool.ThreadPool;
 
+import java.io.Closeable;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.EnumSet;
@@ -74,7 +77,7 @@ import java.util.concurrent.TimeUnit;
  * There are still several TODOs left in this class, some easily addressable, some more complex, but the support
  * is functional.
  */
-public class IndicesRequestCache extends AbstractComponent implements RemovalListener<IndicesRequestCache.Key, IndicesRequestCache.Value> {
+public class IndicesRequestCache extends AbstractComponent implements RemovalListener<IndicesRequestCache.Key, IndicesRequestCache.Value>, Closeable {
 
     /**
      * A setting to enable or disable request caching on an index level. Its dynamic by default
@@ -89,7 +92,6 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
     private static final Set<SearchType> CACHEABLE_SEARCH_TYPES = EnumSet.of(SearchType.QUERY_THEN_FETCH, SearchType.QUERY_AND_FETCH);
 
     private final ThreadPool threadPool;
-    private final ClusterService clusterService;
 
     private final TimeValue cleanInterval;
     private final Reaper reaper;
@@ -104,18 +106,13 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
     private volatile Cache<Key, Value> cache;
 
-    @Inject
-    public IndicesRequestCache(Settings settings, ClusterService clusterService, ThreadPool threadPool) {
+    public IndicesRequestCache(Settings settings, ThreadPool threadPool) {
         super(settings);
-        this.clusterService = clusterService;
         this.threadPool = threadPool;
         this.cleanInterval = INDICES_CACHE_REQUEST_CLEAN_INTERVAL.get(settings);
-
         this.size = INDICES_CACHE_QUERY_SIZE.get(settings);
-
         this.expire = INDICES_CACHE_QUERY_EXPIRE.exists(settings) ? INDICES_CACHE_QUERY_EXPIRE.get(settings) : null;
         buildCache();
-
         this.reaper = new Reaper();
         threadPool.schedule(cleanInterval, ThreadPool.Names.SAME, reaper);
     }
@@ -123,10 +120,8 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
     private void buildCache() {
         long sizeInBytes = size.bytes();
-
         CacheBuilder<Key, Value> cacheBuilder = CacheBuilder.<Key, Value>builder()
                 .setMaximumWeight(sizeInBytes).weigher((k, v) -> k.ramBytesUsed() + v.ramBytesUsed()).removalListener(this);
-        // cacheBuilder.concurrencyLevel(concurrencyLevel);
 
         if (expire != null) {
             cacheBuilder.setExpireAfterAccess(TimeUnit.MILLISECONDS.toNanos(expire.millis()));
@@ -135,6 +130,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         cache = cacheBuilder.build();
     }
 
+    @Override
     public void close() {
         reaper.close();
         cache.invalidateAll();
@@ -174,21 +170,17 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         if (!CACHEABLE_SEARCH_TYPES.contains(context.searchType())) {
             return false;
         }
-
-        IndexMetaData index = clusterService.state().getMetaData().index(request.index());
-        if (index == null) { // in case we didn't yet have the cluster state, or it just got deleted
-            return false;
-        }
+        IndexSettings settings = context.indexShard().getIndexSettings();
         // if not explicitly set in the request, use the index setting, if not, use the request
         if (request.requestCache() == null) {
-            if (INDEX_CACHE_REQUEST_ENABLED_SETTING.get(index.getSettings()) == false) {
+            if (settings.getValue(INDEX_CACHE_REQUEST_ENABLED_SETTING) == false) {
                 return false;
             }
-        } else if (!request.requestCache()) {
+        } else if (request.requestCache() == false) {
             return false;
         }
         // if the reader is not a directory reader, we can't get the version from it
-        if (!(context.searcher().getIndexReader() instanceof DirectoryReader)) {
+        if ((context.searcher().getIndexReader() instanceof DirectoryReader) == false) {
             return false;
         }
         // if now in millis is used (or in the future, a more generic "isDeterministic" flag
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
index 26c288c..adb775d 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
@@ -403,7 +403,7 @@ public class RecoverySourceHandler {
             } catch (IllegalIndexShardStateException e) {
                 // we can ignore this exception since, on the other node, when it moved to phase3
                 // it will also send shard started, which might cause the index shard we work against
-                // to move be closed by the time we get to the the relocated method
+                // to move be closed by the time we get to the relocated method
             }
         }
         stopWatch.stop();
diff --git a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
index 6d225af..d0aec81 100644
--- a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
+++ b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
@@ -68,7 +68,7 @@ import java.util.concurrent.atomic.AtomicInteger;
  */
 public class IndicesStore extends AbstractComponent implements ClusterStateListener, Closeable {
 
-    // TODO this class can be foled into either IndicesService and partially into IndicesClusterStateService there is no need for a seperate public service
+    // TODO this class can be foled into either IndicesService and partially into IndicesClusterStateService there is no need for a separate public service
     public static final Setting<TimeValue> INDICES_STORE_DELETE_SHARD_TIMEOUT = Setting.positiveTimeSetting("indices.store.delete.shard.timeout", new TimeValue(30, TimeUnit.SECONDS), false, Setting.Scope.CLUSTER);
     public static final String ACTION_SHARD_EXISTS = "internal:index/shard/exists";
     private static final EnumSet<IndexShardState> ACTIVE_STATES = EnumSet.of(IndexShardState.STARTED, IndexShardState.RELOCATED);
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java b/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
index 4b0f6ac..23d36c9 100644
--- a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
+++ b/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
@@ -254,7 +254,7 @@ public final class IngestDocument {
      * If the path identifies a list, the value will be appended to the existing list.
      * If the path identifies a scalar, the scalar will be converted to a list and
      * the provided value will be added to the newly created list.
-     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
+     * Supports multiple values too provided in forms of list, in that case all the values will be appended to the
      * existing (or newly created) list.
      * @param path The path within the document in dot-notation
      * @param value The value or values to append to the existing ones
@@ -270,7 +270,7 @@ public final class IngestDocument {
      * If the path identifies a list, the value will be appended to the existing list.
      * If the path identifies a scalar, the scalar will be converted to a list and
      * the provided value will be added to the newly created list.
-     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
+     * Supports multiple values too provided in forms of list, in that case all the values will be appended to the
      * existing (or newly created) list.
      * @param fieldPathTemplate Resolves to the path with dot-notation within the document
      * @param valueSource The value source that will produce the value or values to append to the existing ones
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 88d2176..ee523e9 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -70,9 +70,7 @@ import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.analysis.AnalysisModule;
 import org.elasticsearch.indices.breaker.CircuitBreakerModule;
-import org.elasticsearch.indices.cache.query.IndicesQueryCache;
 import org.elasticsearch.indices.cluster.IndicesClusterStateService;
-import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
 import org.elasticsearch.indices.store.IndicesStore;
 import org.elasticsearch.indices.ttl.IndicesTTLService;
 import org.elasticsearch.monitor.MonitorService;
@@ -391,7 +389,6 @@ public class Node implements Closeable {
         toClose.add(injector.getInstance(IndicesTTLService.class));
         toClose.add(injector.getInstance(IndicesService.class));
         // close filter/fielddata caches after indices
-        toClose.add(injector.getInstance(IndicesQueryCache.class));
         toClose.add(injector.getInstance(IndicesStore.class));
         toClose.add(() ->stopWatch.stop().start("routing"));
         toClose.add(injector.getInstance(RoutingService.class));
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
index f73c8f3..7572ebc 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.LeafReaderContext;
@@ -53,7 +52,6 @@ import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -116,15 +114,13 @@ public class PercolateContext extends SearchContext {
     private final QueryShardContext queryShardContext;
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
     private SearchLookup searchLookup;
-    private final FetchPhase fetchPhase;
 
     public PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, IndexShard indexShard,
-            IndexService indexService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ScriptService scriptService,
-            Query aliasFilter, ParseFieldMatcher parseFieldMatcher, FetchPhase fetchPhase) {
+                            IndexService indexService, PageCacheRecycler pageCacheRecycler,
+                            BigArrays bigArrays, ScriptService scriptService, Query aliasFilter, ParseFieldMatcher parseFieldMatcher) {
         super(parseFieldMatcher);
         this.indexShard = indexShard;
         this.indexService = indexService;
-        this.fetchPhase = fetchPhase;
         this.fieldDataService = indexService.fieldData();
         this.mapperService = indexService.mapperService();
         this.searchShardTarget = searchShardTarget;
@@ -158,7 +154,6 @@ public class PercolateContext extends SearchContext {
         this.numberOfShards = 0;
         this.onlyCount = true;
         this.queryShardContext = queryShardContext;
-        this.fetchPhase = null;
     }
 
     public IndexSearcher docSearcher() {
@@ -650,11 +645,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return mapperService().fullName(name);
     }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
index 3b1b627..3fb2b34 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.memory.ExtendedMemoryIndex;
@@ -71,7 +70,6 @@ import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
 import org.elasticsearch.search.aggregations.pipeline.SiblingPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.highlight.HighlightField;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.internal.SearchContext;
@@ -100,25 +98,23 @@ public class PercolatorService extends AbstractComponent implements Releasable {
     private final HighlightPhase highlightPhase;
     private final AggregationPhase aggregationPhase;
     private final PageCacheRecycler pageCacheRecycler;
+    private final ParseFieldMatcher parseFieldMatcher;
     private final CloseableThreadLocal<MemoryIndex> cache;
     private final IndexNameExpressionResolver indexNameExpressionResolver;
     private final PercolateDocumentParser percolateDocumentParser;
 
     private final PercolatorIndex single;
     private final PercolatorIndex multi;
-    private final ParseFieldMatcher parseFieldMatcher;
-    private final FetchPhase fetchPhase;
 
     @Inject
     public PercolatorService(Settings settings, IndexNameExpressionResolver indexNameExpressionResolver, IndicesService indicesService,
                              PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
                              HighlightPhase highlightPhase, ClusterService clusterService,
                              AggregationPhase aggregationPhase, ScriptService scriptService,
-                             PercolateDocumentParser percolateDocumentParser, FetchPhase fetchPhase) {
+                             PercolateDocumentParser percolateDocumentParser) {
         super(settings);
         this.indexNameExpressionResolver = indexNameExpressionResolver;
         this.percolateDocumentParser = percolateDocumentParser;
-        this.fetchPhase = fetchPhase;
         this.parseFieldMatcher = new ParseFieldMatcher(settings);
         this.indicesService = indicesService;
         this.pageCacheRecycler = pageCacheRecycler;
@@ -145,7 +141,7 @@ public class PercolatorService extends AbstractComponent implements Releasable {
             long finalCount = 0;
             for (PercolateShardResponse shardResponse : shardResponses) {
                 finalCount += shardResponse.topDocs().totalHits;
-    }
+            }
 
             InternalAggregations reducedAggregations = reduceAggregations(shardResponses);
             return new PercolatorService.ReduceResult(finalCount, reducedAggregations);
@@ -189,10 +185,10 @@ public class PercolatorService extends AbstractComponent implements Releasable {
         );
         Query aliasFilter = percolateIndexService.aliasFilter(percolateIndexService.newQueryShardContext(), filteringAliases);
 
-        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(),
-                request.shardId().id());
-        final PercolateContext context = new PercolateContext(request, searchShardTarget, indexShard, percolateIndexService,
-                pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher, fetchPhase);
+        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(), request.shardId().id());
+        final PercolateContext context = new PercolateContext(
+                request, searchShardTarget, indexShard, percolateIndexService, pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher
+        );
         SearchContext.setCurrent(context);
         try {
             ParsedDocument parsedDocument = percolateDocumentParser.parse(request, context, percolateIndexService.mapperService());
@@ -219,14 +215,15 @@ public class PercolatorService extends AbstractComponent implements Releasable {
             if (context.aggregations() != null) {
                 AggregationContext aggregationContext = new AggregationContext(context);
                 context.aggregations().aggregationContext(aggregationContext);
-                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators();
+
+                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators(aggregationContext);
                 List<Aggregator> aggregatorCollectors = new ArrayList<>(aggregators.length);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (!(aggregators[i] instanceof GlobalAggregator)) {
                         Aggregator aggregator = aggregators[i];
                         aggregatorCollectors.add(aggregator);
+                    }
                 }
-            }
                 context.aggregations().aggregators(aggregators);
                 aggregatorCollector = BucketCollector.wrap(aggregatorCollectors);
                 aggregatorCollector.preCollection();
@@ -248,14 +245,14 @@ public class PercolatorService extends AbstractComponent implements Releasable {
         }
         if (context.percolateQuery() != null || context.aliasFilter() != null) {
             BooleanQuery.Builder bq = new BooleanQuery.Builder();
-                        if (context.percolateQuery() != null) {
+            if (context.percolateQuery() != null) {
                 bq.add(context.percolateQuery(), MUST);
-                        }
+            }
             if (context.aliasFilter() != null) {
                 bq.add(context.aliasFilter(), FILTER);
-                        }
+            }
             builder.setPercolateQuery(bq.build());
-                    }
+        }
         PercolatorQuery percolatorQuery = builder.build();
 
         if (context.isOnlyCount() || context.size() == 0) {
@@ -264,20 +261,20 @@ public class PercolatorService extends AbstractComponent implements Releasable {
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-                        }
+            }
             return new PercolateShardResponse(new TopDocs(collector.getTotalHits(), Lucene.EMPTY_SCORE_DOCS, 0f), Collections.emptyMap(), Collections.emptyMap(), context);
         } else {
             int size = context.size();
             if (size > context.searcher().getIndexReader().maxDoc()) {
                 // prevent easy OOM if more than the total number of docs that exist is requested...
                 size = context.searcher().getIndexReader().maxDoc();
-        }
+            }
             TopScoreDocCollector collector = TopScoreDocCollector.create(size);
             context.searcher().search(percolatorQuery, MultiCollector.wrap(collector, aggregatorCollector));
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-    }
+            }
 
             TopDocs topDocs = collector.topDocs();
             Map<Integer, String> ids = new HashMap<>(topDocs.scoreDocs.length);
@@ -286,7 +283,7 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                 if (context.trackScores() == false) {
                     // No sort or tracking scores was provided, so use special value to indicate to not show the scores:
                     scoreDoc.score = NO_SCORE;
-        }
+                }
 
                 int segmentIdx = ReaderUtil.subIndex(scoreDoc.doc, context.searcher().getIndexReader().leaves());
                 LeafReaderContext atomicReaderContext = context.searcher().getIndexReader().leaves().get(segmentIdx);
@@ -299,18 +296,18 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                     Query query = queriesRegistry.getPercolateQueries().get(new BytesRef(id));
                     context.parsedQuery(new ParsedQuery(query));
                     context.hitContext().cache().clear();
-                                highlightPhase.hitExecute(context, context.hitContext());
+                    highlightPhase.hitExecute(context, context.hitContext());
                     hls.put(scoreDoc.doc, context.hitContext().hit().getHighlightFields());
-                            }
-                        }
-            return new PercolateShardResponse(topDocs, ids, hls, context);
                 }
             }
+            return new PercolateShardResponse(topDocs, ids, hls, context);
+        }
+    }
 
     @Override
     public void close() {
         cache.close();
-        }
+    }
 
     private InternalAggregations reduceAggregations(List<PercolateShardResponse> shardResults) {
         if (shardResults.get(0).aggregations() == null) {
@@ -331,12 +328,12 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                 for (SiblingPipelineAggregator pipelineAggregator : pipelineAggregators) {
                     InternalAggregation newAgg = pipelineAggregator.doReduce(new InternalAggregations(newAggs), new InternalAggregation.ReduceContext(bigArrays, scriptService));
                     newAggs.add(newAgg);
-            }
+                }
                 aggregations = new InternalAggregations(newAggs);
-        }
+            }
         }
         return aggregations;
-        }
+    }
 
     public final static class ReduceResult {
 
diff --git a/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java b/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java
index aeff769..767f6d4 100644
--- a/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java
+++ b/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java
@@ -284,7 +284,7 @@ class InstallPluginCommand extends CliTool.Command {
         List<PluginsService.Bundle> bundles = PluginsService.getPluginBundles(pluginsDir);
 
         // if we aren't isolated, we need to jarhellcheck against any other non-isolated plugins
-        // thats always the first bundle
+        // that's always the first bundle
         if (isolated == false) {
             jars.addAll(bundles.get(0).urls);
         }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
index ac3e616..3e36c5d 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
@@ -159,7 +159,7 @@ public class PluginsService extends AbstractComponent {
             }
         }
 
-        // we don't log jars in lib/ we really shouldnt log modules,
+        // we don't log jars in lib/ we really shouldn't log modules,
         // but for now: just be transparent so we can debug any potential issues
         Set<String> moduleNames = new HashSet<>();
         Set<String> jvmPluginNames = new HashSet<>();
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
index 59812f9..552e6aa 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
@@ -528,7 +528,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
     }
 
     /**
-     * In v2.0.0 we changed the matadata file format
+     * In v2.0.0 we changed the metadata file format
      * @return true if legacy version should be used false otherwise
      */
     public static boolean legacyMetaData(Version version) {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
index ce1e781..f11efec 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
@@ -88,7 +88,7 @@ public class RestNodesInfoAction extends BaseRestHandler {
 
         final NodesInfoRequest nodesInfoRequest = new NodesInfoRequest(nodeIds);
         nodesInfoRequest.timeout(request.param("timeout"));
-        // shortcut, dont do checks if only all is specified
+        // shortcut, don't do checks if only all is specified
         if (metrics.size() == 1 && metrics.contains("_all")) {
             nodesInfoRequest.all();
         } else {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
index 5d9ac11..540dd26 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
@@ -42,7 +42,6 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
 import org.elasticsearch.script.Template;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.util.Map;
@@ -59,14 +58,11 @@ public class RestMultiSearchAction extends BaseRestHandler {
 
     private final boolean allowExplicitIndex;
     private final IndicesQueriesRegistry indicesQueriesRegistry;
-    private final AggregatorParsers aggParsers;
 
 
     @Inject
-    public RestMultiSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry,
-            AggregatorParsers aggParsers) {
+    public RestMultiSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
         super(settings, client);
-        this.aggParsers = aggParsers;
 
         controller.registerHandler(GET, "/_msearch", this);
         controller.registerHandler(POST, "/_msearch", this);
@@ -97,7 +93,7 @@ public class RestMultiSearchAction extends BaseRestHandler {
         IndicesOptions indicesOptions = IndicesOptions.fromRequest(request, multiSearchRequest.indicesOptions());
         parseRequest(multiSearchRequest, RestActions.getRestContent(request), isTemplateRequest, indices, types,
                 request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex, indicesQueriesRegistry,
-                parseFieldMatcher, aggParsers);
+                parseFieldMatcher);
         client.multiSearch(multiSearchRequest, new RestToXContentListener<>(channel));
     }
 
@@ -112,7 +108,7 @@ public class RestMultiSearchAction extends BaseRestHandler {
                                                    @Nullable String routing,
                                                    IndicesOptions indicesOptions,
                                                    boolean allowExplicitIndex, IndicesQueriesRegistry indicesQueriesRegistry,
-            ParseFieldMatcher parseFieldMatcher, AggregatorParsers aggParsers) throws Exception {
+                                                   ParseFieldMatcher parseFieldMatcher) throws Exception {
         XContent xContent = XContentFactory.xContent(data);
         int from = 0;
         int length = data.length();
@@ -193,7 +189,7 @@ public class RestMultiSearchAction extends BaseRestHandler {
             } else {
                 try (XContentParser requestParser = XContentFactory.xContent(slice).createParser(slice)) {
                     queryParseContext.reset(requestParser);
-                    searchRequest.source(SearchSourceBuilder.parseSearchSource(requestParser, queryParseContext, aggParsers));
+                    searchRequest.source(SearchSourceBuilder.parseSearchSource(requestParser, queryParseContext));
                 }
             }
             // move pointers
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
index acd8cab..e58caea 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
@@ -42,7 +42,6 @@ import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.internal.SearchContext;
@@ -63,14 +62,11 @@ import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
 public class RestSearchAction extends BaseRestHandler {
 
     private final IndicesQueriesRegistry queryRegistry;
-    private final AggregatorParsers aggParsers;
 
     @Inject
-    public RestSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry,
-            AggregatorParsers aggParsers) {
+    public RestSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
         super(settings, client);
         this.queryRegistry = queryRegistry;
-        this.aggParsers = aggParsers;
         controller.registerHandler(GET, "/_search", this);
         controller.registerHandler(POST, "/_search", this);
         controller.registerHandler(GET, "/{index}/_search", this);
@@ -88,12 +84,11 @@ public class RestSearchAction extends BaseRestHandler {
     @Override
     public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
         SearchRequest searchRequest;
-        searchRequest = RestSearchAction.parseSearchRequest(queryRegistry, request, parseFieldMatcher, aggParsers);
+        searchRequest = RestSearchAction.parseSearchRequest(queryRegistry, request, parseFieldMatcher);
         client.search(searchRequest, new RestStatusToXContentListener<>(channel));
     }
 
-    public static SearchRequest parseSearchRequest(IndicesQueriesRegistry indicesQueriesRegistry, RestRequest request,
-            ParseFieldMatcher parseFieldMatcher, AggregatorParsers aggParsers) throws IOException {
+    public static SearchRequest parseSearchRequest(IndicesQueriesRegistry indicesQueriesRegistry,  RestRequest request, ParseFieldMatcher parseFieldMatcher) throws IOException {
         String[] indices = Strings.splitStringByCommaToArray(request.param("index"));
         SearchRequest searchRequest = new SearchRequest(indices);
         // get the content, and put it in the body
@@ -112,7 +107,7 @@ public class RestSearchAction extends BaseRestHandler {
                 }
                 builder = null;
             } else {
-                builder = RestActions.getRestSearchSource(restContent, indicesQueriesRegistry, parseFieldMatcher, aggParsers);
+                builder = RestActions.getRestSearchSource(restContent, indicesQueriesRegistry, parseFieldMatcher);
             }
         } else {
             builder = null;
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index d8055ba..9c0bb61 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -40,7 +40,6 @@ import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
@@ -115,13 +114,13 @@ public class RestActions {
     }
 
     public static SearchSourceBuilder getRestSearchSource(BytesReference sourceBytes, IndicesQueriesRegistry queryRegistry,
-            ParseFieldMatcher parseFieldMatcher, AggregatorParsers aggParsers)
+            ParseFieldMatcher parseFieldMatcher)
             throws IOException {
         XContentParser parser = XContentFactory.xContent(sourceBytes).createParser(sourceBytes);
         QueryParseContext queryParseContext = new QueryParseContext(queryRegistry);
         queryParseContext.reset(parser);
         queryParseContext.parseFieldMatcher(parseFieldMatcher);
-        return SearchSourceBuilder.parseSearchSource(parser, queryParseContext, aggParsers);
+        return SearchSourceBuilder.parseSearchSource(parser, queryParseContext);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/script/Script.java b/core/src/main/java/org/elasticsearch/script/Script.java
index dc0e383..fb2226e 100644
--- a/core/src/main/java/org/elasticsearch/script/Script.java
+++ b/core/src/main/java/org/elasticsearch/script/Script.java
@@ -34,24 +34,12 @@ import org.elasticsearch.script.ScriptService.ScriptType;
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.function.Supplier;
 
 /**
  * Script holds all the parameters necessary to compile or find in cache and then execute a script.
  */
 public class Script implements ToXContent, Streamable {
 
-    /**
-     * A {@link Supplier} implementation for use when reading a {@link Script}
-     * using {@link StreamInput#readOptionalStreamable(Supplier)}
-     */
-    public static final Supplier<Script> SUPPLIER = new Supplier<Script>() {
-
-        @Override
-        public Script get() {
-            return new Script();
-        }
-    };
     public static final ScriptType DEFAULT_TYPE = ScriptType.INLINE;
     private static final ScriptParser PARSER = new ScriptParser();
 
@@ -86,7 +74,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Constructor for Script.
-     *
+     * 
      * @param script
      *            The cache key of the script to be compiled/executed. For
      *            inline scripts this is the actual script source code. For
@@ -124,7 +112,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the type.
-     *
+     * 
      * @return The type of script -- inline, indexed, or file.
      */
     public ScriptType getType() {
@@ -133,7 +121,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting language.
-     *
+     * 
      * @return The language of the script to be compiled/executed.
      */
     public String getLang() {
@@ -142,7 +130,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the parameters.
-     *
+     * 
      * @return The map of parameters the script will be executed with.
      */
     public Map<String, Object> getParams() {
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModule.java b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
index 11bc2b5..0a386a4 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModule.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
@@ -66,7 +66,7 @@ public class ScriptModule extends AbstractModule {
     }
 
     /**
-     * This method is called after all modules have been processed but before we actually validate all settings. This allwos the
+     * This method is called after all modules have been processed but before we actually validate all settings. This allows the
      * script extensions to add all their settings.
      */
     public void prepareSettings(SettingsModule settingsModule) {
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptService.java b/core/src/main/java/org/elasticsearch/script/ScriptService.java
index d010465..d21283d 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptService.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptService.java
@@ -247,7 +247,7 @@ public class ScriptService extends AbstractComponent implements Closeable {
             throw new ScriptException("scripts of type [" + script.getType() + "], operation [" + scriptContext.getKey() + "] and lang [" + lang + "] are disabled");
         }
 
-        // TODO: fix this through some API or something, thats wrong
+        // TODO: fix this through some API or something, that's wrong
         // special exception to prevent expressions from compiling as update or mapping scripts
         boolean expression = "expression".equals(script.getLang());
         boolean notSupported = scriptContext.getKey().equals(ScriptContext.Standard.UPDATE.getKey());
diff --git a/core/src/main/java/org/elasticsearch/search/SearchModule.java b/core/src/main/java/org/elasticsearch/search/SearchModule.java
index 6d663a3..739b970 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchModule.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchModule.java
@@ -96,7 +96,6 @@ import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionPar
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.action.SearchServiceTransportAction;
-import org.elasticsearch.search.aggregations.AggregationBinaryParseElement;
 import org.elasticsearch.search.aggregations.AggregationParseElement;
 import org.elasticsearch.search.aggregations.AggregationPhase;
 import org.elasticsearch.search.aggregations.Aggregator;
@@ -128,7 +127,6 @@ import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanc
 import org.elasticsearch.search.aggregations.bucket.range.geodistance.InternalGeoDistance;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.InternalIPv4Range;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.IpRangeParser;
-import org.elasticsearch.search.aggregations.bucket.sampler.DiversifiedSamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.InternalSampler;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.UnmappedSampler;
@@ -209,26 +207,20 @@ import org.elasticsearch.search.controller.SearchPhaseController;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSubPhase;
-import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsFetchSubPhase;
 import org.elasticsearch.search.fetch.matchedqueries.MatchedQueriesFetchSubPhase;
 import org.elasticsearch.search.fetch.parent.ParentFieldSubFetchPhase;
 import org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
 import org.elasticsearch.search.fetch.source.FetchSourceSubPhase;
 import org.elasticsearch.search.fetch.version.VersionFetchSubPhase;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.highlight.Highlighter;
-import org.elasticsearch.search.highlight.HighlighterParseElement;
 import org.elasticsearch.search.highlight.Highlighters;
 import org.elasticsearch.search.query.QueryPhase;
 import org.elasticsearch.search.rescore.QueryRescorerBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.sort.SortParseElement;
 import org.elasticsearch.search.suggest.Suggester;
 import org.elasticsearch.search.suggest.Suggesters;
 
@@ -245,8 +237,8 @@ import java.util.function.Supplier;
  */
 public class SearchModule extends AbstractModule {
 
-    private final Set<Aggregator.Parser> aggParsers = new HashSet<>();
-    private final Set<PipelineAggregator.Parser> pipelineAggParsers = new HashSet<>();
+    private final Set<Class<? extends Aggregator.Parser>> aggParsers = new HashSet<>();
+    private final Set<Class<? extends PipelineAggregator.Parser>> pipelineAggParsers = new HashSet<>();
     private final Highlighters highlighters = new Highlighters();
     private final Suggesters suggesters = new Suggesters();
     /**
@@ -261,8 +253,8 @@ public class SearchModule extends AbstractModule {
      */
     private final List<Supplier<QueryParser<?>>> queryParsers = new ArrayList<>();
     private final Set<Class<? extends FetchSubPhase>> fetchSubPhases = new HashSet<>();
-    private final Set<SignificanceHeuristicParser> heuristicParsers = new HashSet<>();
-    private final Set<MovAvgModel.AbstractModelParser> modelParsers = new HashSet<>();
+    private final Set<Class<? extends SignificanceHeuristicParser>> heuristicParsers = new HashSet<>();
+    private final Set<Class<? extends MovAvgModel.AbstractModelParser>> modelParsers = new HashSet<>();
 
     private final Settings settings;
     private final NamedWriteableRegistry namedWriteableRegistry;
@@ -307,11 +299,11 @@ public class SearchModule extends AbstractModule {
         fetchSubPhases.add(subPhase);
     }
 
-    public void registerHeuristicParser(SignificanceHeuristicParser parser) {
+    public void registerHeuristicParser(Class<? extends SignificanceHeuristicParser> parser) {
         heuristicParsers.add(parser);
     }
 
-    public void registerModelParser(MovAvgModel.AbstractModelParser parser) {
+    public void registerModelParser(Class<? extends MovAvgModel.AbstractModelParser> parser) {
         modelParsers.add(parser);
     }
 
@@ -320,22 +312,21 @@ public class SearchModule extends AbstractModule {
      *
      * @param parser The parser for the custom aggregator.
      */
-    public void registerAggregatorParser(Aggregator.Parser parser) {
+    public void registerAggregatorParser(Class<? extends Aggregator.Parser> parser) {
         aggParsers.add(parser);
     }
 
-    public void registerPipelineParser(PipelineAggregator.Parser parser) {
+    public void registerPipelineParser(Class<? extends PipelineAggregator.Parser> parser) {
         pipelineAggParsers.add(parser);
     }
 
     @Override
     protected void configure() {
-        IndicesQueriesRegistry indicesQueriesRegistry = buildQueryParserRegistry();
-        bind(IndicesQueriesRegistry.class).toInstance(indicesQueriesRegistry);
         configureSearch();
-        configureAggs(indicesQueriesRegistry);
+        configureAggs();
         configureHighlighters();
         configureSuggesters();
+        bind(IndicesQueriesRegistry.class).toInstance(buildQueryParserRegistry());
         configureFetchSubPhase();
         configureShapes();
         configureRescorers();
@@ -380,67 +371,75 @@ public class SearchModule extends AbstractModule {
        highlighters.bind(binder());
     }
 
-    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
-
-        MovAvgModelParserMapper movAvgModelParserMapper = new MovAvgModelParserMapper(modelParsers);
-
-        SignificanceHeuristicParserMapper significanceHeuristicParserMapper = new SignificanceHeuristicParserMapper(heuristicParsers);
-
-        registerAggregatorParser(new AvgParser());
-        registerAggregatorParser(new SumParser());
-        registerAggregatorParser(new MinParser());
-        registerAggregatorParser(new MaxParser());
-        registerAggregatorParser(new StatsParser());
-        registerAggregatorParser(new ExtendedStatsParser());
-        registerAggregatorParser(new ValueCountParser());
-        registerAggregatorParser(new PercentilesParser());
-        registerAggregatorParser(new PercentileRanksParser());
-        registerAggregatorParser(new CardinalityParser());
-        registerAggregatorParser(new GlobalParser());
-        registerAggregatorParser(new MissingParser());
-        registerAggregatorParser(new FilterParser());
-        registerAggregatorParser(new FiltersParser(indicesQueriesRegistry));
-        registerAggregatorParser(new SamplerParser());
-        registerAggregatorParser(new DiversifiedSamplerParser());
-        registerAggregatorParser(new TermsParser());
-        registerAggregatorParser(new SignificantTermsParser(significanceHeuristicParserMapper, indicesQueriesRegistry));
-        registerAggregatorParser(new RangeParser());
-        registerAggregatorParser(new DateRangeParser());
-        registerAggregatorParser(new IpRangeParser());
-        registerAggregatorParser(new HistogramParser());
-        registerAggregatorParser(new DateHistogramParser());
-        registerAggregatorParser(new GeoDistanceParser());
-        registerAggregatorParser(new GeoHashGridParser());
-        registerAggregatorParser(new NestedParser());
-        registerAggregatorParser(new ReverseNestedParser());
-        registerAggregatorParser(new TopHitsParser(new SortParseElement(), new FetchSourceParseElement(), new HighlighterParseElement(),
-                new FieldDataFieldsParseElement(), new ScriptFieldsParseElement(), new FieldsParseElement()));
-        registerAggregatorParser(new GeoBoundsParser());
-        registerAggregatorParser(new GeoCentroidParser());
-        registerAggregatorParser(new ScriptedMetricParser());
-        registerAggregatorParser(new ChildrenParser());
-
-        registerPipelineParser(new DerivativeParser());
-        registerPipelineParser(new MaxBucketParser());
-        registerPipelineParser(new MinBucketParser());
-        registerPipelineParser(new AvgBucketParser());
-        registerPipelineParser(new SumBucketParser());
-        registerPipelineParser(new StatsBucketParser());
-        registerPipelineParser(new ExtendedStatsBucketParser());
-        registerPipelineParser(new PercentilesBucketParser());
-        registerPipelineParser(new MovAvgParser(movAvgModelParserMapper));
-        registerPipelineParser(new CumulativeSumParser());
-        registerPipelineParser(new BucketScriptParser());
-        registerPipelineParser(new BucketSelectorParser());
-        registerPipelineParser(new SerialDiffParser());
-
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(aggParsers, pipelineAggParsers, namedWriteableRegistry);
-        AggregationParseElement aggParseElement = new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationBinaryParseElement aggBinaryParseElement = new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationPhase aggPhase = new AggregationPhase(aggParseElement, aggBinaryParseElement);
-        bind(AggregatorParsers.class).toInstance(aggregatorParsers);
-        bind(AggregationParseElement.class).toInstance(aggParseElement);
-        bind(AggregationPhase.class).toInstance(aggPhase);
+    protected void configureAggs() {
+        Multibinder<Aggregator.Parser> multibinderAggParser = Multibinder.newSetBinder(binder(), Aggregator.Parser.class);
+        multibinderAggParser.addBinding().to(AvgParser.class);
+        multibinderAggParser.addBinding().to(SumParser.class);
+        multibinderAggParser.addBinding().to(MinParser.class);
+        multibinderAggParser.addBinding().to(MaxParser.class);
+        multibinderAggParser.addBinding().to(StatsParser.class);
+        multibinderAggParser.addBinding().to(ExtendedStatsParser.class);
+        multibinderAggParser.addBinding().to(ValueCountParser.class);
+        multibinderAggParser.addBinding().to(PercentilesParser.class);
+        multibinderAggParser.addBinding().to(PercentileRanksParser.class);
+        multibinderAggParser.addBinding().to(CardinalityParser.class);
+        multibinderAggParser.addBinding().to(GlobalParser.class);
+        multibinderAggParser.addBinding().to(MissingParser.class);
+        multibinderAggParser.addBinding().to(FilterParser.class);
+        multibinderAggParser.addBinding().to(FiltersParser.class);
+        multibinderAggParser.addBinding().to(SamplerParser.class);
+        multibinderAggParser.addBinding().to(TermsParser.class);
+        multibinderAggParser.addBinding().to(SignificantTermsParser.class);
+        multibinderAggParser.addBinding().to(RangeParser.class);
+        multibinderAggParser.addBinding().to(DateRangeParser.class);
+        multibinderAggParser.addBinding().to(IpRangeParser.class);
+        multibinderAggParser.addBinding().to(HistogramParser.class);
+        multibinderAggParser.addBinding().to(DateHistogramParser.class);
+        multibinderAggParser.addBinding().to(GeoDistanceParser.class);
+        multibinderAggParser.addBinding().to(GeoHashGridParser.class);
+        multibinderAggParser.addBinding().to(NestedParser.class);
+        multibinderAggParser.addBinding().to(ReverseNestedParser.class);
+        multibinderAggParser.addBinding().to(TopHitsParser.class);
+        multibinderAggParser.addBinding().to(GeoBoundsParser.class);
+        multibinderAggParser.addBinding().to(GeoCentroidParser.class);
+        multibinderAggParser.addBinding().to(ScriptedMetricParser.class);
+        multibinderAggParser.addBinding().to(ChildrenParser.class);
+        for (Class<? extends Aggregator.Parser> parser : aggParsers) {
+            multibinderAggParser.addBinding().to(parser);
+        }
+
+        Multibinder<PipelineAggregator.Parser> multibinderPipelineAggParser = Multibinder.newSetBinder(binder(), PipelineAggregator.Parser.class);
+        multibinderPipelineAggParser.addBinding().to(DerivativeParser.class);
+        multibinderPipelineAggParser.addBinding().to(MaxBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MinBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(AvgBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(SumBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(StatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(ExtendedStatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(PercentilesBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MovAvgParser.class);
+        multibinderPipelineAggParser.addBinding().to(CumulativeSumParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketScriptParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketSelectorParser.class);
+        multibinderPipelineAggParser.addBinding().to(SerialDiffParser.class);
+        for (Class<? extends PipelineAggregator.Parser> parser : pipelineAggParsers) {
+            multibinderPipelineAggParser.addBinding().to(parser);
+        }
+        bind(AggregatorParsers.class).asEagerSingleton();
+        bind(AggregationParseElement.class).asEagerSingleton();
+        bind(AggregationPhase.class).asEagerSingleton();
+
+        Multibinder<SignificanceHeuristicParser> heuristicParserMultibinder = Multibinder.newSetBinder(binder(), SignificanceHeuristicParser.class);
+        for (Class<? extends SignificanceHeuristicParser> clazz : heuristicParsers) {
+            heuristicParserMultibinder.addBinding().to(clazz);
+        }
+        bind(SignificanceHeuristicParserMapper.class);
+
+        Multibinder<MovAvgModel.AbstractModelParser> modelParserMultibinder = Multibinder.newSetBinder(binder(), MovAvgModel.AbstractModelParser.class);
+        for (Class<? extends MovAvgModel.AbstractModelParser> clazz : modelParsers) {
+            modelParserMultibinder.addBinding().to(clazz);
+        }
+        bind(MovAvgModelParserMapper.class);
     }
 
     protected void configureSearch() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index a2521e2..4afef71 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -59,11 +59,6 @@ import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.aggregations.AggregationInitializationException;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
-import org.elasticsearch.search.aggregations.SearchContextAggregations;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.dfs.DfsSearchResult;
@@ -151,14 +146,11 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
     private final Map<String, SearchParseElement> elementParsers;
 
     private final ParseFieldMatcher parseFieldMatcher;
-    private AggregatorParsers aggParsers;
 
     @Inject
-    public SearchService(Settings settings, ClusterSettings clusterSettings, ClusterService clusterService, IndicesService indicesService,
-            ThreadPool threadPool, ScriptService scriptService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, DfsPhase dfsPhase,
-            QueryPhase queryPhase, FetchPhase fetchPhase, IndicesRequestCache indicesQueryCache, AggregatorParsers aggParsers) {
+    public SearchService(Settings settings, ClusterSettings clusterSettings, ClusterService clusterService, IndicesService indicesService, ThreadPool threadPool,
+                         ScriptService scriptService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, DfsPhase dfsPhase, QueryPhase queryPhase, FetchPhase fetchPhase) {
         super(settings);
-        this.aggParsers = aggParsers;
         this.parseFieldMatcher = new ParseFieldMatcher(settings);
         this.threadPool = threadPool;
         this.clusterService = clusterService;
@@ -169,7 +161,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         this.dfsPhase = dfsPhase;
         this.queryPhase = queryPhase;
         this.fetchPhase = fetchPhase;
-        this.indicesQueryCache = indicesQueryCache;
+        this.indicesQueryCache = indicesService.getIndicesRequestCache();
 
         TimeValue keepAliveInterval = KEEPALIVE_INTERVAL_SETTING.get(settings);
         this.defaultKeepAlive = DEFAULT_KEEPALIVE_SETTING.get(settings).millis();
@@ -541,10 +533,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
 
         Engine.Searcher engineSearcher = searcher == null ? indexShard.acquireSearcher("search") : searcher;
 
-        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher,
-                indexService,
-                indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
-                defaultSearchTimeout, fetchPhase);
+        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher, indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, defaultSearchTimeout);
         SearchContext.setCurrent(context);
 
         try {
@@ -559,7 +548,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
                     QueryParseContext queryParseContext = new QueryParseContext(indicesService.getIndicesQueryRegistry());
                     queryParseContext.reset(parser);
                     queryParseContext.parseFieldMatcher(parseFieldMatcher);
-                    parseSource(context, SearchSourceBuilder.parseSearchSource(parser, queryParseContext, aggParsers));
+                    parseSource(context, SearchSourceBuilder.parseSearchSource(parser, queryParseContext));
                 }
             }
             parseSource(context, request.source());
@@ -712,13 +701,32 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         context.timeoutInMillis(source.timeoutInMillis());
         context.terminateAfter(source.terminateAfter());
         if (source.aggregations() != null) {
+            XContentParser completeAggregationsParser = null;
             try {
-                AggregationContext aggContext = new AggregationContext(context);
-                AggregatorFactories factories = source.aggregations().build(aggContext, null);
-                factories.validate();
-                context.aggregations(new SearchContextAggregations(factories));
-            } catch (IOException e) {
-                throw new AggregationInitializationException("Failed to create aggregators", e);
+                XContentBuilder completeAggregationsBuilder = XContentFactory.jsonBuilder();
+                completeAggregationsBuilder.startObject();
+                for (BytesReference agg : source.aggregations()) {
+                    XContentParser parser = XContentFactory.xContent(agg).createParser(agg);
+                    parser.nextToken();
+                    parser.nextToken();
+                    completeAggregationsBuilder.field(parser.currentName());
+                    parser.nextToken();
+                    completeAggregationsBuilder.copyCurrentStructure(parser);
+                }
+                completeAggregationsBuilder.endObject();
+                BytesReference completeAggregationsBytes = completeAggregationsBuilder.bytes();
+                completeAggregationsParser = XContentFactory.xContent(completeAggregationsBytes).createParser(completeAggregationsBytes);
+                completeAggregationsParser.nextToken();
+                this.elementParsers.get("aggregations").parse(completeAggregationsParser, context);
+            } catch (Exception e) {
+                String sSource = "_na_";
+                try {
+                    sSource = source.toString();
+                } catch (Throwable e1) {
+                    // ignore
+                }
+                XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
+                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
             }
         }
         if (source.suggest() != null) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java
new file mode 100644
index 0000000..31ee3fd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.common.xcontent.ToXContent;
+
+/**
+ * Base structure for aggregation builders.
+ */
+public abstract class AbstractAggregationBuilder implements ToXContent {
+
+    private final String name;
+    protected final String type;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected AbstractAggregationBuilder(String name, String type) {
+        this.name = name;
+        this.type = type;
+    }
+
+    /**
+     * Return the name of the aggregation that is being built.
+     */
+    public String getName() {
+        return name;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
index d1fd5b1..f2a9f08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.internal.SearchContext;
 
 /**
@@ -30,8 +29,8 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationBinaryParseElement extends AggregationParseElement {
 
     @Inject
-    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
-        super(aggregatorParsers, queriesRegistry);
+    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers) {
+        super(aggregatorParsers);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
new file mode 100644
index 0000000..ed1ba6c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.ElasticsearchGenerationException;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * A base class for all bucket aggregation builders.
+ */
+public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extends AbstractAggregationBuilder {
+
+    private List<AbstractAggregationBuilder> aggregations;
+    private BytesReference aggregationsBinary;
+    private Map<String, Object> metaData;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected AggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Add a sub get to this bucket get.
+     */
+    @SuppressWarnings("unchecked")
+    public B subAggregation(AbstractAggregationBuilder aggregation) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
+        }
+        aggregations.add(aggregation);
+        return (B) this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(byte[] aggregationsBinary) {
+        return subAggregation(aggregationsBinary, 0, aggregationsBinary.length);
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
+        return subAggregation(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    @SuppressWarnings("unchecked")
+    public B subAggregation(BytesReference aggregationsBinary) {
+        this.aggregationsBinary = aggregationsBinary;
+        return (B) this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(XContentBuilder aggs) {
+        return subAggregation(aggs.bytes());
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(Map<String, Object> aggs) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(aggs);
+            return subAggregation(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + aggs + "]", e);
+        }
+    }
+
+    /**
+     * Sets the meta data to be included in the aggregation response
+     */
+    public B setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+        return (B)this;
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName());
+
+        if (this.metaData != null) {
+            builder.field("meta", this.metaData);
+        }
+        builder.field(type);
+        internalXContent(builder, params);
+
+        if (aggregations != null || aggregationsBinary != null) {
+
+            if (aggregations != null) {
+                builder.startObject("aggregations");
+                for (AbstractAggregationBuilder subAgg : aggregations) {
+                    subAgg.toXContent(builder, params);
+                }
+                builder.endObject();
+            }
+
+            if (aggregationsBinary != null) {
+                if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
+                    builder.rawField("aggregations", aggregationsBinary);
+                } else {
+                    builder.field("aggregations_binary", aggregationsBinary);
+                }
+            }
+
+        }
+
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
index c4f477e..dd9c5a3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
@@ -19,68 +19,64 @@
 package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.children.Children;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.ChildrenAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.children.ChildrenBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.FilterAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filters.Filters;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.FiltersAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
+import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGrid;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridBuilder;
 import org.elasticsearch.search.aggregations.bucket.global.Global;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator.GlobalAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.global.GlobalBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramBuilder;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.HistogramAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramBuilder;
 import org.elasticsearch.search.aggregations.bucket.missing.Missing;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator.MissingAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.missing.MissingBuilder;
 import org.elasticsearch.search.aggregations.bucket.nested.Nested;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator.NestedAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.nested.NestedBuilder;
 import org.elasticsearch.search.aggregations.bucket.nested.ReverseNested;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator.ReverseNestedAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedBuilder;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.RangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.DiversifiedAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.SamplerAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator.AvgAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
 import org.elasticsearch.search.aggregations.metrics.cardinality.Cardinality;
-import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityBuilder;
 import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBounds;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.GeoBoundsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsBuilder;
 import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroid;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator.GeoCentroidAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator.MaxAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
 import org.elasticsearch.search.aggregations.metrics.min.Min;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator.MinAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
 import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetric;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator.StatsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.stats.StatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator.SumAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.search.aggregations.metrics.tophits.TopHits;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator.TopHitsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsBuilder;
 import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCount;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator.ValueCountAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountBuilder;
 
 /**
  * Utility class to create aggregations.
@@ -93,234 +89,217 @@ public class AggregationBuilders {
     /**
      * Create a new {@link ValueCount} aggregation with the given name.
      */
-    public static ValueCountAggregatorBuilder count(String name) {
-        return new ValueCountAggregatorBuilder(name, null);
+    public static ValueCountBuilder count(String name) {
+        return new ValueCountBuilder(name);
     }
 
     /**
      * Create a new {@link Avg} aggregation with the given name.
      */
-    public static AvgAggregatorBuilder avg(String name) {
-        return new AvgAggregatorBuilder(name);
+    public static AvgBuilder avg(String name) {
+        return new AvgBuilder(name);
     }
 
     /**
      * Create a new {@link Max} aggregation with the given name.
      */
-    public static MaxAggregatorBuilder max(String name) {
-        return new MaxAggregatorBuilder(name);
+    public static MaxBuilder max(String name) {
+        return new MaxBuilder(name);
     }
 
     /**
      * Create a new {@link Min} aggregation with the given name.
      */
-    public static MinAggregatorBuilder min(String name) {
-        return new MinAggregatorBuilder(name);
+    public static MinBuilder min(String name) {
+        return new MinBuilder(name);
     }
 
     /**
      * Create a new {@link Sum} aggregation with the given name.
      */
-    public static SumAggregatorBuilder sum(String name) {
-        return new SumAggregatorBuilder(name);
+    public static SumBuilder sum(String name) {
+        return new SumBuilder(name);
     }
 
     /**
      * Create a new {@link Stats} aggregation with the given name.
      */
-    public static StatsAggregatorBuilder stats(String name) {
-        return new StatsAggregatorBuilder(name);
+    public static StatsBuilder stats(String name) {
+        return new StatsBuilder(name);
     }
 
     /**
      * Create a new {@link ExtendedStats} aggregation with the given name.
      */
-    public static ExtendedStatsAggregatorBuilder extendedStats(String name) {
-        return new ExtendedStatsAggregatorBuilder(name);
+    public static ExtendedStatsBuilder extendedStats(String name) {
+        return new ExtendedStatsBuilder(name);
     }
 
     /**
      * Create a new {@link Filter} aggregation with the given name.
      */
-    public static FilterAggregatorBuilder filter(String name, QueryBuilder<?> filter) {
-        return new FilterAggregatorBuilder(name, filter);
+    public static FilterAggregationBuilder filter(String name) {
+        return new FilterAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Filters} aggregation with the given name.
      */
-    public static FiltersAggregatorBuilder filters(String name, KeyedFilter... filters) {
-        return new FiltersAggregatorBuilder(name, filters);
-    }
-
-    /**
-     * Create a new {@link Filters} aggregation with the given name.
-     */
-    public static FiltersAggregatorBuilder filters(String name, QueryBuilder<?>... filters) {
-        return new FiltersAggregatorBuilder(name, filters);
-    }
-
-    /**
-     * Create a new {@link Sampler} aggregation with the given name.
-     */
-    public static SamplerAggregatorBuilder sampler(String name) {
-        return new SamplerAggregatorBuilder(name);
+    public static FiltersAggregationBuilder filters(String name) {
+        return new FiltersAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Sampler} aggregation with the given name.
      */
-    public static DiversifiedAggregatorBuilder diversifiedSampler(String name) {
-        return new DiversifiedAggregatorBuilder(name);
+    public static SamplerAggregationBuilder sampler(String name) {
+        return new SamplerAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Global} aggregation with the given name.
      */
-    public static GlobalAggregatorBuilder global(String name) {
-        return new GlobalAggregatorBuilder(name);
+    public static GlobalBuilder global(String name) {
+        return new GlobalBuilder(name);
     }
 
     /**
      * Create a new {@link Missing} aggregation with the given name.
      */
-    public static MissingAggregatorBuilder missing(String name) {
-        return new MissingAggregatorBuilder(name, null);
+    public static MissingBuilder missing(String name) {
+        return new MissingBuilder(name);
     }
 
     /**
      * Create a new {@link Nested} aggregation with the given name.
      */
-    public static NestedAggregatorBuilder nested(String name, String path) {
-        return new NestedAggregatorBuilder(name, path);
+    public static NestedBuilder nested(String name) {
+        return new NestedBuilder(name);
     }
 
     /**
      * Create a new {@link ReverseNested} aggregation with the given name.
      */
-    public static ReverseNestedAggregatorBuilder reverseNested(String name) {
-        return new ReverseNestedAggregatorBuilder(name);
+    public static ReverseNestedBuilder reverseNested(String name) {
+        return new ReverseNestedBuilder(name);
     }
 
     /**
      * Create a new {@link Children} aggregation with the given name.
      */
-    public static ChildrenAggregatorBuilder children(String name, String childType) {
-        return new ChildrenAggregatorBuilder(name, childType);
+    public static ChildrenBuilder children(String name) {
+        return new ChildrenBuilder(name);
     }
 
     /**
      * Create a new {@link GeoDistance} aggregation with the given name.
      */
-    public static GeoDistanceAggregatorBuilder geoDistance(String name, GeoPoint origin) {
-        return new GeoDistanceAggregatorBuilder(name, origin);
+    public static GeoDistanceBuilder geoDistance(String name) {
+        return new GeoDistanceBuilder(name);
     }
 
     /**
      * Create a new {@link Histogram} aggregation with the given name.
      */
-    public static HistogramAggregatorBuilder histogram(String name) {
-        return new HistogramAggregatorBuilder(name);
+    public static HistogramBuilder histogram(String name) {
+        return new HistogramBuilder(name);
     }
 
     /**
      * Create a new {@link GeoHashGrid} aggregation with the given name.
      */
-    public static GeoGridAggregatorBuilder geohashGrid(String name) {
-        return new GeoGridAggregatorBuilder(name);
+    public static GeoHashGridBuilder geohashGrid(String name) {
+        return new GeoHashGridBuilder(name);
     }
 
     /**
      * Create a new {@link SignificantTerms} aggregation with the given name.
      */
-    public static SignificantTermsAggregatorBuilder significantTerms(String name) {
-        return new SignificantTermsAggregatorBuilder(name, null);
+    public static SignificantTermsBuilder significantTerms(String name) {
+        return new SignificantTermsBuilder(name);
     }
 
     /**
-     * Create a new {@link DateHistogramAggregatorBuilder} aggregation with the given
-     * name.
+     * Create a new {@link DateHistogramBuilder} aggregation with the given name.
      */
-    public static DateHistogramAggregatorBuilder dateHistogram(String name) {
-        return new DateHistogramAggregatorBuilder(name);
+    public static DateHistogramBuilder dateHistogram(String name) {
+        return new DateHistogramBuilder(name);
     }
 
     /**
      * Create a new {@link Range} aggregation with the given name.
      */
-    public static RangeAggregatorBuilder range(String name) {
-        return new RangeAggregatorBuilder(name);
+    public static RangeBuilder range(String name) {
+        return new RangeBuilder(name);
     }
 
     /**
-     * Create a new {@link DateRangeAggregatorBuilder} aggregation with the
-     * given name.
+     * Create a new {@link DateRangeBuilder} aggregation with the given name.
      */
-    public static DateRangeAggregatorBuilder dateRange(String name) {
-        return new DateRangeAggregatorBuilder(name);
+    public static DateRangeBuilder dateRange(String name) {
+        return new DateRangeBuilder(name);
     }
 
     /**
-     * Create a new {@link IPv4RangeAggregatorBuilder} aggregation with the
-     * given name.
+     * Create a new {@link IPv4RangeBuilder} aggregation with the given name.
      */
-    public static IPv4RangeAggregatorBuilder ipRange(String name) {
-        return new IPv4RangeAggregatorBuilder(name);
+    public static IPv4RangeBuilder ipRange(String name) {
+        return new IPv4RangeBuilder(name);
     }
 
     /**
      * Create a new {@link Terms} aggregation with the given name.
      */
-    public static TermsAggregatorBuilder terms(String name) {
-        return new TermsAggregatorBuilder(name, null);
+    public static TermsBuilder terms(String name) {
+        return new TermsBuilder(name);
     }
 
     /**
      * Create a new {@link Percentiles} aggregation with the given name.
      */
-    public static PercentilesAggregatorBuilder percentiles(String name) {
-        return new PercentilesAggregatorBuilder(name);
+    public static PercentilesBuilder percentiles(String name) {
+        return new PercentilesBuilder(name);
     }
 
     /**
      * Create a new {@link PercentileRanks} aggregation with the given name.
      */
-    public static PercentileRanksAggregatorBuilder percentileRanks(String name) {
-        return new PercentileRanksAggregatorBuilder(name);
+    public static PercentileRanksBuilder percentileRanks(String name) {
+        return new PercentileRanksBuilder(name);
     }
 
     /**
      * Create a new {@link Cardinality} aggregation with the given name.
      */
-    public static CardinalityAggregatorBuilder cardinality(String name) {
-        return new CardinalityAggregatorBuilder(name, null);
+    public static CardinalityBuilder cardinality(String name) {
+        return new CardinalityBuilder(name);
     }
 
     /**
      * Create a new {@link TopHits} aggregation with the given name.
      */
-    public static TopHitsAggregatorBuilder topHits(String name) {
-        return new TopHitsAggregatorBuilder(name);
+    public static TopHitsBuilder topHits(String name) {
+        return new TopHitsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoBounds} aggregation with the given name.
      */
-    public static GeoBoundsAggregatorBuilder geoBounds(String name) {
-        return new GeoBoundsAggregatorBuilder(name);
+    public static GeoBoundsBuilder geoBounds(String name) {
+        return new GeoBoundsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoCentroid} aggregation with the given name.
      */
-    public static GeoCentroidAggregatorBuilder geoCentroid(String name) {
-        return new GeoCentroidAggregatorBuilder(name);
+    public static GeoCentroidBuilder geoCentroid(String name) {
+        return new GeoCentroidBuilder(name);
     }
 
     /**
      * Create a new {@link ScriptedMetric} aggregation with the given name.
      */
-    public static ScriptedMetricAggregatorBuilder scriptedMetric(String name) {
-        return new ScriptedMetricAggregatorBuilder(name);
+    public static ScriptedMetricBuilder scriptedMetric(String name) {
+        return new ScriptedMetricBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
index fee445d..767cbfb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
@@ -20,10 +20,7 @@ package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchParseElement;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext;
 
 /**
@@ -52,23 +49,15 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationParseElement implements SearchParseElement {
 
     private final AggregatorParsers aggregatorParsers;
-    private IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public AggregationParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
+    public AggregationParseElement(AggregatorParsers aggregatorParsers) {
         this.aggregatorParsers = aggregatorParsers;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(context.parseFieldMatcher());
-        AggregatorFactories.Builder builders = aggregatorParsers.parseAggregators(parser, parseContext);
-        AggregationContext aggContext = new AggregationContext(context);
-        AggregatorFactories factories = builders.build(aggContext, null);
-        factories.validate();
+        AggregatorFactories factories = aggregatorParsers.parseAggregators(parser, context);
         context.aggregations(new SearchContextAggregations(factories));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index 4c17a23..050ffa6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -77,7 +77,7 @@ public class AggregationPhase implements SearchPhase {
             Aggregator[] aggregators;
             try {
                 AggregatorFactories factories = context.aggregations().factories();
-                aggregators = factories.createTopLevelAggregators();
+                aggregators = factories.createTopLevelAggregators(aggregationContext);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (aggregators[i] instanceof GlobalAggregator == false) {
                         collectors.add(aggregators[i]);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
index 374ac14..8ee4d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
@@ -22,14 +22,11 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -43,7 +40,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     /**
      * Parses the aggregation request and creates the appropriate aggregator factory for it.
      *
-     * @see AggregatorBuilder
+     * @see AggregatorFactory
     */
     public interface Parser {
 
@@ -62,13 +59,8 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
          * @return                  The resolved aggregator factory or {@code null} in case the aggregation should be skipped
          * @throws java.io.IOException      When parsing fails
          */
-        AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException;
+        AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException;
 
-        /**
-         * @return an empty {@link AggregatorBuilder} instance for this parser
-         *         that can be used for deserialization
-         */
-        AggregatorBuilder<?> getFactoryPrototypes();
     }
 
     /**
@@ -115,7 +107,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     public abstract InternalAggregation buildEmptyAggregation();
 
     /** Aggregation mode for sub aggregations. */
-    public enum SubAggCollectionMode implements Writeable<SubAggCollectionMode> {
+    public enum SubAggCollectionMode {
 
         /**
          * Creates buckets and delegates to child aggregators in a single pass over
@@ -151,19 +143,5 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
             }
             throw new ElasticsearchParseException("no [{}] found for value [{}]", KEY.getPreferredName(), value);
         }
-
-        @Override
-        public SubAggCollectionMode readFrom(StreamInput in) throws IOException {
-            int ordinal = in.readVInt();
-            if (ordinal < 0 || ordinal >= values().length) {
-                throw new IOException("Unknown SubAggCollectionMode ordinal [" + ordinal + "]");
-            }
-            return values()[ordinal];
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(ordinal());
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBase.java
index a44d01b..e70780e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBase.java
@@ -133,7 +133,7 @@ public abstract class AggregatorBase extends Aggregator {
     }
 
     /**
-     * Can be overriden by aggregator implementation to be called back when the collection phase starts.
+     * Can be overridden by aggregator implementation to be called back when the collection phase starts.
      */
     protected void doPreCollection() throws IOException {
     }
@@ -252,7 +252,7 @@ public abstract class AggregatorBase extends Aggregator {
     protected void doClose() {}
 
     /**
-     * Can be overriden by aggregator implementation to be called back when the collection phase ends.
+     * Can be overridden by aggregator implementation to be called back when the collection phase ends.
      */
     protected void doPostCollection() throws IOException {
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBuilder.java
deleted file mode 100644
index fc9d6e6..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBuilder.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations;
-
-
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- * A factory that knows how to create an {@link Aggregator} of a specific type.
- */
-public abstract class AggregatorBuilder<AB extends AggregatorBuilder<AB>> extends ToXContentToBytes
-        implements NamedWriteable<AB>, ToXContent {
-
-    protected String name;
-    protected Type type;
-    protected AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder();
-    protected Map<String, Object> metaData;
-
-    /**
-     * Constructs a new aggregator factory.
-     *
-     * @param name  The aggregation name
-     * @param type  The aggregation type
-     */
-    public AggregatorBuilder(String name, Type type) {
-        this.name = name;
-        this.type = type;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public AB subAggregation(AggregatorBuilder<?> aggregation) {
-        factoriesBuilder.addAggregator(aggregation);
-        return (AB) this;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public AB subAggregation(PipelineAggregatorBuilder aggregation) {
-        factoriesBuilder.addPipelineAggregator(aggregation);
-        return (AB) this;
-    }
-
-    /**
-     * Registers sub-factories with this factory. The sub-factory will be
-     * responsible for the creation of sub-aggregators under the aggregator
-     * created by this factory.
-     *
-     * @param subFactories
-     *            The sub-factories
-     * @return this factory (fluent interface)
-     */
-    @SuppressWarnings("unchecked")
-    public AB subAggregations(AggregatorFactories.Builder subFactories) {
-        this.factoriesBuilder = subFactories;
-        return (AB) this;
-    }
-
-    public AB setMetaData(Map<String, Object> metaData) {
-        this.metaData = metaData;
-        return (AB) this;
-    }
-
-    public String getType() {
-        return type.name();
-    }
-
-    public final AggregatorFactory<?> build(AggregationContext context, AggregatorFactory<?> parent) throws IOException {
-        AggregatorFactory<?> factory = doBuild(context, parent, factoriesBuilder);
-        return factory;
-    }
-
-    protected abstract AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subfactoriesBuilder) throws IOException;
-
-    @Override
-    public final AB readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        AB factory = doReadFrom(name, in);
-        factory.factoriesBuilder = AggregatorFactories.Builder.PROTOTYPE.readFrom(in);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    protected abstract AB doReadFrom(String name, StreamInput in) throws IOException;
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        doWriteTo(out);
-        factoriesBuilder.writeTo(out);
-        out.writeMap(metaData);
-    }
-
-    protected abstract void doWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(name);
-
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.field(type.name());
-        internalXContent(builder, params);
-
-        if (factoriesBuilder != null && (factoriesBuilder.count()) > 0) {
-            builder.field("aggregations");
-            factoriesBuilder.toXContent(builder, params);
-
-        }
-
-        return builder.endObject();
-    }
-
-    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    public String getWriteableName() {
-        return type.stream().toUtf8();
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(factoriesBuilder, metaData, name, type, doHashCode());
-    }
-
-    protected abstract int doHashCode();
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        AggregatorBuilder<AB> other = (AggregatorBuilder<AB>) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        if (!Objects.equals(factoriesBuilder, other.factoriesBuilder))
-            return false;
-        return doEquals(obj);
-    }
-
-    protected abstract boolean doEquals(Object obj);
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
index 32b7fb7..6a1cd27 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
@@ -18,13 +18,8 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.aggregations.support.AggregationPath;
 import org.elasticsearch.search.aggregations.support.AggregationPath.PathElement;
@@ -36,7 +31,6 @@ import java.util.HashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 /**
@@ -44,27 +38,24 @@ import java.util.Set;
  */
 public class AggregatorFactories {
 
-    public static final AggregatorFactories EMPTY = new AggregatorFactories(null, new AggregatorFactory<?>[0],
-            new ArrayList<PipelineAggregatorBuilder>());
+    public static final AggregatorFactories EMPTY = new Empty();
 
-    private AggregatorFactory<?> parent;
-    private AggregatorFactory<?>[] factories;
-    private List<PipelineAggregatorBuilder> pipelineAggregatorFactories;
+    private AggregatorFactory parent;
+    private AggregatorFactory[] factories;
+    private List<PipelineAggregatorFactory> pipelineAggregatorFactories;
 
     public static Builder builder() {
         return new Builder();
     }
 
-    private AggregatorFactories(AggregatorFactory<?> parent, AggregatorFactory<?>[] factories,
-            List<PipelineAggregatorBuilder> pipelineAggregators) {
-        this.parent = parent;
+    private AggregatorFactories(AggregatorFactory[] factories, List<PipelineAggregatorFactory> pipelineAggregators) {
         this.factories = factories;
         this.pipelineAggregatorFactories = pipelineAggregators;
     }
 
     public List<PipelineAggregator> createPipelineAggregators() throws IOException {
         List<PipelineAggregator> pipelineAggregators = new ArrayList<>();
-        for (PipelineAggregatorBuilder factory : this.pipelineAggregatorFactories) {
+        for (PipelineAggregatorFactory factory : this.pipelineAggregatorFactories) {
             pipelineAggregators.add(factory.create());
         }
         return pipelineAggregators;
@@ -75,129 +66,122 @@ public class AggregatorFactories {
      * buckets.
      */
     public Aggregator[] createSubAggregators(Aggregator parent) throws IOException {
-        Aggregator[] aggregators = new Aggregator[countAggregators()];
+        Aggregator[] aggregators = new Aggregator[count()];
         for (int i = 0; i < factories.length; ++i) {
             // TODO: sometimes even sub aggregations always get called with bucket 0, eg. if
             // you have a terms agg under a top-level filter agg. We should have a way to
             // propagate the fact that only bucket 0 will be collected with single-bucket
             // aggs
             final boolean collectsFromSingleBucket = false;
-            aggregators[i] = factories[i].create(parent, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(parent.context(), parent, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    public Aggregator[] createTopLevelAggregators() throws IOException {
+    public Aggregator[] createTopLevelAggregators(AggregationContext ctx) throws IOException {
         // These aggregators are going to be used with a single bucket ordinal, no need to wrap the PER_BUCKET ones
         Aggregator[] aggregators = new Aggregator[factories.length];
         for (int i = 0; i < factories.length; i++) {
             // top-level aggs only get called with bucket 0
             final boolean collectsFromSingleBucket = true;
-            aggregators[i] = factories[i].create(null, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(ctx, null, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    /**
-     * @return the number of sub-aggregator factories not including pipeline
-     *         aggregator factories
-     */
-    public int countAggregators() {
+    public int count() {
         return factories.length;
     }
 
-    /**
-     * @return the number of pipeline aggregator factories
-     */
-    public int countPipelineAggregators() {
-        return pipelineAggregatorFactories.size();
+    void setParent(AggregatorFactory parent) {
+        this.parent = parent;
+        for (AggregatorFactory factory : factories) {
+            factory.parent = parent;
+        }
     }
 
     public void validate() {
-        for (AggregatorFactory<?> factory : factories) {
+        for (AggregatorFactory factory : factories) {
             factory.validate();
         }
-        for (PipelineAggregatorBuilder factory : pipelineAggregatorFactories) {
+        for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
             factory.validate(parent, factories, pipelineAggregatorFactories);
         }
     }
 
-    public static class Builder extends ToXContentToBytes implements Writeable<Builder> {
+    private final static class Empty extends AggregatorFactories {
 
-        public final static Builder PROTOTYPE = new Builder();
+        private static final AggregatorFactory[] EMPTY_FACTORIES = new AggregatorFactory[0];
+        private static final Aggregator[] EMPTY_AGGREGATORS = new Aggregator[0];
+        private static final List<PipelineAggregatorFactory> EMPTY_PIPELINE_AGGREGATORS = new ArrayList<>();
 
-        private final Set<String> names = new HashSet<>();
-        private final List<AggregatorBuilder<?>> aggregatorBuilders = new ArrayList<>();
-        private final List<PipelineAggregatorBuilder> pipelineAggregatorFactories = new ArrayList<>();
-        private boolean skipResolveOrder;
+        private Empty() {
+            super(EMPTY_FACTORIES, EMPTY_PIPELINE_AGGREGATORS);
+        }
+
+        @Override
+        public Aggregator[] createSubAggregators(Aggregator parent) {
+            return EMPTY_AGGREGATORS;
+        }
 
-        public Builder addAggregators(AggregatorFactories factories) {
-            throw new UnsupportedOperationException("This needs to be removed");
+        @Override
+        public Aggregator[] createTopLevelAggregators(AggregationContext ctx) {
+            return EMPTY_AGGREGATORS;
         }
 
-        public Builder addAggregator(AggregatorBuilder<?> factory) {
+    }
+
+    public static class Builder {
+
+        private final Set<String> names = new HashSet<>();
+        private final List<AggregatorFactory> factories = new ArrayList<>();
+        private final List<PipelineAggregatorFactory> pipelineAggregatorFactories = new ArrayList<>();
+
+        public Builder addAggregator(AggregatorFactory factory) {
             if (!names.add(factory.name)) {
                 throw new IllegalArgumentException("Two sibling aggregations cannot have the same name: [" + factory.name + "]");
             }
-            aggregatorBuilders.add(factory);
+            factories.add(factory);
             return this;
         }
 
-        public Builder addPipelineAggregator(PipelineAggregatorBuilder pipelineAggregatorFactory) {
+        public Builder addPipelineAggregator(PipelineAggregatorFactory pipelineAggregatorFactory) {
             this.pipelineAggregatorFactories.add(pipelineAggregatorFactory);
             return this;
         }
 
-        /**
-         * FOR TESTING ONLY
-         */
-        Builder skipResolveOrder() {
-            this.skipResolveOrder = true;
-            return this;
-        }
-
-        public AggregatorFactories build(AggregationContext context, AggregatorFactory<?> parent) throws IOException {
-            if (aggregatorBuilders.isEmpty() && pipelineAggregatorFactories.isEmpty()) {
+        public AggregatorFactories build() {
+            if (factories.isEmpty() && pipelineAggregatorFactories.isEmpty()) {
                 return EMPTY;
             }
-            List<PipelineAggregatorBuilder> orderedpipelineAggregators = null;
-            if (skipResolveOrder) {
-                orderedpipelineAggregators = new ArrayList<>(pipelineAggregatorFactories);
-            } else {
-                orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.aggregatorBuilders);
-            }
-            AggregatorFactory<?>[] aggFactories = new AggregatorFactory<?>[aggregatorBuilders.size()];
-            for (int i = 0; i < aggregatorBuilders.size(); i++) {
-                aggFactories[i] = aggregatorBuilders.get(i).build(context, parent);
-            }
-            return new AggregatorFactories(parent, aggFactories, orderedpipelineAggregators);
+            List<PipelineAggregatorFactory> orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.factories);
+            return new AggregatorFactories(factories.toArray(new AggregatorFactory[factories.size()]), orderedpipelineAggregators);
         }
 
-        private List<PipelineAggregatorBuilder> resolvePipelineAggregatorOrder(List<PipelineAggregatorBuilder> pipelineAggregatorFactories,
-                List<AggregatorBuilder<?>> aggFactories) {
-            Map<String, PipelineAggregatorBuilder> pipelineAggregatorFactoriesMap = new HashMap<>();
-            for (PipelineAggregatorBuilder factory : pipelineAggregatorFactories) {
+        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories, List<AggregatorFactory> aggFactories) {
+            Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap = new HashMap<>();
+            for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
                 pipelineAggregatorFactoriesMap.put(factory.getName(), factory);
             }
-            Map<String, AggregatorBuilder<?>> aggFactoriesMap = new HashMap<>();
-            for (AggregatorBuilder<?> aggFactory : aggFactories) {
+            Map<String, AggregatorFactory> aggFactoriesMap = new HashMap<>();
+            for (AggregatorFactory aggFactory : aggFactories) {
                 aggFactoriesMap.put(aggFactory.name, aggFactory);
             }
-            List<PipelineAggregatorBuilder> orderedPipelineAggregatorrs = new LinkedList<>();
-            List<PipelineAggregatorBuilder> unmarkedFactories = new ArrayList<PipelineAggregatorBuilder>(pipelineAggregatorFactories);
-            Set<PipelineAggregatorBuilder> temporarilyMarked = new HashSet<PipelineAggregatorBuilder>();
+            List<PipelineAggregatorFactory> orderedPipelineAggregatorrs = new LinkedList<>();
+            List<PipelineAggregatorFactory> unmarkedFactories = new ArrayList<PipelineAggregatorFactory>(pipelineAggregatorFactories);
+            Set<PipelineAggregatorFactory> temporarilyMarked = new HashSet<PipelineAggregatorFactory>();
             while (!unmarkedFactories.isEmpty()) {
-                PipelineAggregatorBuilder factory = unmarkedFactories.get(0);
+                PipelineAggregatorFactory factory = unmarkedFactories.get(0);
                 resolvePipelineAggregatorOrder(aggFactoriesMap, pipelineAggregatorFactoriesMap, orderedPipelineAggregatorrs,
                         unmarkedFactories, temporarilyMarked, factory);
             }
             return orderedPipelineAggregatorrs;
         }
 
-        private void resolvePipelineAggregatorOrder(Map<String, AggregatorBuilder<?>> aggFactoriesMap,
-                Map<String, PipelineAggregatorBuilder> pipelineAggregatorFactoriesMap,
-                List<PipelineAggregatorBuilder> orderedPipelineAggregators, List<PipelineAggregatorBuilder> unmarkedFactories, Set<PipelineAggregatorBuilder> temporarilyMarked,
-                PipelineAggregatorBuilder factory) {
+        private void resolvePipelineAggregatorOrder(Map<String, AggregatorFactory> aggFactoriesMap,
+                Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap,
+                List<PipelineAggregatorFactory> orderedPipelineAggregators, List<PipelineAggregatorFactory> unmarkedFactories, Set<PipelineAggregatorFactory> temporarilyMarked,
+                PipelineAggregatorFactory factory) {
             if (temporarilyMarked.contains(factory)) {
                 throw new IllegalArgumentException("Cyclical dependancy found with pipeline aggregator [" + factory.getName() + "]");
             } else if (unmarkedFactories.contains(factory)) {
@@ -209,7 +193,7 @@ public class AggregatorFactories {
                     if (bucketsPath.equals("_count") || bucketsPath.equals("_key")) {
                         continue;
                     } else if (aggFactoriesMap.containsKey(firstAggName)) {
-                        AggregatorBuilder<?> aggFactory = aggFactoriesMap.get(firstAggName);
+                        AggregatorFactory aggFactory = aggFactoriesMap.get(firstAggName);
                         for (int i = 1; i < bucketsPathElements.size(); i++) {
                             PathElement pathElement = bucketsPathElements.get(i);
                             String aggName = pathElement.name;
@@ -218,9 +202,9 @@ public class AggregatorFactories {
                             } else {
                                 // Check the non-pipeline sub-aggregator
                                 // factories
-                                AggregatorBuilder<?>[] subFactories = aggFactory.factoriesBuilder.getAggregatorFactories();
+                                AggregatorFactory[] subFactories = aggFactory.factories.factories;
                                 boolean foundSubFactory = false;
-                                for (AggregatorBuilder<?> subFactory : subFactories) {
+                                for (AggregatorFactory subFactory : subFactories) {
                                     if (aggName.equals(subFactory.name)) {
                                         aggFactory = subFactory;
                                         foundSubFactory = true;
@@ -229,8 +213,8 @@ public class AggregatorFactories {
                                 }
                                 // Check the pipeline sub-aggregator factories
                                 if (!foundSubFactory && (i == bucketsPathElements.size() - 1)) {
-                                    List<PipelineAggregatorBuilder> subPipelineFactories = aggFactory.factoriesBuilder.pipelineAggregatorFactories;
-                                    for (PipelineAggregatorBuilder subFactory : subPipelineFactories) {
+                                    List<PipelineAggregatorFactory> subPipelineFactories = aggFactory.factories.pipelineAggregatorFactories;
+                                    for (PipelineAggregatorFactory subFactory : subPipelineFactories) {
                                         if (aggName.equals(subFactory.name())) {
                                             foundSubFactory = true;
                                             break;
@@ -245,7 +229,7 @@ public class AggregatorFactories {
                         }
                         continue;
                     } else {
-                        PipelineAggregatorBuilder matchingFactory = pipelineAggregatorFactoriesMap.get(firstAggName);
+                        PipelineAggregatorFactory matchingFactory = pipelineAggregatorFactoriesMap.get(firstAggName);
                         if (matchingFactory != null) {
                             resolvePipelineAggregatorOrder(aggFactoriesMap, pipelineAggregatorFactoriesMap, orderedPipelineAggregators,
                                     unmarkedFactories,
@@ -261,80 +245,12 @@ public class AggregatorFactories {
             }
         }
 
-        AggregatorBuilder<?>[] getAggregatorFactories() {
-            return this.aggregatorBuilders.toArray(new AggregatorBuilder<?>[this.aggregatorBuilders.size()]);
+        AggregatorFactory[] getAggregatorFactories() {
+            return this.factories.toArray(new AggregatorFactory[this.factories.size()]);
         }
 
-        List<PipelineAggregatorBuilder> getPipelineAggregatorFactories() {
+        List<PipelineAggregatorFactory> getPipelineAggregatorFactories() {
             return this.pipelineAggregatorFactories;
         }
-
-        public int count() {
-            return aggregatorBuilders.size() + pipelineAggregatorFactories.size();
-        }
-
-        @Override
-        public Builder readFrom(StreamInput in) throws IOException {
-            Builder builder = new Builder();
-            int factoriesSize = in.readVInt();
-            for (int i = 0; i < factoriesSize; i++) {
-                AggregatorBuilder<?> factory = in.readAggregatorFactory();
-                builder.addAggregator(factory);
-            }
-            int pipelineFactoriesSize = in.readVInt();
-            for (int i = 0; i < pipelineFactoriesSize; i++) {
-                PipelineAggregatorBuilder factory = in.readPipelineAggregatorFactory();
-                builder.addPipelineAggregator(factory);
-            }
-            return builder;
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(this.aggregatorBuilders.size());
-            for (AggregatorBuilder<?> factory : aggregatorBuilders) {
-                out.writeAggregatorFactory(factory);
-            }
-            out.writeVInt(this.pipelineAggregatorFactories.size());
-            for (PipelineAggregatorBuilder factory : pipelineAggregatorFactories) {
-                out.writePipelineAggregatorFactory(factory);
-            }
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (aggregatorBuilders != null) {
-                for (AggregatorBuilder<?> subAgg : aggregatorBuilders) {
-                    subAgg.toXContent(builder, params);
-                }
-            }
-            if (pipelineAggregatorFactories != null) {
-                for (PipelineAggregatorBuilder subAgg : pipelineAggregatorFactories) {
-                    subAgg.toXContent(builder, params);
-                }
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(aggregatorBuilders, pipelineAggregatorFactories);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null)
-                return false;
-            if (getClass() != obj.getClass())
-                return false;
-            Builder other = (Builder) obj;
-            if (!Objects.equals(aggregatorBuilders, other.aggregatorBuilders))
-                return false;
-            if (!Objects.equals(pipelineAggregatorFactories, other.pipelineAggregatorFactories))
-                return false;
-            return true;
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
index 3fced89..680e3ef 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
@@ -16,7 +16,6 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations;
 
 import org.apache.lucene.index.LeafReaderContext;
@@ -24,7 +23,6 @@ import org.apache.lucene.search.Scorer;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.ObjectArray;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext.Lifetime;
@@ -33,33 +31,39 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> {
+/**
+ * A factory that knows how to create an {@link Aggregator} of a specific type.
+ */
+public abstract class AggregatorFactory {
 
-    protected final String name;
-    protected final Type type;
-    protected final AggregatorFactory<?> parent;
-    protected final AggregatorFactories factories;
-    protected final Map<String, Object> metaData;
-    protected final AggregationContext context;
+    protected String name;
+    protected String type;
+    protected AggregatorFactory parent;
+    protected AggregatorFactories factories = AggregatorFactories.EMPTY;
+    protected Map<String, Object> metaData;
 
     /**
      * Constructs a new aggregator factory.
      *
-     * @param name
-     *            The aggregation name
-     * @param type
-     *            The aggregation type
-     * @throws IOException
-     *             if an error occurs creating the factory
+     * @param name  The aggregation name
+     * @param type  The aggregation type
      */
-    public AggregatorFactory(String name, Type type, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
+    public AggregatorFactory(String name, String type) {
         this.name = name;
         this.type = type;
-        this.context = context;
-        this.parent = parent;
-        this.factories = subFactoriesBuilder.build(context, this);
-        this.metaData = metaData;
+    }
+
+    /**
+     * Registers sub-factories with this factory. The sub-factory will be responsible for the creation of sub-aggregators under the
+     * aggregator created by this factory.
+     *
+     * @param subFactories  The sub-factories
+     * @return  this factory (fluent interface)
+     */
+    public AggregatorFactory subFactories(AggregatorFactories subFactories) {
+        this.factories = subFactories;
+        this.factories.setParent(this);
+        return this;
     }
 
     public String name() {
@@ -67,50 +71,52 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> {
     }
 
     /**
-     * Validates the state of this factory (makes sure the factory is properly
-     * configured)
+     * Validates the state of this factory (makes sure the factory is properly configured)
      */
     public final void validate() {
         doValidate();
         factories.validate();
     }
 
-    public void doValidate() {
+    /**
+     * @return  The parent factory if one exists (will always return {@code null} for top level aggregator factories).
+     */
+    public AggregatorFactory parent() {
+        return parent;
     }
 
-    protected abstract Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket,
+    protected abstract Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException;
 
     /**
      * Creates the aggregator
      *
-     * @param parent
-     *            The parent aggregator (if this is a top level factory, the
-     *            parent will be {@code null})
-     * @param collectsFromSingleBucket
-     *            If true then the created aggregator will only be collected
-     *            with <tt>0</tt> as a bucket ordinal. Some factories can take
-     *            advantage of this in order to return more optimized
-     *            implementations.
+     * @param context               The aggregation context
+     * @param parent                The parent aggregator (if this is a top level factory, the parent will be {@code null})
+     * @param collectsFromSingleBucket  If true then the created aggregator will only be collected with <tt>0</tt> as a bucket ordinal.
+     *                              Some factories can take advantage of this in order to return more optimized implementations.
      *
-     * @return The created aggregator
+     * @return                      The created aggregator
      */
-    public final Aggregator create(Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
-        return createInternal(parent, collectsFromSingleBucket, this.factories.createPipelineAggregators(), this.metaData);
+    public final Aggregator create(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
+        return createInternal(context, parent, collectsFromSingleBucket, this.factories.createPipelineAggregators(), this.metaData);
+    }
+
+    public void doValidate() {
     }
 
-    public String getType() {
-        return type.name();
+    public void setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
     }
 
+
+
     /**
-     * Utility method. Given an {@link AggregatorFactory} that creates
-     * {@link Aggregator}s that only know how to collect bucket <tt>0</tt>, this
-     * returns an aggregator that can collect any bucket.
+     * Utility method. Given an {@link AggregatorFactory} that creates {@link Aggregator}s that only know how
+     * to collect bucket <tt>0</tt>, this returns an aggregator that can collect any bucket.
      */
-    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory<?> factory, final AggregationContext context,
-            final Aggregator parent) throws IOException {
-        final Aggregator first = factory.create(parent, true);
+    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory factory, final AggregationContext context, final Aggregator parent) throws IOException {
+        final Aggregator first = factory.create(context, parent, true);
         final BigArrays bigArrays = context.bigArrays();
         return new Aggregator() {
 
@@ -191,7 +197,7 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> {
                         if (collector == null) {
                             Aggregator aggregator = aggregators.get(bucket);
                             if (aggregator == null) {
-                                aggregator = factory.create(parent, true);
+                                aggregator = factory.create(context, parent, true);
                                 aggregator.preCollection();
                                 aggregators.set(bucket, aggregator);
                             }
@@ -228,4 +234,4 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> {
         };
     }
 
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
index 34520ad..f38138f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
@@ -18,14 +18,13 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -55,20 +54,15 @@ public class AggregatorParsers {
      *            ).
      */
     @Inject
-    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers,
-            NamedWriteableRegistry namedWriteableRegistry) {
+    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers) {
         Map<String, Aggregator.Parser> aggParsersBuilder = new HashMap<>(aggParsers.size());
         for (Aggregator.Parser parser : aggParsers) {
             aggParsersBuilder.put(parser.type(), parser);
-            AggregatorBuilder<?> factoryPrototype = parser.getFactoryPrototypes();
-            namedWriteableRegistry.registerPrototype(AggregatorBuilder.class, factoryPrototype);
         }
         this.aggParsers = unmodifiableMap(aggParsersBuilder);
         Map<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = new HashMap<>(pipelineAggregatorParsers.size());
         for (PipelineAggregator.Parser parser : pipelineAggregatorParsers) {
             pipelineAggregatorParsersBuilder.put(parser.type(), parser);
-            PipelineAggregatorBuilder factoryPrototype = parser.getFactoryPrototype();
-            namedWriteableRegistry.registerPrototype(PipelineAggregatorBuilder.class, factoryPrototype);
         }
         this.pipelineAggregatorParsers = unmodifiableMap(pipelineAggregatorParsersBuilder);
     }
@@ -99,50 +93,48 @@ public class AggregatorParsers {
      * Parses the aggregation request recursively generating aggregator factories in turn.
      *
      * @param parser    The input xcontent that will be parsed.
-     * @param parseContext   The parse context.
+     * @param context   The search context.
      *
      * @return          The parsed aggregator factories.
      *
      * @throws IOException When parsing fails for unknown reasons.
      */
-    public AggregatorFactories.Builder parseAggregators(XContentParser parser, QueryParseContext parseContext) throws IOException {
-        return parseAggregators(parser, parseContext, 0);
+    public AggregatorFactories parseAggregators(XContentParser parser, SearchContext context) throws IOException {
+        return parseAggregators(parser, context, 0);
     }
 
 
-    private AggregatorFactories.Builder parseAggregators(XContentParser parser, QueryParseContext parseContext, int level)
-            throws IOException {
+    private AggregatorFactories parseAggregators(XContentParser parser, SearchContext context, int level) throws IOException {
         Matcher validAggMatcher = VALID_AGG_NAME.matcher("");
         AggregatorFactories.Builder factories = new AggregatorFactories.Builder();
 
         XContentParser.Token token = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token != XContentParser.Token.FIELD_NAME) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [aggs]: aggregations definitions must start with the name of the aggregation.");
+                throw new SearchParseException(context, "Unexpected token " + token
+                        + " in [aggs]: aggregations definitions must start with the name of the aggregation.", parser.getTokenLocation());
             }
             final String aggregationName = parser.currentName();
             if (!validAggMatcher.reset(aggregationName).matches()) {
-                throw new ParsingException(parser.getTokenLocation(), "Invalid aggregation name [" + aggregationName
-                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'");
+                throw new SearchParseException(context, "Invalid aggregation name [" + aggregationName
+                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'", parser.getTokenLocation());
             }
 
             token = parser.nextToken();
             if (token != XContentParser.Token.START_OBJECT) {
-                throw new ParsingException(parser.getTokenLocation(), "Aggregation definition for [" + aggregationName + " starts with a ["
-                        + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
+                throw new SearchParseException(context, "Aggregation definition for [" + aggregationName + " starts with a [" + token
+                        + "], expected a [" + XContentParser.Token.START_OBJECT + "].", parser.getTokenLocation());
             }
 
-            AggregatorBuilder<?> aggFactory = null;
-            PipelineAggregatorBuilder pipelineAggregatorFactory = null;
-            AggregatorFactories.Builder subFactories = null;
+            AggregatorFactory aggFactory = null;
+            PipelineAggregatorFactory pipelineAggregatorFactory = null;
+            AggregatorFactories subFactories = null;
 
             Map<String, Object> metaData = null;
 
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token != XContentParser.Token.FIELD_NAME) {
-                    throw new ParsingException(
-                            parser.getTokenLocation(), "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
                             + XContentParser.Token.START_OBJECT + "], but got a [" + token + "] in [" + aggregationName + "]",
                             parser.getTokenLocation());
                 }
@@ -151,8 +143,7 @@ public class AggregatorParsers {
                 token = parser.nextToken();
                 if ("aggregations_binary".equals(fieldName)) {
                     if (subFactories != null) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Found two sub aggregation definitions under [" + aggregationName + "]",
+                        throw new SearchParseException(context, "Found two sub aggregation definitions under [" + aggregationName + "]",
                                 parser.getTokenLocation());
                     }
                     XContentParser binaryParser = null;
@@ -160,17 +151,17 @@ public class AggregatorParsers {
                         byte[] source = parser.binaryValue();
                         binaryParser = XContentFactory.xContent(source).createParser(source);
                     } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.VALUE_STRING + " or " + XContentParser.Token.VALUE_EMBEDDED_OBJECT
-                                        + "] for [" + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.VALUE_STRING + " or "
+                                + XContentParser.Token.VALUE_EMBEDDED_OBJECT + "] for [" + fieldName + "], but got a [" + token + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
                     XContentParser.Token binaryToken = binaryParser.nextToken();
                     if (binaryToken != XContentParser.Token.START_OBJECT) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.START_OBJECT + "] as first token when parsing [" + fieldName
-                                        + "], but got a [" + binaryToken + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT
+                                + "] as first token when parsing [" + fieldName + "], but got a [" + binaryToken + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
-                    subFactories = parseAggregators(binaryParser, parseContext, level + 1);
+                    subFactories = parseAggregators(binaryParser, context, level + 1);
                 } else if (token == XContentParser.Token.START_OBJECT) {
                     switch (fieldName) {
                     case "meta":
@@ -179,66 +170,73 @@ public class AggregatorParsers {
                     case "aggregations":
                     case "aggs":
                         if (subFactories != null) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    "Found two sub aggregation definitions under [" + aggregationName + "]");
+                            throw new SearchParseException(context,
+                                    "Found two sub aggregation definitions under [" + aggregationName + "]", parser.getTokenLocation());
                         }
-                        subFactories = parseAggregators(parser, parseContext, level + 1);
+                        subFactories = parseAggregators(parser, context, level + 1);
                         break;
                     default:
                         if (aggFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + aggFactory.type + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + aggFactory.type + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
                         if (pipelineAggregatorFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
 
                         Aggregator.Parser aggregatorParser = parser(fieldName);
                         if (aggregatorParser == null) {
                             PipelineAggregator.Parser pipelineAggregatorParser = pipelineAggregator(fieldName);
                             if (pipelineAggregatorParser == null) {
-                                throw new ParsingException(parser.getTokenLocation(),
-                                        "Could not find aggregator type [" + fieldName + "] in [" + aggregationName + "]");
+                                throw new SearchParseException(context, "Could not find aggregator type [" + fieldName + "] in ["
+                                        + aggregationName + "]", parser.getTokenLocation());
                             } else {
-                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, parseContext);
+                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, context);
                             }
                         } else {
-                            aggFactory = aggregatorParser.parse(aggregationName, parser, parseContext);
+                            aggFactory = aggregatorParser.parse(aggregationName, parser, context);
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] under ["
-                            + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT + "] under [" + fieldName
+                            + "], but got a [" + token + "] in [" + aggregationName + "]", parser.getTokenLocation());
                 }
             }
 
             if (aggFactory == null && pipelineAggregatorFactory == null) {
-                throw new ParsingException(parser.getTokenLocation(), "Missing definition for aggregation [" + aggregationName + "]",
+                throw new SearchParseException(context, "Missing definition for aggregation [" + aggregationName + "]",
                         parser.getTokenLocation());
             } else if (aggFactory != null) {
                 assert pipelineAggregatorFactory == null;
-                if (metaData != null) {
+            if (metaData != null) {
                     aggFactory.setMetaData(metaData);
-                }
+            }
 
-                if (subFactories != null) {
-                    aggFactory.subAggregations(subFactories);
-                }
+            if (subFactories != null) {
+                    aggFactory.subFactories(subFactories);
+            }
+
+            if (level == 0) {
+                    aggFactory.validate();
+            }
 
                 factories.addAggregator(aggFactory);
             } else {
                 assert pipelineAggregatorFactory != null;
                 if (subFactories != null) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
+                    throw new SearchParseException(context, "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
                             parser.getTokenLocation());
                 }
+                if (level == 0) {
+                    pipelineAggregatorFactory
+                            .validate(null, factories.getAggregatorFactories(), factories.getPipelineAggregatorFactories());
+                }
                 factories.addPipelineAggregator(pipelineAggregatorFactory);
             }
         }
 
-        return factories;
+        return factories.build();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java
new file mode 100644
index 0000000..2d3c0cf
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService.ScriptType;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A base class for all bucket aggregation builders that are based on values (either script generated or field data values)
+ */
+public abstract class ValuesSourceAggregationBuilder<B extends ValuesSourceAggregationBuilder<B>> extends AggregationBuilder<B> {
+
+    private Script script;
+    private String field;
+    @Deprecated
+    private String scriptString;
+    @Deprecated
+    private String lang;
+    @Deprecated
+    private Map<String, Object> params;
+    private Object missing;
+
+    /**
+     * Constructs a new builder.
+     *
+     * @param name  The name of the aggregation.
+     * @param type  The type of the aggregation.
+     */
+    protected ValuesSourceAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Sets the field from which the values will be extracted.
+     *
+     * @param field     The name of the field
+     * @return          This builder (fluent interface support)
+     */
+    @SuppressWarnings("unchecked")
+    public B field(String field) {
+        this.field = field;
+        return (B) this;
+    }
+
+    /**
+     * Sets the script which generates the values. If the script is configured along with the field (as in {@link #field(String)}), then
+     * this script will be treated as a {@code value script}. A <i>value script</i> will be applied on the values that are extracted from
+     * the field data (you can refer to that value in the script using the {@code _value} reserved variable). If only the script is configured
+     * (and the no field is configured next to it), then the script will be responsible to generate the values that will be aggregated.
+     *
+     * @param script    The configured script.
+     * @return          This builder (fluent interface support)
+     */
+    @SuppressWarnings("unchecked")
+    public B script(Script script) {
+        this.script = script;
+        return (B) this;
+    }
+
+    /**
+     * Configure the value to use when documents miss a value.
+     */
+    public B missing(Object missingValue) {
+        this.missing = missingValue;
+        return (B) this;
+    }
+
+    @Override
+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (script == null) {
+            if (scriptString != null) {
+                builder.field("script", new Script(scriptString, ScriptType.INLINE, lang, params));
+            }
+        } else {
+            builder.field("script", script);
+        }
+        if (missing != null) {
+            builder.field("missing", missing);
+        }
+
+        doInternalXContent(builder, builderParams);
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenAggregatorFactory.java
deleted file mode 100644
index cb87f12..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenAggregatorFactory.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.children;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.ParentChild;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ChildrenAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild, ChildrenAggregatorFactory> {
-
-    private final String parentType;
-    private final Query parentFilter;
-    private final Query childFilter;
-
-    public ChildrenAggregatorFactory(String name, Type type, ValuesSourceConfig<ParentChild> config, String parentType, Query childFilter,
-            Query parentFilter, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.parentType = parentType;
-        this.childFilter = childFilter;
-        this.parentFilter = parentFilter;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new NonCollectingAggregator(name, context, parent, pipelineAggregators, metaData) {
-
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return new InternalChildren(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
-            }
-
-        };
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Bytes.WithOrdinals.ParentChild valuesSource, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        long maxOrd = valuesSource.globalMaxOrd(context.searchContext().searcher(), parentType);
-        return new ParentToChildrenAggregator(name, factories, context, parent, parentType, childFilter, parentFilter, valuesSource, maxOrd,
-                pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java
new file mode 100644
index 0000000..dc2670c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.children;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Children} aggregation.
+ */
+public class ChildrenBuilder extends AggregationBuilder<ChildrenBuilder> {
+
+    private String childType;
+
+    /**
+     * Sole constructor.
+     */
+    public ChildrenBuilder(String name) {
+        super(name, InternalChildren.TYPE.name());
+    }
+
+    /**
+     * Set the type of children documents. This parameter is compulsory.
+     */
+    public ChildrenBuilder childType(String childType) {
+        this.childType = childType;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (childType == null) {
+            throw new SearchSourceBuilderException("child_type must be set on children aggregation [" + getName() + "]");
+        }
+        builder.field("type", childType);
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
index 9246ee6..438e872 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
@@ -18,11 +18,18 @@
  */
 package org.elasticsearch.search.aggregations.bucket.children;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.FieldContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +44,7 @@ public class ChildrenParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String childType = null;
 
         XContentParser.Token token;
@@ -49,25 +56,45 @@ public class ChildrenParser implements Aggregator.Parser {
                 if ("type".equals(currentFieldName)) {
                     childType = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (childType == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing [child_type] field for children aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [child_type] field for children aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
+        ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config = new ValuesSourceConfig<>(ValuesSource.Bytes.WithOrdinals.ParentChild.class);
+        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
 
-        return new ParentToChildrenAggregator.ChildrenAggregatorBuilder(aggregationName, childType);
+        String parentType = null;
+        Query parentFilter = null;
+        Query childFilter = null;
+        if (childDocMapper != null) {
+            ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
+            if (!parentFieldMapper.active()) {
+                throw new SearchParseException(context, "[children] no [_parent] field not configured that points to a parent type", parser.getTokenLocation());
             }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ParentToChildrenAggregator.ChildrenAggregatorBuilder(null, null);
+            parentType = parentFieldMapper.type();
+            DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+            if (parentDocMapper != null) {
+                // TODO: use the query API
+                parentFilter = parentDocMapper.typeFilter();
+                childFilter = childDocMapper.typeFilter();
+                ParentChildIndexFieldData parentChildIndexFieldData = context.fieldData().getForField(parentFieldMapper.fieldType());
+                config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData, parentFieldMapper.fieldType()));
+            } else {
+                config.unmapped(true);
+            }
+        } else {
+            config.unmapped(true);
+        }
+        return new ParentToChildrenAggregator.Factory(aggregationName, config, parentType, parentFilter, childFilter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
index f7790e7..63819b9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
@@ -27,47 +27,31 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongArray;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.FieldContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.ParentChild;
 
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 // The RecordingPerReaderBucketCollector assumes per segment recording which isn't the case for this
 // aggregation, for this reason that collector can't be used
 public class ParentToChildrenAggregator extends SingleBucketAggregator {
 
-    static final ParseField TYPE_FIELD = new ParseField("type");
-
     private final String parentType;
     private final Weight childFilter;
     private final Weight parentFilter;
@@ -191,87 +175,39 @@ public class ParentToChildrenAggregator extends SingleBucketAggregator {
         Releasables.close(parentOrdToBuckets, parentOrdToOtherBuckets);
     }
 
-    public static class ChildrenAggregatorBuilder extends ValuesSourceAggregatorBuilder<ParentChild, ChildrenAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild> {
 
-        private String parentType;
-        private final String childType;
-        private Query parentFilter;
-        private Query childFilter;
+        private final String parentType;
+        private final Query parentFilter;
+        private final Query childFilter;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param childType
-         *            the type of children documents
-         */
-        public ChildrenAggregatorBuilder(String name, String childType) {
-            super(name, InternalChildren.TYPE, ValuesSourceType.BYTES, ValueType.STRING);
-            this.childType = childType;
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config, String parentType, Query parentFilter, Query childFilter) {
+            super(name, InternalChildren.TYPE.name(), config);
+            this.parentType = parentType;
+            this.parentFilter = parentFilter;
+            this.childFilter = childFilter;
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ParentChild, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ParentChild> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new ChildrenAggregatorFactory(name, type, config, parentType, childFilter, parentFilter, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
 
-        @Override
-        protected ValuesSourceConfig<ParentChild> resolveConfig(AggregationContext aggregationContext) {
-            ValuesSourceConfig<ParentChild> config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            DocumentMapper childDocMapper = aggregationContext.searchContext().mapperService().documentMapper(childType);
-
-            if (childDocMapper != null) {
-                ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
-                if (!parentFieldMapper.active()) {
-                    throw new IllegalArgumentException("[children] no [_parent] field not configured that points to a parent type");
-                }
-                parentType = parentFieldMapper.type();
-                DocumentMapper parentDocMapper = aggregationContext.searchContext().mapperService().documentMapper(parentType);
-                if (parentDocMapper != null) {
-                    parentFilter = parentDocMapper.typeFilter();
-                    childFilter = childDocMapper.typeFilter();
-                    ParentChildIndexFieldData parentChildIndexFieldData = aggregationContext.searchContext().fieldData()
-                            .getForField(parentFieldMapper.fieldType());
-                    config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData,
-                            parentFieldMapper.fieldType()));
-                } else {
-                    config.unmapped(true);
+                @Override
+                public InternalAggregation buildEmptyAggregation() {
+                    return new InternalChildren(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
                 }
-            } else {
-                config.unmapped(true);
-            }
-            return config;
-        }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TYPE_FIELD.getPreferredName(), childType);
-            return builder;
-        }
-
-        @Override
-        protected ChildrenAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            String childType = in.readString();
-            ChildrenAggregatorBuilder factory = new ChildrenAggregatorBuilder(name, childType);
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeString(childType);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(childType);
+            };
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            ChildrenAggregatorBuilder other = (ChildrenAggregatorBuilder) obj;
-            return Objects.equals(childType, other.childType);
+        protected Aggregator doCreateInternal(ValuesSource.Bytes.WithOrdinals.ParentChild valuesSource,
+                AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            long maxOrd = valuesSource.globalMaxOrd(aggregationContext.searchContext().searcher(), parentType);
+            return new ParentToChildrenAggregator(name, factories, aggregationContext, parent, parentType, childFilter, parentFilter,
+                    valuesSource, maxOrd, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java
new file mode 100644
index 0000000..45d40bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.filter;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Filter} aggregation.
+ */
+public class FilterAggregationBuilder extends AggregationBuilder<FilterAggregationBuilder> {
+
+    private QueryBuilder filter;
+
+    /**
+     * Sole constructor.
+     */
+    public FilterAggregationBuilder(String name) {
+        super(name, InternalFilter.TYPE.name());
+    }
+
+    /**
+     * Set the filter to use, only documents that match this filter will fall
+     * into the bucket defined by this {@link Filter} aggregation.
+     */
+    public FilterAggregationBuilder filter(QueryBuilder filter) {
+        this.filter = filter;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (filter == null) {
+            throw new SearchSourceBuilderException("filter must be set on filter aggregation [" + getName() + "]");
+        }
+        filter.toXContent(builder, params);
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
index 9fae8bf..30c34c3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
@@ -19,17 +19,14 @@
 package org.elasticsearch.search.aggregations.bucket.filter;
 
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
@@ -40,7 +37,6 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate all docs that match a filter.
@@ -85,57 +81,31 @@ public class FilterAggregator extends SingleBucketAggregator {
         return new InternalFilter(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class FilterAggregatorBuilder extends AggregatorBuilder<FilterAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
-        private QueryBuilder<?> filter;
+        private final Query filter;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filter
-         *            Set the filter to use, only documents that match this
-         *            filter will fall into the bucket defined by this
-         *            {@link Filter} aggregation.
-         */
-        public FilterAggregatorBuilder(String name, QueryBuilder<?> filter) {
-            super(name, InternalFilter.TYPE);
+        public Factory(String name, Query filter) {
+            super(name, InternalFilter.TYPE.name());
             this.filter = filter;
         }
 
-        @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent,
-                AggregatorFactories.Builder subFactoriesBuilder) throws IOException {
-            return new FilterAggregatorFactory(name, type, filter, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (filter != null) {
-                filter.toXContent(builder, params);
-            }
-            return builder;
-        }
+        // TODO: refactor in order to initialize the factory once with its parent,
+        // the context, etc. and then have a no-arg lightweight create method
+        // (since create may be called thousands of times)
 
-        @Override
-        protected FilterAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            FilterAggregatorBuilder factory = new FilterAggregatorBuilder(name, in.readQuery());
-            return factory;
-        }
+        private IndexSearcher searcher;
+        private Weight weight;
 
         @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeQuery(filter);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filter);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            FilterAggregatorBuilder other = (FilterAggregatorBuilder) obj;
-            return Objects.equals(filter, other.filter);
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            IndexSearcher contextSearcher = context.searchContext().searcher();
+            if (searcher != contextSearcher) {
+                searcher = contextSearcher;
+                weight = contextSearcher.createNormalizedWeight(filter, false);
+            }
+            return new FilterAggregator(name, weight, factories, context, parent, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregatorFactory.java
deleted file mode 100644
index 13b1e62..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.filter;
-
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Weight;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class FilterAggregatorFactory extends AggregatorFactory<FilterAggregatorFactory> {
-
-    private final Weight weight;
-
-    public FilterAggregatorFactory(String name, Type type, QueryBuilder<?> filterBuilder, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactoriesBuilder, metaData);
-        IndexSearcher contextSearcher = context.searchContext().searcher();
-        Query filter = filterBuilder.toQuery(context.searchContext().getQueryShardContext());
-        weight = contextSearcher.createNormalizedWeight(filter, false);
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        return new FilterAggregator(name, weight, factories, context, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
index 972d29a..d532dc4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
@@ -18,13 +18,12 @@
  */
 package org.elasticsearch.search.aggregations.bucket.filter;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -39,22 +38,10 @@ public class FilterParser implements Aggregator.Parser {
     }
 
     @Override
-    public FilterAggregator.FilterAggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-        QueryBuilder<?> filter = context.parseInnerQueryBuilder();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ParsedQuery filter = context.getQueryShardContext().parseInnerFilter(parser);
 
-        if (filter == null) {
-            throw new ParsingException(null, "filter cannot be null in filter aggregation [{}]", aggregationName);
-        }
-
-        FilterAggregator.FilterAggregatorBuilder factory = new FilterAggregator.FilterAggregatorBuilder(aggregationName,
-                filter == null ? new MatchAllQueryBuilder() : filter);
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new FilterAggregator.FilterAggregatorBuilder(null, null);
+        return new FilterAggregator.Factory(aggregationName, filter == null ? new MatchAllDocsQuery() : filter.query());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
new file mode 100644
index 0000000..6f61a89
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.filters;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Builder for the {@link Filters} aggregation.
+ */
+public class FiltersAggregationBuilder extends AggregationBuilder<FiltersAggregationBuilder> {
+
+    private Map<String, QueryBuilder> keyedFilters = null;
+    private List<QueryBuilder> nonKeyedFilters = null;
+    private Boolean otherBucket;
+    private String otherBucketKey;
+
+    /**
+     * Sole constructor.
+     */
+    public FiltersAggregationBuilder(String name) {
+        super(name, InternalFilters.TYPE.name());
+    }
+
+    /**
+     * Add a new filter with the given key.
+     * NOTE: if a filter was already defined for this key, then this filter will replace it.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters
+     */
+    public FiltersAggregationBuilder filter(String key, QueryBuilder filter) {
+        if (keyedFilters == null) {
+            keyedFilters = new LinkedHashMap<>();
+        }
+        keyedFilters.put(key, filter);
+        return this;
+    }
+
+    /**
+     * Add a new filter with no key.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters.
+     */
+    public FiltersAggregationBuilder filter(QueryBuilder filter) {
+        if (nonKeyedFilters == null) {
+            nonKeyedFilters = new ArrayList<>();
+        }
+        nonKeyedFilters.add(filter);
+        return this;
+    }
+
+    /**
+     * Include a bucket for documents not matching any filter
+     */
+    public FiltersAggregationBuilder otherBucket(boolean otherBucket) {
+        this.otherBucket = otherBucket;
+        return this;
+    }
+
+    /**
+     * The key to use for the bucket for documents not matching any filter. Will
+     * implicitly enable the other bucket if set.
+     */
+    public FiltersAggregationBuilder otherBucketKey(String otherBucketKey) {
+        this.otherBucketKey = otherBucketKey;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (keyedFilters == null && nonKeyedFilters == null) {
+            throw new SearchSourceBuilderException("At least one filter must be set on filter aggregation [" + getName() + "]");
+        }
+        if (keyedFilters != null && nonKeyedFilters != null) {
+            throw new SearchSourceBuilderException("Cannot add both keyed and non-keyed filters to filters aggregation");
+        }
+
+        if (keyedFilters != null) {
+            builder.startObject(FiltersParser.FILTERS_FIELD.getPreferredName());
+            for (Map.Entry<String, QueryBuilder> entry : keyedFilters.entrySet()) {
+                builder.field(entry.getKey());
+                entry.getValue().toXContent(builder, params);
+            }
+            builder.endObject();
+        }
+        if (nonKeyedFilters != null) {
+            builder.startArray(FiltersParser.FILTERS_FIELD.getPreferredName());
+            for (QueryBuilder filterBuilder : nonKeyedFilters) {
+                filterBuilder.toXContent(builder, params);
+            }
+            builder.endArray();
+
+        }
+        if (otherBucketKey != null) {
+            builder.field(FiltersParser.OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
+        }
+        if (otherBucket != null) {
+            builder.field(FiltersParser.OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
index 0a7ae5e..c16089e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
@@ -20,100 +20,41 @@
 package org.elasticsearch.search.aggregations.bucket.filters;
 
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class FiltersAggregator extends BucketsAggregator {
 
-    public static final ParseField FILTERS_FIELD = new ParseField("filters");
-    public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
-    public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
+    static class KeyedFilter {
 
-    public static class KeyedFilter implements Writeable<KeyedFilter>, ToXContent {
+        final String key;
+        final Query filter;
 
-        static final KeyedFilter PROTOTYPE = new KeyedFilter(null, null);
-        private final String key;
-        private final QueryBuilder<?> filter;
-
-        public KeyedFilter(String key, QueryBuilder<?> filter) {
+        KeyedFilter(String key, Query filter) {
             this.key = key;
             this.filter = filter;
         }
-
-        public String key() {
-            return key;
-        }
-
-        public QueryBuilder<?> filter() {
-            return filter;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(key, filter);
-            return builder;
-        }
-
-        @Override
-        public KeyedFilter readFrom(StreamInput in) throws IOException {
-            String key = in.readString();
-            QueryBuilder<?> filter = in.readQuery();
-            return new KeyedFilter(key, filter);
-    }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(key);
-            out.writeQuery(filter);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, filter);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            KeyedFilter other = (KeyedFilter) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(filter, other.filter);
-        }
     }
 
     private final String[] keys;
@@ -198,159 +139,45 @@ public class FiltersAggregator extends BucketsAggregator {
         return owningBucketOrdinal * totalNumKeys + filterOrd;
     }
 
-    public static class FiltersAggregatorBuilder extends AggregatorBuilder<FiltersAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
         private final List<KeyedFilter> filters;
-        private final boolean keyed;
-        private boolean otherBucket = false;
-        private String otherBucketKey = "_other_";
-
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the KeyedFilters to use with this aggregation.
-         */
-        public FiltersAggregatorBuilder(String name, KeyedFilter... filters) {
-            this(name, Arrays.asList(filters));
-        }
+        private final String[] keys;
+        private boolean keyed;
+        private String otherBucketKey;
 
-        private FiltersAggregatorBuilder(String name, List<KeyedFilter> filters) {
-            super(name, InternalFilters.TYPE);
+        public Factory(String name, List<KeyedFilter> filters, boolean keyed, String otherBucketKey) {
+            super(name, InternalFilters.TYPE.name());
             this.filters = filters;
-            this.keyed = true;
-        }
-
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the filters to use with this aggregation
-         */
-        public FiltersAggregatorBuilder(String name, QueryBuilder<?>... filters) {
-            super(name, InternalFilters.TYPE);
-            List<KeyedFilter> keyedFilters = new ArrayList<>(filters.length);
-            for (int i = 0; i < filters.length; i++) {
-                keyedFilters.add(new KeyedFilter(String.valueOf(i), filters[i]));
-            }
-            this.filters = keyedFilters;
-            this.keyed = false;
-        }
-
-        /**
-         * Set whether to include a bucket for documents not matching any filter
-         */
-        public FiltersAggregatorBuilder otherBucket(boolean otherBucket) {
-            this.otherBucket = otherBucket;
-            return this;
-        }
-
-        /**
-         * Get whether to include a bucket for documents not matching any filter
-         */
-        public boolean otherBucket() {
-            return otherBucket;
-        }
-
-        /**
-         * Set the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public FiltersAggregatorBuilder otherBucketKey(String otherBucketKey) {
+            this.keyed = keyed;
             this.otherBucketKey = otherBucketKey;
-            return this;
-        }
-
-        /**
-         * Get the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public String otherBucketKey() {
-            return otherBucketKey;
-        }
-
-        @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new FiltersAggregatorFactory(name, type, filters, keyed, otherBucket, otherBucketKey, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (keyed) {
-                builder.startObject(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.field(keyedFilter.key(), keyedFilter.filter());
-                }
-                builder.endObject();
-            } else {
-                builder.startArray(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.value(keyedFilter.filter());
-                }
-                builder.endArray();
+            this.keys = new String[filters.size()];
+            for (int i = 0; i < filters.size(); ++i) {
+                KeyedFilter keyedFilter = filters.get(i);
+                this.keys[i] = keyedFilter.key;
             }
-            builder.field(OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
-            builder.field(OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
-            builder.endObject();
-            return builder;
         }
 
-        @Override
-        protected FiltersAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            FiltersAggregatorBuilder factory;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<KeyedFilter> filters = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    filters.add(KeyedFilter.PROTOTYPE.readFrom(in));
-                }
-                factory = new FiltersAggregatorBuilder(name, filters);
-            } else {
-                int size = in.readVInt();
-                QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-                for (int i = 0; i < size; i++) {
-                    filters[i] = in.readQuery();
-                }
-                factory = new FiltersAggregatorBuilder(name, filters);
-            }
-            factory.otherBucket = in.readBoolean();
-            factory.otherBucketKey = in.readString();
-            return factory;
-        }
+        // TODO: refactor in order to initialize the factory once with its parent,
+        // the context, etc. and then have a no-arg lightweight create method
+        // (since create may be called thousands of times)
+
+        private IndexSearcher searcher;
+        private Weight[] weights;
 
         @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(keyed);
-            if (keyed) {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    keyedFilter.writeTo(out);
-                }
-            } else {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    out.writeQuery(keyedFilter.filter());
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            IndexSearcher contextSearcher = context.searchContext().searcher();
+            if (searcher != contextSearcher) {
+                searcher = contextSearcher;
+                weights = new Weight[filters.size()];
+                for (int i = 0; i < filters.size(); ++i) {
+                    KeyedFilter keyedFilter = filters.get(i);
+                    this.weights[i] = contextSearcher.createNormalizedWeight(keyedFilter.filter, false);
                 }
             }
-            out.writeBoolean(otherBucket);
-            out.writeString(otherBucketKey);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filters, keyed, otherBucket, otherBucketKey);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            FiltersAggregatorBuilder other = (FiltersAggregatorBuilder) obj;
-            return Objects.equals(filters, other.filters)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(otherBucket, other.otherBucket)
-                    && Objects.equals(otherBucketKey, other.otherBucketKey);
+            return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucketKey, context, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregatorFactory.java
deleted file mode 100644
index 9b7def2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregatorFactory.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.filters;
-
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Weight;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class FiltersAggregatorFactory extends AggregatorFactory<FiltersAggregatorFactory> {
-
-    private final String[] keys;
-    private final Weight[] weights;
-    private final boolean keyed;
-    private final boolean otherBucket;
-    private final String otherBucketKey;
-
-    public FiltersAggregatorFactory(String name, Type type, List<KeyedFilter> filters, boolean keyed, boolean otherBucket,
-            String otherBucketKey, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactories,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.keyed = keyed;
-        this.otherBucket = otherBucket;
-        this.otherBucketKey = otherBucketKey;
-        IndexSearcher contextSearcher = context.searchContext().searcher();
-        weights = new Weight[filters.size()];
-        keys = new String[filters.size()];
-        for (int i = 0; i < filters.size(); ++i) {
-            KeyedFilter keyedFilter = filters.get(i);
-            this.keys[i] = keyedFilter.key();
-            Query filter = keyedFilter.filter().toFilter(context.searchContext().getQueryShardContext());
-            this.weights[i] = contextSearcher.createNormalizedWeight(filter, false);
-        }
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucket ? otherBucketKey : null, context, parent,
-                pipelineAggregators, metaData);
-    }
-
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
index be9eb26..b7e5e6f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
@@ -20,15 +20,13 @@
 package org.elasticsearch.search.aggregations.bucket.filters;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,12 +40,6 @@ public class FiltersParser implements Aggregator.Parser {
     public static final ParseField FILTERS_FIELD = new ParseField("filters");
     public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
     public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
-    private final IndicesQueriesRegistry queriesRegistry;
-
-    @Inject
-    public FiltersParser(IndicesQueriesRegistry queriesRegistry) {
-        this.queriesRegistry = queriesRegistry;
-    }
 
     @Override
     public String type() {
@@ -55,15 +47,15 @@ public class FiltersParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        List<FiltersAggregator.KeyedFilter> keyedFilters = null;
-        List<QueryBuilder<?>> nonKeyedFilters = null;
+        List<FiltersAggregator.KeyedFilter> filters = new ArrayList<>();
 
         XContentParser.Token token = null;
         String currentFieldName = null;
+        Boolean keyed = null;
         String otherBucketKey = null;
-        Boolean otherBucket = false;
+        boolean otherBucket = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -71,53 +63,50 @@ public class FiltersParser implements Aggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_FIELD)) {
                     otherBucket = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_KEY_FIELD)) {
                     otherBucketKey = parser.text();
+                    otherBucket = true;
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    keyedFilters = new ArrayList<>();
+                    keyed = true;
                     String key = null;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             key = parser.currentName();
                         } else {
-                            QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                            queryParseContext.reset(parser);
-                            queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                            QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                            keyedFilters
-                                    .add(new FiltersAggregator.KeyedFilter(key, filter == null ? QueryBuilders.matchAllQuery() : filter));
+                            ParsedQuery filter = context.getQueryShardContext().parseInnerFilter(parser);
+                            filters.add(new FiltersAggregator.KeyedFilter(key, filter == null ? Queries.newMatchAllQuery() : filter.query()));
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    nonKeyedFilters = new ArrayList<>();
+                    keyed = false;
+                    int idx = 0;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                        queryParseContext.reset(parser);
-                        queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                        QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                        nonKeyedFilters.add(filter == null ? QueryBuilders.matchAllQuery() : filter);
+                        ParsedQuery filter = context.getQueryShardContext().parseInnerFilter(parser);
+                        filters.add(new FiltersAggregator.KeyedFilter(String.valueOf(idx), filter == null ? Queries.newMatchAllQuery()
+                                : filter.query()));
+                        idx++;
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
 
@@ -125,24 +114,7 @@ public class FiltersParser implements Aggregator.Parser {
             otherBucketKey = "_other_";
         }
 
-        FiltersAggregator.FiltersAggregatorBuilder factory;
-        if (keyedFilters != null) {
-            factory = new FiltersAggregator.FiltersAggregatorBuilder(aggregationName, keyedFilters.toArray(new FiltersAggregator.KeyedFilter[keyedFilters.size()]));
-        } else {
-            factory = new FiltersAggregator.FiltersAggregatorBuilder(aggregationName, nonKeyedFilters.toArray(new QueryBuilder<?>[nonKeyedFilters.size()]));
-        }
-        if (otherBucket != null) {
-            factory.otherBucket(otherBucket);
-        }
-        if (otherBucketKey != null) {
-            factory.otherBucketKey(otherBucketKey);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new FiltersAggregator.FiltersAggregatorBuilder(null, new FiltersAggregator.KeyedFilter[0]);
+        return new FiltersAggregator.Factory(aggregationName, filters, keyed, otherBucketKey);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
index c00f90c..ef2915f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
@@ -46,10 +46,10 @@ public class GeoHashGridAggregator extends BucketsAggregator {
 
     private final int requiredSize;
     private final int shardSize;
-    private final GeoHashGridParser.GeoGridAggregatorBuilder.CellIdSource valuesSource;
+    private final GeoHashGridParser.GeoGridFactory.CellIdSource valuesSource;
     private final LongHash bucketOrds;
 
-    public GeoHashGridAggregator(String name, AggregatorFactories factories, GeoHashGridParser.GeoGridAggregatorBuilder.CellIdSource valuesSource,
+    public GeoHashGridAggregator(String name, AggregatorFactories factories, GeoHashGridParser.GeoGridFactory.CellIdSource valuesSource,
             int requiredSize, int shardSize, AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
             Map<String, Object> metaData) throws IOException {
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregatorFactory.java
deleted file mode 100644
index 4659060..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregatorFactory.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.geogrid;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridAggregatorBuilder.CellIdSource;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.GeoPoint;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public class GeoHashGridAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoHashGridAggregatorFactory> {
-
-    private final int precision;
-    private final int requiredSize;
-    private final int shardSize;
-
-    public GeoHashGridAggregatorFactory(String name, Type type, ValuesSourceConfig<GeoPoint> config, int precision, int requiredSize,
-            int shardSize, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.precision = precision;
-        this.requiredSize = requiredSize;
-        this.shardSize = shardSize;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
-                Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
-        return new NonCollectingAggregator(name, context, parent, pipelineAggregators, metaData) {
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return aggregation;
-            }
-        };
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        CellIdSource cellIdSource = new CellIdSource(valuesSource, precision);
-        return new GeoHashGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, context, parent,
-                pipelineAggregators, metaData);
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java
new file mode 100644
index 0000000..a1f12f4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.geogrid;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Creates an aggregation based on bucketing points into GeoHashes
+ */
+public class GeoHashGridBuilder extends AggregationBuilder<GeoHashGridBuilder> {
+
+
+    private String field;
+    private int precision = GeoHashGridParams.DEFAULT_PRECISION;
+    private int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+    private int shardSize = 0;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoHashGridBuilder(String name) {
+        super(name, InternalGeoHashGrid.TYPE.name());
+    }
+
+    /**
+     * Set the field to use to get geo points.
+     */
+    public GeoHashGridBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the geohash precision to use for this aggregation. The higher the
+     * precision, the more fine-grained this aggregation will be.
+     */
+    public GeoHashGridBuilder precision(int precision) {
+        this.precision = GeoHashGridParams.checkPrecision(precision);
+        return this;
+    }
+
+    /**
+     * Set the number of buckets to return.
+     */
+    public GeoHashGridBuilder size(int requiredSize) {
+        this.requiredSize = requiredSize;
+        return this;
+    }
+
+    /**
+     * Expert: Set the number of buckets to get on each shard to improve
+     * accuracy.
+     */
+    public GeoHashGridBuilder shardSize(int shardSize) {
+        this.shardSize = shardSize;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        if (precision != GeoHashGridParams.DEFAULT_PRECISION) {
+            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
+        }
+        if (requiredSize != GeoHashGridParams.DEFAULT_MAX_NUM_CELLS) {
+            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
+        }
+        if (shardSize != 0) {
+            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
+        }
+
+        return builder.endObject();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
index b67edfe..6473b5a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
@@ -21,204 +21,133 @@ package org.elasticsearch.search.aggregations.bucket.geogrid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.GeoHashUtils;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortingNumericDocValues;
 import org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregates Geo information into cells determined by geohashes of a given precision.
  * WARNING - for high-precision geohashes it may prove necessary to use a {@link GeoBoundingBoxQueryBuilder}
  * aggregation to focus in on a smaller area to avoid generating too many buckets and using too much RAM
  */
-public class GeoHashGridParser extends GeoPointValuesSourceParser {
-
-    public static final int DEFAULT_PRECISION = 5;
-    public static final int DEFAULT_MAX_NUM_CELLS = 10000;
-
-    public GeoHashGridParser() {
-        super(false, false);
-    }
+public class GeoHashGridParser implements Aggregator.Parser {
 
     @Override
     public String type() {
         return InternalGeoHashGrid.TYPE.name();
     }
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoGridAggregatorBuilder(null);
-    }
 
     @Override
-    protected GeoGridAggregatorBuilder createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoGridAggregatorBuilder factory = new GeoGridAggregatorBuilder(aggregationName);
-        Integer precision = (Integer) otherOptions.get(GeoHashGridParams.FIELD_PRECISION);
-        if (precision != null) {
-            factory.precision(precision);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoHashGrid.TYPE, context).build();
+
+        int precision = GeoHashGridParams.DEFAULT_PRECISION;
+        int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+        int shardSize = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER ||
+                    token == XContentParser.Token.VALUE_STRING) { //Be lenient and also allow numbers enclosed in quotes
+                if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
+                    precision = GeoHashGridParams.checkPrecision(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
+                    requiredSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
+                    shardSize = parser.intValue();
                 }
-        Integer size = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SIZE);
-        if (size != null) {
-            factory.size(size);
-        }
-        Integer shardSize = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SHARD_SIZE);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER || token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
-                otherOptions.put(GeoHashGridParams.FIELD_PRECISION, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SIZE, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SHARD_SIZE, parser.intValue());
-                return true;
-        }
-        }
-        return false;
-        }
-
-    public static class GeoGridAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource.GeoPoint, GeoGridAggregatorBuilder> {
-
-        private int precision = DEFAULT_PRECISION;
-        private int requiredSize = DEFAULT_MAX_NUM_CELLS;
-        private int shardSize = -1;
-
-        public GeoGridAggregatorBuilder(String name) {
-            super(name, InternalGeoHashGrid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-    }
-
-        public GeoGridAggregatorBuilder precision(int precision) {
-            this.precision = GeoHashGridParams.checkPrecision(precision);
-            return this;
+            } else if (token != XContentParser.Token.START_OBJECT) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
 
-        public int precision() {
-            return precision;
+        if (shardSize == 0) {
+            shardSize = Integer.MAX_VALUE;
         }
 
-        public GeoGridAggregatorBuilder size(int size) {
-            this.requiredSize = size;
-            return this;
+        if (requiredSize == 0) {
+            requiredSize = Integer.MAX_VALUE;
         }
 
-        public int size() {
-            return requiredSize;
+        if (shardSize < 0) {
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, context.numberOfShards());
         }
 
-        public GeoGridAggregatorBuilder shardSize(int shardSize) {
-            this.shardSize = shardSize;
-            return this;
+        if (shardSize < requiredSize) {
+            shardSize = requiredSize;
         }
 
-        public int shardSize() {
-            return shardSize;
-        }
+        return new GeoGridFactory(aggregationName, vsParser.config(), precision, requiredSize, shardSize);
 
-        @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource.GeoPoint> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                        throws IOException {
-            int shardSize = this.shardSize;
-            if (shardSize == 0) {
-                shardSize = Integer.MAX_VALUE;
-            }
-
-            int requiredSize = this.requiredSize;
-            if (requiredSize == 0) {
-                requiredSize = Integer.MAX_VALUE;
-            }
+    }
 
-            if (shardSize < 0) {
-                // Use default heuristic to avoid any wrong-ranking caused by distributed counting
-                shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, context.searchContext().numberOfShards());
-            }
 
-            if (shardSize < requiredSize) {
-                shardSize = requiredSize;
-            }
-            return new GeoHashGridAggregatorFactory(name, type, config, precision, requiredSize, shardSize, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
+    static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        @Override
-        protected GeoGridAggregatorBuilder innerReadFrom(
-                String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            GeoGridAggregatorBuilder factory = new GeoGridAggregatorBuilder(name);
-            factory.precision = in.readVInt();
-            factory.requiredSize = in.readVInt();
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
+        private final int precision;
+        private final int requiredSize;
+        private final int shardSize;
 
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(precision);
-            out.writeVInt(requiredSize);
-            out.writeVInt(shardSize);
+        public GeoGridFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, int precision, int requiredSize, int shardSize) {
+            super(name, InternalGeoHashGrid.TYPE.name(), config);
+            this.precision = precision;
+            this.requiredSize = requiredSize;
+            this.shardSize = shardSize;
         }
 
         @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
-            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
-            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
-            return builder;
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
+                    Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
+            return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
+                public InternalAggregation buildEmptyAggregation() {
+                    return aggregation;
+                }
+            };
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            GeoGridAggregatorBuilder other = (GeoGridAggregatorBuilder) obj;
-            if (precision != other.precision) {
-                return false;
-            }
-            if (requiredSize != other.requiredSize) {
-                return false;
-            }
-            if (shardSize != other.shardSize) {
-                return false;
+        protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
+                Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            return true;
-        }
+            CellIdSource cellIdSource = new CellIdSource(valuesSource, precision);
+            return new GeoHashGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, aggregationContext, parent, pipelineAggregators,
+                    metaData);
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(precision, requiredSize, shardSize);
         }
 
         private static class CellValues extends SortingNumericDocValues {
@@ -278,4 +207,4 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
 
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
index 42b9bcc..63f47e1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
@@ -19,16 +19,13 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -71,43 +68,23 @@ public class GlobalAggregator extends SingleBucketAggregator {
         throw new UnsupportedOperationException("global aggregations cannot serve as sub-aggregations, hence should never be called on #buildEmptyAggregations");
     }
 
-    public static class GlobalAggregatorBuilder extends AggregatorBuilder<GlobalAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
-        public GlobalAggregatorBuilder(String name) {
-            super(name, InternalGlobal.TYPE);
+        public Factory(String name) {
+            super(name, InternalGlobal.TYPE.name());
         }
 
         @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new GlobalAggregatorFactory(name, type, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected GlobalAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            return new GlobalAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            // Nothing to write
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return 0;
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (parent != null) {
+                throw new AggregationExecutionException("Aggregation [" + parent.name() + "] cannot have a global " +
+                        "sub-aggregation [" + name + "]. Global aggregations can only be defined as top level aggregations");
+            }
+            if (collectsFromSingleBucket == false) {
+                throw new IllegalStateException();
+            }
+            return new GlobalAggregator(name, factories, context, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregatorFactory.java
deleted file mode 100644
index f92241f..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregatorFactory.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.global;
-
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GlobalAggregatorFactory extends AggregatorFactory<GlobalAggregatorFactory> {
-
-    public GlobalAggregatorFactory(String name, Type type, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        if (parent != null) {
-            throw new AggregationExecutionException("Aggregation [" + parent.name() + "] cannot have a global " + "sub-aggregation [" + name
-                    + "]. Global aggregations can only be defined as top level aggregations");
-        }
-        if (collectsFromSingleBucket == false) {
-            throw new IllegalStateException();
-        }
-        return new GlobalAggregator(name, factories, context, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java
new file mode 100644
index 0000000..3e9f2ba
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.global;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Global} aggregation.
+ */
+public class GlobalBuilder extends AggregationBuilder<GlobalBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public GlobalBuilder(String name) {
+        super(name, InternalGlobal.TYPE.name());
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        return builder.startObject().endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
index cc33153..c70cf0f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
@@ -19,9 +19,9 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -36,14 +36,9 @@ public class GlobalParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         parser.nextToken();
-        return new GlobalAggregator.GlobalAggregatorBuilder(aggregationName);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GlobalAggregator.GlobalAggregatorBuilder(null);
+        return new GlobalAggregator.Factory(aggregationName);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java
deleted file mode 100644
index a53b31a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.histogram;
-
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public abstract class AbstractHistogramAggregatorFactory<AF extends AbstractHistogramAggregatorFactory<AF>>
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
-
-    protected final long interval;
-    protected final long offset;
-    protected final InternalOrder order;
-    protected final boolean keyed;
-    protected final long minDocCount;
-    protected final ExtendedBounds extendedBounds;
-    private final InternalHistogram.Factory<?> histogramFactory;
-
-    public AbstractHistogramAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, long interval, long offset,
-            InternalOrder order, boolean keyed, long minDocCount, ExtendedBounds extendedBounds,
-            InternalHistogram.Factory<?> histogramFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.interval = interval;
-        this.offset = offset;
-        this.order = order;
-        this.keyed = keyed;
-        this.minDocCount = minDocCount;
-        this.extendedBounds = extendedBounds;
-        this.histogramFactory = histogramFactory;
-    }
-
-    public long minDocCount() {
-        return minDocCount;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        Rounding rounding = createRounding();
-        return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, extendedBounds, null, config.formatter(),
-                histogramFactory, context, parent, pipelineAggregators, metaData);
-    }
-
-    protected Rounding createRounding() {
-        if (interval < 1) {
-            throw new ParsingException(null, "[interval] must be 1 or greater for histogram aggregation [" + name() + "]: " + interval);
-        }
-
-        Rounding rounding = new Rounding.Interval(interval);
-        if (offset != 0) {
-            rounding = new Rounding.OffsetRounding(rounding, offset);
-        }
-        return rounding;
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        Rounding rounding = createRounding();
-        // we need to round the bounds given by the user and we have to do it
-        // for every aggregator we create
-        // as the rounding is not necessarily an idempotent operation.
-        // todo we need to think of a better structure to the factory/agtor
-        // code so we won't need to do that
-        ExtendedBounds roundedBounds = null;
-        if (extendedBounds != null) {
-            // we need to process & validate here using the parser
-            extendedBounds.processAndValidate(name, context.searchContext(), config.parser());
-            roundedBounds = extendedBounds.round(rounding);
-        }
-        return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, roundedBounds, valuesSource,
-                config.formatter(), histogramFactory, context, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java
deleted file mode 100644
index 949542a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.histogram;
-
-import org.elasticsearch.common.rounding.DateTimeUnit;
-import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.rounding.TimeZoneRounding;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
-
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public class DateHistogramAggregatorFactory extends AbstractHistogramAggregatorFactory<DateHistogramAggregatorFactory> {
-    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
-    private final DateHistogramInterval dateHistogramInterval;
-
-    static {
-        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
-        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
-        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
-        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
-        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
-        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
-        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
-        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
-        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
-        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
-        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
-        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
-        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
-        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
-        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
-        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
-        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
-        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
-    }
-
-    public DateHistogramAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, long interval,
-            DateHistogramInterval dateHistogramInterval, long offset, InternalOrder order, boolean keyed, long minDocCount,
-            ExtendedBounds extendedBounds, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, interval, offset, order, keyed, minDocCount, extendedBounds, InternalDateHistogram.HISTOGRAM_FACTORY,
-                context, parent, subFactoriesBuilder, metaData);
-        this.dateHistogramInterval = dateHistogramInterval;
-    }
-
-    @Override
-    protected Rounding createRounding() {
-        TimeZoneRounding.Builder tzRoundingBuilder;
-        if (dateHistogramInterval != null) {
-            DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(dateHistogramInterval.toString());
-            if (dateTimeUnit != null) {
-                tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
-            } else {
-                // the interval is a time value?
-                tzRoundingBuilder = TimeZoneRounding.builder(
-                        TimeValue.parseTimeValue(dateHistogramInterval.toString(), null, getClass().getSimpleName() + ".interval"));
-            }
-        } else {
-            // the interval is an integer time value in millis?
-            tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.timeValueMillis(interval));
-        }
-        if (timeZone() != null) {
-            tzRoundingBuilder.timeZone(timeZone());
-        }
-        Rounding rounding = tzRoundingBuilder.offset(offset).build();
-        return rounding;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
new file mode 100644
index 0000000..67caf37
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.histogram;
+
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+import org.joda.time.DateTime;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@code DateHistogram} aggregation.
+ */
+public class DateHistogramBuilder extends ValuesSourceAggregationBuilder<DateHistogramBuilder> {
+
+    private Object interval;
+    private Histogram.Order order;
+    private Long minDocCount;
+    private Object extendedBoundsMin;
+    private Object extendedBoundsMax;
+    private String timeZone;
+    private String format;
+    private String offset;
+
+    /**
+     * Sole constructor.
+     */
+    public DateHistogramBuilder(String name) {
+        super(name, InternalDateHistogram.TYPE.name());
+    }
+
+    /**
+     * Set the interval in milliseconds.
+     */
+    public DateHistogramBuilder interval(long interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Set the interval.
+     */
+    public DateHistogramBuilder interval(DateHistogramInterval interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Set the order by which the buckets will be returned.
+     */
+    public DateHistogramBuilder order(Histogram.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Set the minimum document count per bucket. Buckets with less documents
+     * than this min value will not be returned.
+     */
+    public DateHistogramBuilder minDocCount(long minDocCount) {
+        this.minDocCount = minDocCount;
+        return this;
+    }
+
+    /**
+     * Set the timezone in which to translate dates before computing buckets.
+     */
+    public DateHistogramBuilder timeZone(String timeZone) {
+        this.timeZone = timeZone;
+        return this;
+    }
+
+    /**
+     * @param offset sets the offset of time intervals in this histogram
+     * @return the current builder
+     */
+    public DateHistogramBuilder offset(String offset) {
+       this.offset = offset;
+       return this;
+    }
+
+    /**
+     * Set the format to use for dates.
+     */
+    public DateHistogramBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(Long min, Long max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(String min, String max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(DateTime min, DateTime max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (interval == null) {
+            throw new SearchSourceBuilderException("[interval] must be defined for histogram aggregation [" + getName() + "]");
+        }
+        if (interval instanceof Number) {
+            interval = TimeValue.timeValueMillis(((Number) interval).longValue()).toString();
+        }
+        builder.field("interval", interval);
+
+        if (minDocCount != null) {
+            builder.field("min_doc_count", minDocCount);
+        }
+
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+
+        if (offset != null) {
+            builder.field("offset", offset);
+        }
+
+        if (format != null) {
+            builder.field("format", format);
+        }
+
+        if (extendedBoundsMin != null || extendedBoundsMax != null) {
+            builder.startObject(DateHistogramParser.EXTENDED_BOUNDS.getPreferredName());
+            if (extendedBoundsMin != null) {
+                builder.field("min", extendedBoundsMin);
+            }
+            if (extendedBoundsMax != null) {
+                builder.field("max", extendedBoundsMax);
+            }
+            builder.endObject();
+        }
+
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
index cd2c854..7e99b38 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
@@ -19,17 +19,10 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-import java.util.Objects;
-
 /**
  * The interval the date histogram is based on.
  */
-public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
+public class DateHistogramInterval {
 
     public static final DateHistogramInterval SECOND = new DateHistogramInterval("1s");
     public static final DateHistogramInterval MINUTE = new DateHistogramInterval("1m");
@@ -40,10 +33,6 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public static final DateHistogramInterval QUARTER = new DateHistogramInterval("1q");
     public static final DateHistogramInterval YEAR = new DateHistogramInterval("1y");
 
-    public static final DateHistogramInterval readFromStream(StreamInput in) throws IOException {
-        return SECOND.readFrom(in);
-    }
-
     public static DateHistogramInterval seconds(int sec) {
         return new DateHistogramInterval(sec + "s");
     }
@@ -74,31 +63,4 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public String toString() {
         return expression;
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(expression);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        DateHistogramInterval other = (DateHistogramInterval) obj;
-        return Objects.equals(expression, other.expression);
-    }
-
-    @Override
-    public DateHistogramInterval readFrom(StreamInput in) throws IOException {
-        return new DateHistogramInterval(in.readString());
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(expression);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index cbd2e4f..694abf2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -19,23 +19,55 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramAggregatorBuilder;
+import org.elasticsearch.common.rounding.TimeZoneRounding;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
-public class DateHistogramParser extends HistogramParser {
+public class DateHistogramParser implements Aggregator.Parser {
+
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
+    static final ParseField OFFSET = new ParseField("offset");
+    static final ParseField INTERVAL = new ParseField("interval");
+
+    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
 
-    public DateHistogramParser() {
-        super(true);
+    static {
+        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
+        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
+        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
+        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
     }
 
     @Override
@@ -44,47 +76,127 @@ public class DateHistogramParser extends HistogramParser {
     }
 
     @Override
-    protected Object parseStringInterval(String text) {
-        return new DateHistogramInterval(text);
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected DateHistogramAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.DateHistogramAggregatorBuilder factory = new HistogramAggregator.DateHistogramAggregatorBuilder(aggregationName);
-        Object interval = otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else if (interval instanceof Long) {
-            factory.interval((Long) interval);
-        } else if (interval instanceof DateHistogramInterval) {
-            factory.dateHistogramInterval((DateHistogramInterval) interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateHistogram.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .timezoneAware(true)
+                .build();
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+        boolean keyed = false;
+        long minDocCount = 0;
+        ExtendedBounds extendedBounds = null;
+        InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
+        String interval = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, OFFSET)) {
+                    offset = parseOffset(parser.text());
+                } else if (context.parseFieldMatcher().match(currentFieldName, INTERVAL)) {
+                    interval = parser.text();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            order = resolveOrder(currentFieldName, asc);
+                            //TODO should we throw an error if the value is not "asc" or "desc"???
+                        }
+                    }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.minAsStr = parser.text();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.maxAsStr = parser.text();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else {
+                            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                    + currentFieldName + "].", parser.getTokenLocation());
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
+
+        if (interval == null) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
         }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
+
+        TimeZoneRounding.Builder tzRoundingBuilder;
+        DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(interval);
+        if (dateTimeUnit != null) {
+            tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
+        } else {
+            // the interval is a time value?
+            tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.parseTimeValue(interval, null, getClass().getSimpleName() + ".interval"));
         }
-        return factory;
+
+        Rounding rounding = tzRoundingBuilder
+                .timeZone(vsParser.input().timezone())
+                .offset(offset).build();
+
+        ValuesSourceConfig config = vsParser.config();
+        return new HistogramAggregator.Factory(aggregationName, config, rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalDateHistogram.Factory());
+
     }
 
-    static InternalOrder resolveOrder(String key, boolean asc) {
+    private static InternalOrder resolveOrder(String key, boolean asc) {
         if ("_key".equals(key) || "_time".equals(key)) {
             return (InternalOrder) (asc ? InternalOrder.KEY_ASC : InternalOrder.KEY_DESC);
         }
@@ -94,13 +206,11 @@ public class DateHistogramParser extends HistogramParser {
         return new InternalOrder.Aggregation(key, asc);
     }
 
-    @Override
-    protected long parseStringOffset(String offset) throws IOException {
-        return DateHistogramAggregatorBuilder.parseStringOffset(offset);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return HistogramAggregator.DateHistogramAggregatorBuilder.PROTOTYPE;
+    private long parseOffset(String offset) throws IOException {
+        if (offset.charAt(0) == '-') {
+            return -TimeValue.parseTimeValue(offset.substring(1), null, getClass().getSimpleName() + ".parseOffset").millis();
+        }
+        int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
+        return TimeValue.parseTimeValue(offset.substring(beginIndex), null, getClass().getSimpleName() + ".parseOffset").millis();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
index 91a3d23..c703058 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
@@ -19,32 +19,19 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  *
  */
-public class ExtendedBounds implements ToXContent {
-
-    static final ParseField EXTENDED_BOUNDS_FIELD = new ParseField("extended_bounds");
-    static final ParseField MIN_FIELD = new ParseField("min");
-    static final ParseField MAX_FIELD = new ParseField("max");
-
-    private static final ExtendedBounds PROTOTYPE = new ExtendedBounds();
+public class ExtendedBounds {
 
     Long min;
     Long max;
@@ -54,16 +41,11 @@ public class ExtendedBounds implements ToXContent {
 
     ExtendedBounds() {} //for serialization
 
-    public ExtendedBounds(Long min, Long max) {
+    ExtendedBounds(Long min, Long max) {
         this.min = min;
         this.max = max;
     }
 
-    public ExtendedBounds(String minAsStr, String maxAsStr) {
-        this.minAsStr = minAsStr;
-        this.maxAsStr = maxAsStr;
-    }
-
     void processAndValidate(String aggName, SearchContext context, ValueParser parser) {
         assert parser != null;
         if (minAsStr != null) {
@@ -95,8 +77,6 @@ public class ExtendedBounds implements ToXContent {
         } else {
             out.writeBoolean(false);
         }
-        out.writeOptionalString(minAsStr);
-        out.writeOptionalString(maxAsStr);
     }
 
     static ExtendedBounds readFrom(StreamInput in) throws IOException {
@@ -107,79 +87,6 @@ public class ExtendedBounds implements ToXContent {
         if (in.readBoolean()) {
             bounds.max = in.readLong();
         }
-        bounds.minAsStr = in.readOptionalString();
-        bounds.maxAsStr = in.readOptionalString();
         return bounds;
     }
-
-    public ExtendedBounds fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        XContentParser.Token token = null;
-        String currentFieldName = null;
-        ExtendedBounds extendedBounds = new ExtendedBounds();
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("min".equals(currentFieldName)) {
-                    extendedBounds.minAsStr = parser.text();
-                } else if ("max".equals(currentFieldName)) {
-                    extendedBounds.maxAsStr = parser.text();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (parseFieldMatcher.match(currentFieldName, MIN_FIELD)) {
-                    extendedBounds.min = parser.longValue(true);
-                } else if (parseFieldMatcher.match(currentFieldName, MAX_FIELD)) {
-                    extendedBounds.max = parser.longValue(true);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            }
-        }
-        return extendedBounds;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(EXTENDED_BOUNDS_FIELD.getPreferredName());
-        if (min != null) {
-            builder.field(MIN_FIELD.getPreferredName(), min);
-        } else {
-            builder.field(MIN_FIELD.getPreferredName(), minAsStr);
-        }
-        if (max != null) {
-            builder.field(MAX_FIELD.getPreferredName(), max);
-        } else {
-            builder.field(MAX_FIELD.getPreferredName(), maxAsStr);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(min, max);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ExtendedBounds other = (ExtendedBounds) obj;
-        return Objects.equals(min, other.min)
-                && Objects.equals(min, other.min);
-    }
-
-    public static ExtendedBounds parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        return PROTOTYPE.fromXContent(parser, parseFieldMatcher, aggregationName);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
index 36edf92..d2ca0a9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
@@ -21,31 +21,21 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.CollectionUtil;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.internal.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.LongHash;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -53,14 +43,9 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public class HistogramAggregator extends BucketsAggregator {
 
-    public static final ParseField ORDER_FIELD = new ParseField("order");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
-    public static final ParseField MIN_DOC_COUNT_FIELD = new ParseField("min_doc_count");
-
     private final ValuesSource.Numeric valuesSource;
     private final ValueFormatter formatter;
     private final Rounding rounding;
@@ -159,277 +144,57 @@ public class HistogramAggregator extends BucketsAggregator {
         Releasables.close(bucketOrds);
     }
 
-    public static class HistogramAggregatorBuilder extends AbstractBuilder<HistogramAggregatorBuilder> {
-        public static final HistogramAggregatorBuilder PROTOTYPE = new HistogramAggregatorBuilder("");
-
-        public HistogramAggregatorBuilder(String name) {
-            super(name, InternalHistogram.HISTOGRAM_FACTORY);
-        }
-
-        @Override
-        protected HistogramAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-            return new HistogramAggregatorBuilder(name);
-        }
-
-        @Override
-        protected HistogramAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new HistogramAggregatorFactory(name, type, config, interval, offset, order, keyed, minDocCount, extendedBounds, context,
-                    parent, subFactoriesBuilder, metaData);
-        }
-
-    }
-
-    public static abstract class AbstractBuilder<AB extends AbstractBuilder<AB>>
-            extends ValuesSourceAggregatorBuilder<ValuesSource.Numeric, AB> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
-        protected long interval;
-        protected long offset = 0;
-        protected InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
-        protected boolean keyed = false;
-        protected long minDocCount = 0;
-        protected ExtendedBounds extendedBounds;
+        private final Rounding rounding;
+        private final InternalOrder order;
+        private final boolean keyed;
+        private final long minDocCount;
+        private final ExtendedBounds extendedBounds;
+        private final InternalHistogram.Factory<?> histogramFactory;
 
-        private AbstractBuilder(String name, InternalHistogram.Factory<?> histogramFactory) {
-            super(name, histogramFactory.type(), ValuesSourceType.NUMERIC, histogramFactory.valueType());
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> config,
+                       Rounding rounding, InternalOrder order, boolean keyed, long minDocCount,
+                       ExtendedBounds extendedBounds, InternalHistogram.Factory<?> histogramFactory) {
 
-        public long interval() {
-            return interval;
-        }
-
-        public AB interval(long interval) {
-            if (interval < 1) {
-                throw new IllegalArgumentException("[interval] must be 1 or greater for histogram aggregation [" + name + "]");
-            }
-            this.interval = interval;
-            return (AB) this;
-        }
-
-        public long offset() {
-            return offset;
-        }
-
-        public AB offset(long offset) {
-            this.offset = offset;
-            return (AB) this;
-        }
-
-        public Histogram.Order order() {
-            return order;
-        }
-
-        public AB order(Histogram.Order order) {
-            this.order = (InternalOrder) order;
-            return (AB) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
-        }
-
-        public AB keyed(boolean keyed) {
+            super(name, histogramFactory.type(), config);
+            this.rounding = rounding;
+            this.order = order;
             this.keyed = keyed;
-            return (AB) this;
-        }
-
-        public long minDocCount() {
-            return minDocCount;
-        }
-
-        public AB minDocCount(long minDocCount) {
             this.minDocCount = minDocCount;
-            return (AB) this;
-        }
-
-        public ExtendedBounds extendedBounds() {
-            return extendedBounds;
-        }
-
-        public AB extendedBounds(ExtendedBounds extendedBounds) {
             this.extendedBounds = extendedBounds;
-            return (AB) this;
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-
-            builder.field(Rounding.Interval.INTERVAL_FIELD.getPreferredName());
-            doXContentInterval(builder, params);
-            builder.field(Rounding.OffsetRounding.OFFSET_FIELD.getPreferredName(), offset);
-
-            if (order != null) {
-                builder.field(ORDER_FIELD.getPreferredName());
-                order.toXContent(builder, params);
-            }
-
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-
-            builder.field(MIN_DOC_COUNT_FIELD.getPreferredName(), minDocCount);
-
-            if (extendedBounds != null) {
-                extendedBounds.toXContent(builder, params);
-            }
-
-            return builder;
+            this.histogramFactory = histogramFactory;
         }
 
-        protected XContentBuilder doXContentInterval(XContentBuilder builder, Params params) throws IOException {
-            builder.value(interval);
-            return builder;
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalHistogram.TYPE.name();
-        }
-
-        @Override
-        protected AB innerReadFrom(String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in)
-                throws IOException {
-            AbstractBuilder<AB> factory = createFactoryFromStream(name, in);
-            factory.interval = in.readVLong();
-            factory.offset = in.readLong();
-            if (in.readBoolean()) {
-                factory.order = InternalOrder.Streams.readOrder(in);
-            }
-            factory.keyed = in.readBoolean();
-            factory.minDocCount = in.readVLong();
-            if (in.readBoolean()) {
-                factory.extendedBounds = ExtendedBounds.readFrom(in);
-            }
-            return (AB) factory;
-        }
-
-        protected abstract AB createFactoryFromStream(String name, StreamInput in) throws IOException;
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            writeFactoryToStream(out);
-            out.writeVLong(interval);
-            out.writeLong(offset);
-            boolean hasOrder = order != null;
-            out.writeBoolean(hasOrder);
-            if (hasOrder) {
-                InternalOrder.Streams.writeOrder(order, out);
-            }
-            out.writeBoolean(keyed);
-            out.writeVLong(minDocCount);
-            boolean hasExtendedBounds = extendedBounds != null;
-            out.writeBoolean(hasExtendedBounds);
-            if (hasExtendedBounds) {
-                extendedBounds.writeTo(out);
-            }
-        }
-
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            // Default impl does nothing
-    }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(interval, offset, order, keyed, minDocCount, extendedBounds);
-    }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            AbstractBuilder other = (AbstractBuilder) obj;
-            return Objects.equals(interval, other.interval)
-                    && Objects.equals(offset, other.offset)
-                    && Objects.equals(order, other.order)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(extendedBounds, other.extendedBounds);
-        }
-    }
-
-    public static class DateHistogramAggregatorBuilder extends AbstractBuilder<DateHistogramAggregatorBuilder> {
-
-        public static final DateHistogramAggregatorBuilder PROTOTYPE = new DateHistogramAggregatorBuilder("");
-
-        private DateHistogramInterval dateHistogramInterval;
-
-        public DateHistogramAggregatorBuilder(String name) {
-            super(name, InternalDateHistogram.HISTOGRAM_FACTORY);
-        }
-
-        /**
-         * Set the interval.
-         */
-        public DateHistogramAggregatorBuilder dateHistogramInterval(DateHistogramInterval dateHistogramInterval) {
-            this.dateHistogramInterval = dateHistogramInterval;
-            return this;
-        }
-
-        public DateHistogramAggregatorBuilder offset(String offset) {
-            return offset(parseStringOffset(offset));
-        }
-
-        protected static long parseStringOffset(String offset) {
-            if (offset.charAt(0) == '-') {
-                return -TimeValue.parseTimeValue(offset.substring(1), null, DateHistogramAggregatorBuilder.class.getSimpleName() + ".parseOffset")
-                        .millis();
-            }
-            int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
-            return TimeValue.parseTimeValue(offset.substring(beginIndex), null, DateHistogramAggregatorBuilder.class.getSimpleName() + ".parseOffset")
-                    .millis();
-        }
-
-        public DateHistogramInterval dateHistogramInterval() {
-            return dateHistogramInterval;
-        }
-
-        @Override
-        protected DateHistogramAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new DateHistogramAggregatorFactory(name, type, config, interval, dateHistogramInterval, offset, order, keyed,
-                    minDocCount, extendedBounds, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalDateHistogram.TYPE.name();
+        public long minDocCount() {
+            return minDocCount;
         }
 
         @Override
-        protected XContentBuilder doXContentInterval(XContentBuilder builder, Params params) throws IOException {
-            if (dateHistogramInterval == null) {
-                super.doXContentInterval(builder, params);
-            } else {
-                builder.value(dateHistogramInterval.toString());
-            }
-            return builder;
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, extendedBounds, null, config.formatter(),
+                    histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected DateHistogramAggregatorBuilder createFactoryFromStream(String name, StreamInput in)
-                throws IOException {
-            DateHistogramAggregatorBuilder factory = new DateHistogramAggregatorBuilder(name);
-            if (in.readBoolean()) {
-                factory.dateHistogramInterval = DateHistogramInterval.readFromStream(in);
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            return factory;
-        }
-
-        @Override
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            boolean hasDateInterval = dateHistogramInterval != null;
-            out.writeBoolean(hasDateInterval);
-            if (hasDateInterval) {
-                dateHistogramInterval.writeTo(out);
+            // we need to round the bounds given by the user and we have to do it for every aggregator we crate
+            // as the rounding is not necessarily an idempotent operation.
+            // todo we need to think of a better structure to the factory/agtor code so we won't need to do that
+            ExtendedBounds roundedBounds = null;
+            if (extendedBounds != null) {
+                // we need to process & validate here using the parser
+                extendedBounds.processAndValidate(name, aggregationContext.searchContext(), config.parser());
+                roundedBounds = extendedBounds.round(rounding);
             }
+            return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, roundedBounds, valuesSource,
+                    config.formatter(), histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(super.innerHashCode(), dateHistogramInterval);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            DateHistogramAggregatorBuilder other = (DateHistogramAggregatorBuilder) obj;
-            return super.innerEquals(obj)
-                    && Objects.equals(dateHistogramInterval, other.dateHistogramInterval);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java
deleted file mode 100644
index a862aad..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.histogram;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public class HistogramAggregatorFactory extends AbstractHistogramAggregatorFactory<HistogramAggregatorFactory> {
-
-    public HistogramAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, long interval, long offset,
-            InternalOrder order, boolean keyed, long minDocCount, ExtendedBounds extendedBounds, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, interval, offset, order, keyed, minDocCount, extendedBounds, InternalHistogram.HISTOGRAM_FACTORY, context,
-                parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
new file mode 100644
index 0000000..064e046
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
@@ -0,0 +1,134 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.histogram;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Histogram} aggregation.
+ */
+public class HistogramBuilder extends ValuesSourceAggregationBuilder<HistogramBuilder> {
+
+    private Long interval;
+    private Histogram.Order order;
+    private Long minDocCount;
+    private Long extendedBoundsMin;
+    private Long extendedBoundsMax;
+    private Long offset;
+
+    /**
+     * Constructs a new histogram aggregation builder.
+     *
+     * @param name  The name of the aggregation (will serve as the unique identifier for the aggregation result in the response)
+     */
+    public HistogramBuilder(String name) {
+        super(name, InternalHistogram.TYPE.name());
+    }
+
+    /**
+     * Sets the interval for the histogram.
+     *
+     * @param interval  The interval for the histogram
+     * @return          This builder
+     */
+    public HistogramBuilder interval(long interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Sets the order by which the buckets will be returned.
+     *
+     * @param order The order by which the buckets will be returned
+     * @return      This builder
+     */
+    public HistogramBuilder order(Histogram.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Sets the minimum document count per bucket. Buckets with less documents than this min value will not be returned.
+     *
+     * @param minDocCount   The minimum document count per bucket
+     * @return              This builder
+     */
+    public HistogramBuilder minDocCount(long minDocCount) {
+        this.minDocCount = minDocCount;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public HistogramBuilder extendedBounds(Long min, Long max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set the offset to apply to shift bucket boundaries.
+     */
+    public HistogramBuilder offset(long offset) {
+        this.offset = offset;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (interval == null) {
+            throw new SearchSourceBuilderException("[interval] must be defined for histogram aggregation [" + getName() + "]");
+        }
+        builder.field("interval", interval);
+
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+
+        if (offset != null) {
+            builder.field("offset", offset);
+        }
+
+        if (minDocCount != null) {
+            builder.field("min_doc_count", minDocCount);
+        }
+
+        if (extendedBoundsMin != null || extendedBoundsMax != null) {
+            builder.startObject(HistogramParser.EXTENDED_BOUNDS.getPreferredName());
+            if (extendedBoundsMin != null) {
+                builder.field("min", extendedBoundsMin);
+            }
+            if (extendedBoundsMax != null) {
+                builder.field("max", extendedBoundsMax);
+            }
+            builder.endObject();
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
index 83f00d9..c738251 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
@@ -19,31 +19,24 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.rounding.Rounding;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.aggregations.support.format.ValueParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parses the histogram request
  */
-public class HistogramParser extends NumericValuesSourceParser {
+public class HistogramParser implements Aggregator.Parser {
 
-    public HistogramParser() {
-        super(true, true, false);
-    }
-
-    protected HistogramParser(boolean timezoneAware) {
-        super(true, true, timezoneAware);
-    }
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
 
     @Override
     public String type() {
@@ -51,105 +44,100 @@ public class HistogramParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected HistogramAggregator.AbstractBuilder<?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.HistogramAggregatorBuilder factory = new HistogramAggregator.HistogramAggregatorBuilder(aggregationName);
-        Long interval = (Long) otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else {
-            factory.interval(interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
-        }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
-        }
-        return factory;
-    }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context)
+                .targetValueType(ValueType.NUMERIC)
+                .formattable(true)
+                .build();
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, Rounding.Interval.INTERVAL_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parseStringInterval(parser.text()));
-                    return true;
-                } else {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parser.longValue());
-                    return true;
-                }
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.MIN_DOC_COUNT_FIELD)) {
-                otherOptions.put(HistogramAggregator.MIN_DOC_COUNT_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.KEYED_FIELD)) {
-                otherOptions.put(HistogramAggregator.KEYED_FIELD, parser.booleanValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, Rounding.OffsetRounding.OFFSET_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parseStringOffset(parser.text()));
-                    return true;
+        boolean keyed = false;
+        long minDocCount = 0;
+        InternalOrder order = (InternalOrder) InternalOrder.KEY_ASC;
+        long interval = -1;
+        ExtendedBounds extendedBounds = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if ("interval".equals(currentFieldName)) {
+                    interval = parser.longValue();
+                } else if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else if ("offset".equals(currentFieldName)) {
+                    offset = parser.longValue();
                 } else {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parser.longValue());
-                    return true;
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.ORDER_FIELD)) {
-                InternalOrder order = null;
-                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = parser.currentName();
-                    } else if (token == XContentParser.Token.VALUE_STRING) {
-                        String dir = parser.text();
-                        boolean asc = "asc".equals(dir);
-                        if (!asc && !"desc".equals(dir)) {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown order direction in aggregation ["
-                                    + aggregationName + "]: [" + dir
-                                    + "]. Should be either [asc] or [desc]");
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            if (!asc && !"desc".equals(dir)) {
+                                throw new SearchParseException(context, "Unknown order direction [" + dir + "] in aggregation ["
+                                        + aggregationName + "]. Should be either [asc] or [desc]", parser.getTokenLocation());
+                            }
+                            order = resolveOrder(currentFieldName, asc);
                         }
-                        order = resolveOrder(currentFieldName, asc);
                     }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token.isValue()) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue(true);
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue(true);
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(HistogramAggregator.ORDER_FIELD, order);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, ExtendedBounds.EXTENDED_BOUNDS_FIELD)) {
-                ExtendedBounds extendedBounds = ExtendedBounds.parse(parser, parseFieldMatcher, aggregationName);
-                otherOptions.put(ExtendedBounds.EXTENDED_BOUNDS_FIELD, extendedBounds);
-                return true;
             } else {
-                return false;
+                throw new SearchParseException(context, "Unexpected token " + token + " in aggregation [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
-        } else {
-            return false;
         }
-    }
 
-    protected Object parseStringInterval(String interval) {
-        return Long.valueOf(interval);
-    }
+        if (interval < 1) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
+        }
+
+        Rounding rounding = new Rounding.Interval(interval);
+        if (offset != 0) {
+            rounding = new Rounding.OffsetRounding((Rounding.Interval) rounding, offset);
+        }
+
+        if (extendedBounds != null) {
+            // with numeric histogram, we can process here and fail fast if the bounds are invalid
+            extendedBounds.processAndValidate(aggregationName, context, ValueParser.RAW);
+        }
+
+        return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalHistogram.Factory());
 
-    protected long parseStringOffset(String offset) throws IOException {
-        return Long.valueOf(offset);
     }
 
     static InternalOrder resolveOrder(String key, boolean asc) {
@@ -161,9 +149,4 @@ public class HistogramParser extends NumericValuesSourceParser {
         }
         return new InternalOrder.Aggregation(key, asc);
     }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return HistogramAggregator.HistogramAggregatorBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
index 9808eed..1651886 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -31,7 +30,6 @@ import org.joda.time.DateTimeZone;
  */
 public class InternalDateHistogram {
 
-    public static final Factory HISTOGRAM_FACTORY = new Factory();
     final static Type TYPE = new Type("date_histogram", "dhisto");
 
     static class Bucket extends InternalHistogram.Bucket {
@@ -67,13 +65,8 @@ public class InternalDateHistogram {
         }
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType valueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
index c24ab0d..faca359 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
@@ -34,7 +34,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -52,7 +51,6 @@ import java.util.Map;
 public class InternalHistogram<B extends InternalHistogram.Bucket> extends InternalMultiBucketAggregation<InternalHistogram, B> implements
         Histogram {
 
-    public static final Factory<Bucket> HISTOGRAM_FACTORY = new Factory<Bucket>();
     final static Type TYPE = new Type("histogram", "histo");
 
     private final static AggregationStreams.Stream STREAM = new AggregationStreams.Stream() {
@@ -237,12 +235,8 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
         protected Factory() {
         }
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValueType valueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public InternalHistogram<B> create(String name, List<B> buckets, InternalOrder order, long minDocCount,
@@ -511,7 +505,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
     }
 
     @SuppressWarnings("unchecked")
-    protected static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
+    private static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
         if (factoryType.equals(InternalDateHistogram.TYPE.name())) {
             return (Factory<B>) new InternalDateHistogram.Factory();
         } else if (factoryType.equals(TYPE.name())) {
@@ -523,7 +517,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(factory.type().name());
+        out.writeString(factory.type());
         InternalOrder.Streams.writeOrder(order, out);
         out.writeVLong(minDocCount);
         if (minDocCount == 0) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
index d19a839..9d503a8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;
 
 import java.io.IOException;
 import java.util.Comparator;
-import java.util.Objects;
 
 /**
  * An internal {@link Histogram.Order} strategy which is identified by a unique id.
@@ -65,25 +64,6 @@ class InternalOrder extends Histogram.Order {
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         return builder.startObject().field(key, asc ? "asc" : "desc").endObject();
     }
-    
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, key, asc);
-    }
-    
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(key, other.key)
-                && Objects.equals(asc, other.asc);
-    }
 
     static class Aggregation extends InternalOrder {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
index 758b6c9..1ae7341 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
@@ -20,25 +20,17 @@ package org.elasticsearch.search.aggregations.bucket.missing;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 
 import java.io.IOException;
 import java.util.List;
@@ -89,42 +81,22 @@ public class MissingAggregator extends SingleBucketAggregator {
         return new InternalMissing(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class MissingAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, MissingAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource>  {
 
-        public MissingAggregatorBuilder(String name, ValueType targetValueType) {
-            super(name, InternalMissing.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig valueSourceConfig) {
+            super(name, InternalMissing.TYPE.name(), valueSourceConfig);
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new MissingAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected MissingAggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new MissingAggregator(name, factories, null, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected MissingAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MissingAggregatorBuilder(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected MissingAggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new MissingAggregator(name, factories, valuesSource, aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregatorFactory.java
deleted file mode 100644
index d7b478b..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.missing;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class MissingAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, MissingAggregatorFactory> {
-
-    public MissingAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected MissingAggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        return new MissingAggregator(name, factories, null, context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected MissingAggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new MissingAggregator(name, factories, valuesSource, context, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java
new file mode 100644
index 0000000..9f51fd0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.missing;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Missing} aggregation.
+ */
+public class MissingBuilder extends AggregationBuilder<MissingBuilder> {
+
+    private String field;
+
+    /**
+     * Sole constructor.
+     */
+    public MissingBuilder(String name) {
+        super(name, InternalMissing.TYPE.name());
+    }
+
+    /**
+     * Set the field to count missing values on.
+     */
+    public MissingBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
index 43c611b..6ecdc12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
@@ -18,22 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.bucket.missing;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class MissingParser extends AnyValuesSourceParser {
-
-    public MissingParser() {
-        super(true, true);
-    }
+/**
+ *
+ */
+public class MissingParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -41,19 +38,25 @@ public class MissingParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MissingAggregator.MissingAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MissingAggregator.MissingAggregatorBuilder(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new MissingAggregator.MissingAggregatorBuilder(null, null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalMissing.TYPE, context)
+                .scriptable(false)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new MissingAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
index 4b18620..daad510 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
@@ -28,20 +28,16 @@ import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -49,15 +45,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class NestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private BitSetProducer parentFilter;
     private final Query childFilter;
 
@@ -99,7 +92,7 @@ public class NestedAggregator extends SingleBucketAggregator {
                     // So the trick is to set at the last moment just before needed and we can use its child filter as the
                     // parent filter.
 
-                    // Additional NOTE: Before this logic was performed in the setNextReader(...) method, but the the assumption
+                    // Additional NOTE: Before this logic was performed in the setNextReader(...) method, but the assumption
                     // that aggs instances are constructed in reverse doesn't hold when buckets are constructed lazily during
                     // aggs execution
                     Query parentFilterNotCached = findClosestNestedPath(parent());
@@ -127,7 +120,7 @@ public class NestedAggregator extends SingleBucketAggregator {
             }
         };
     }
-
+        
     @Override
     public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {
         return new InternalNested(name, bucketDocCount(owningBucketOrdinal), bucketAggregations(owningBucketOrdinal), pipelineAggregators(),
@@ -150,64 +143,42 @@ public class NestedAggregator extends SingleBucketAggregator {
         return null;
     }
 
-    public static class NestedAggregatorBuilder extends AggregatorBuilder<NestedAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
         private final String path;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param path
-         *            the path to use for this nested aggregation. The path must
-         *            match the path to a nested object in the mappings.
-         */
-        public NestedAggregatorBuilder(String name, String path) {
-            super(name, InternalNested.TYPE);
+        public Factory(String name, String path) {
+            super(name, InternalNested.TYPE.name());
             this.path = path;
         }
 
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
-        }
-
         @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new NestedAggregatorFactory(name, type, path, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(PATH_FIELD.getPreferredName(), path);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected NestedAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            String path = in.readString();
-            NestedAggregatorBuilder factory = new NestedAggregatorBuilder(name, path);
-            return factory;
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, context, parent);
+            }
+            ObjectMapper objectMapper = context.searchContext().getObjectMapper(path);
+            if (objectMapper == null) {
+                return new Unmapped(name, context, parent, pipelineAggregators, metaData);
+            }
+            if (!objectMapper.nested().isNested()) {
+                throw new AggregationExecutionException("[nested] nested path [" + path + "] is not nested");
+            }
+            return new NestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeString(path);
-        }
+        private final static class Unmapped extends NonCollectingAggregator {
 
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
+            public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                    throws IOException {
+                super(name, context, parent, pipelineAggregators, metaData);
+            }
 
-        @Override
-        protected boolean doEquals(Object obj) {
-            NestedAggregatorBuilder other = (NestedAggregatorBuilder) obj;
-            return Objects.equals(path, other.path);
+            @Override
+            public InternalAggregation buildEmptyAggregation() {
+                return new InternalNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
+            }
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java
deleted file mode 100644
index 5a8f47c..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class NestedAggregatorFactory extends AggregatorFactory<NestedAggregatorFactory> {
-
-    private final String path;
-
-    public NestedAggregatorFactory(String name, Type type, String path, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.path = path;
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        ObjectMapper objectMapper = context.searchContext().getObjectMapper(path);
-        if (objectMapper == null) {
-            return new Unmapped(name, context, parent, pipelineAggregators, metaData);
-        }
-        if (!objectMapper.nested().isNested()) {
-            throw new AggregationExecutionException("[nested] nested path [" + path + "] is not nested");
-        }
-        return new NestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
-    }
-
-    private final static class Unmapped extends NonCollectingAggregator {
-
-        public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException {
-            super(name, context, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        public InternalAggregation buildEmptyAggregation() {
-            return new InternalNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java
new file mode 100644
index 0000000..b375f09
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Nested} aggregation.
+ */
+public class NestedBuilder extends AggregationBuilder<NestedBuilder> {
+
+    private String path;
+
+    /**
+     * Sole constructor.
+     */
+    public NestedBuilder(String name) {
+        super(name, InternalNested.TYPE.name());
+    }
+
+    /**
+     * Set the path to use for this nested aggregation. The path must match
+     * the path to a nested object in the mappings. This parameter is
+     * compulsory.
+     */
+    public NestedBuilder path(String path) {
+        this.path = path;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (path == null) {
+            throw new SearchSourceBuilderException("nested path must be set on nested aggregation [" + getName() + "]");
+        }
+        builder.field("path", path);
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
index e96775e..ddf6bf1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class NestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -46,27 +46,24 @@ public class NestedParser implements Aggregator.Parser {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.VALUE_STRING) {
-                if (context.parseFieldMatcher().match(currentFieldName, NestedAggregator.PATH_FIELD)) {
+                if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (path == null) {
             // "field" doesn't exist, so we fall back to the context of the ancestors
-            throw new ParsingException(parser.getTokenLocation(), "Missing [path] field for nested aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [path] field for nested aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        return new NestedAggregator.NestedAggregatorBuilder(aggregationName, path);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new NestedAggregator.NestedAggregatorBuilder(null, null);
+        return new NestedAggregator.Factory(aggregationName, path);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
index b823af5..1b9363c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
@@ -24,20 +24,17 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -45,15 +42,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ReverseNestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private final Query parentFilter;
     private final BitSetProducer parentBitsetProducer;
 
@@ -100,6 +94,15 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
         };
     }
 
+    private static NestedAggregator findClosestNestedAggregator(Aggregator parent) {
+        for (; parent != null; parent = parent.parent()) {
+            if (parent instanceof NestedAggregator) {
+                return (NestedAggregator) parent;
+            }
+        }
+        return null;
+    }
+
     @Override
     public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {
         return new InternalReverseNested(name, bucketDocCount(owningBucketOrdinal), bucketAggregations(owningBucketOrdinal), pipelineAggregators(),
@@ -115,68 +118,51 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
         return parentFilter;
     }
 
-    public static class ReverseNestedAggregatorBuilder extends AggregatorBuilder<ReverseNestedAggregatorBuilder> {
-
-        private String path;
+    public static class Factory extends AggregatorFactory {
 
-        public ReverseNestedAggregatorBuilder(String name) {
-            super(name, InternalReverseNested.TYPE);
-        }
+        private final String path;
 
-        /**
-         * Set the path to use for this nested aggregation. The path must match
-         * the path to a nested object in the mappings. If it is not specified
-         * then this aggregation will go back to the root document.
-         */
-        public ReverseNestedAggregatorBuilder path(String path) {
+        public Factory(String name, String path) {
+            super(name, InternalReverseNested.TYPE.name());
             this.path = path;
-            return this;
-        }
-
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
         }
 
         @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new ReverseNestedAggregatorFactory(name, type, path, context, parent, subFactoriesBuilder, metaData);
-        }
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            // Early validation
+            NestedAggregator closestNestedAggregator = findClosestNestedAggregator(parent);
+            if (closestNestedAggregator == null) {
+                throw new SearchParseException(context.searchContext(), "Reverse nested aggregation [" + name
+                        + "] can only be used inside a [nested] aggregation", null);
+            }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
+            final ObjectMapper objectMapper;
             if (path != null) {
-                builder.field(PATH_FIELD.getPreferredName(), path);
+                objectMapper = context.searchContext().getObjectMapper(path);
+                if (objectMapper == null) {
+                    return new Unmapped(name, context, parent, pipelineAggregators, metaData);
+                }
+                if (!objectMapper.nested().isNested()) {
+                    throw new AggregationExecutionException("[reverse_nested] nested path [" + path + "] is not nested");
+                }
+            } else {
+                objectMapper = null;
             }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected ReverseNestedAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            ReverseNestedAggregatorBuilder factory = new ReverseNestedAggregatorBuilder(name);
-            factory.path = in.readOptionalString();
-            return factory;
+            return new ReverseNestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(path);
-        }
+        private final static class Unmapped extends NonCollectingAggregator {
 
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
+            public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                    throws IOException {
+                super(name, context, parent, pipelineAggregators, metaData);
+            }
 
-        @Override
-        protected boolean doEquals(Object obj) {
-            ReverseNestedAggregatorBuilder other = (ReverseNestedAggregatorBuilder) obj;
-            return Objects.equals(path, other.path);
+            @Override
+            public InternalAggregation buildEmptyAggregation() {
+                return new InternalReverseNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
+            }
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java
deleted file mode 100644
index b1a4ce7..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.search.SearchParseException;
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ReverseNestedAggregatorFactory extends AggregatorFactory<ReverseNestedAggregatorFactory> {
-
-    private final String path;
-
-    public ReverseNestedAggregatorFactory(String name, Type type, String path, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.path = path;
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        // Early validation
-        NestedAggregator closestNestedAggregator = findClosestNestedAggregator(parent);
-        if (closestNestedAggregator == null) {
-            throw new SearchParseException(context.searchContext(),
-                    "Reverse nested aggregation [" + name + "] can only be used inside a [nested] aggregation", null);
-        }
-
-        final ObjectMapper objectMapper;
-        if (path != null) {
-            objectMapper = context.searchContext().getObjectMapper(path);
-            if (objectMapper == null) {
-                return new Unmapped(name, context, parent, pipelineAggregators, metaData);
-            }
-            if (!objectMapper.nested().isNested()) {
-                throw new AggregationExecutionException("[reverse_nested] nested path [" + path + "] is not nested");
-            }
-        } else {
-            objectMapper = null;
-        }
-        return new ReverseNestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
-    }
-
-    private static NestedAggregator findClosestNestedAggregator(Aggregator parent) {
-        for (; parent != null; parent = parent.parent()) {
-            if (parent instanceof NestedAggregator) {
-                return (NestedAggregator) parent;
-            }
-        }
-        return null;
-    }
-
-    private final static class Unmapped extends NonCollectingAggregator {
-
-        public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException {
-            super(name, context, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        public InternalAggregation buildEmptyAggregation() {
-            return new InternalReverseNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java
new file mode 100644
index 0000000..591655e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ReverseNested} aggregation.
+ */
+public class ReverseNestedBuilder extends AggregationBuilder<ReverseNestedBuilder> {
+
+    private String path;
+
+    /**
+     * Sole constructor.
+     */
+    public ReverseNestedBuilder(String name) {
+        super(name, InternalReverseNested.TYPE.name());
+    }
+
+    /**
+     * Set the path to use for this nested aggregation. The path must match
+     * the path to a nested object in the mappings. If it is not specified
+     * then this aggregation will go back to the root document.
+     */
+    public ReverseNestedBuilder path(String path) {
+        this.path = path;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (path != null) {
+            builder.field("path", path);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
index 59f3232..80ab9f5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class ReverseNestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -49,24 +49,15 @@ public class ReverseNestedParser implements Aggregator.Parser {
                 if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
-        ReverseNestedAggregator.ReverseNestedAggregatorBuilder factory = new ReverseNestedAggregator.ReverseNestedAggregatorBuilder(
-                aggregationName);
-        if (path != null) {
-            factory.path(path);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ReverseNestedAggregator.ReverseNestedAggregatorBuilder(null);
+        return new ReverseNestedAggregator.Factory(aggregationName, path);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java
deleted file mode 100644
index ce394ad..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class AbstractRangeAggregatorFactory<AF extends AbstractRangeAggregatorFactory<AF, R>, R extends Range>
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
-
-    private final InternalRange.Factory<?, ?> rangeFactory;
-    private final List<R> ranges;
-    private final boolean keyed;
-
-    public AbstractRangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, List<R> ranges, boolean keyed,
-            InternalRange.Factory<?, ?> rangeFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.ranges = ranges;
-        this.keyed = keyed;
-        this.rangeFactory = rangeFactory;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new Unmapped(name, ranges, keyed, config.format(), context, parent, rangeFactory, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new RangeAggregator(name, factories, valuesSource, config.format(), rangeFactory, ranges, keyed, context, parent,
-                pipelineAggregators, metaData);
-    }
-
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java
new file mode 100644
index 0000000..c4f0c76
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range;
+
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ *
+ */
+public abstract class AbstractRangeBuilder<B extends AbstractRangeBuilder<B>> extends ValuesSourceAggregationBuilder<B> {
+
+    protected static class Range implements ToXContent {
+
+        private String key;
+        private Object from;
+        private Object to;
+
+        public Range(String key, Object from, Object to) {
+            this.key = key;
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            if (key != null) {
+                builder.field("key", key);
+            }
+            if (from != null) {
+                builder.field("from", from);
+            }
+            if (to != null) {
+                builder.field("to", to);
+            }
+            return builder.endObject();
+        }
+    }
+
+    protected List<Range> ranges = new ArrayList<>();
+
+    protected AbstractRangeBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (ranges.isEmpty()) {
+            throw new SearchSourceBuilderException("at least one range must be defined for range aggregation [" + getName() + "]");
+        }
+        builder.startArray("ranges");
+        for (Range range : ranges) {
+            range.toXContent(builder, params);
+        }
+        return builder.endArray();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
index d96e860..5303d7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
@@ -29,8 +29,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -223,16 +221,8 @@ public class InternalRange<B extends InternalRange.Bucket, R extends InternalRan
 
     public static class Factory<B extends Bucket, R extends InternalRange<B, R>> {
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.NUMERIC;
-        }
-
-        public ValueType getValueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public R create(String name, List<B> ranges, ValueFormatter formatter, boolean keyed, List<PipelineAggregator> pipelineAggregators,
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
index 118a6f3..125fca4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
@@ -20,33 +20,20 @@ package org.elasticsearch.search.aggregations.bucket.range;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.InPlaceMergeSorter;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
@@ -56,38 +43,21 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class RangeAggregator extends BucketsAggregator {
 
-    public static final ParseField RANGES_FIELD = new ParseField("ranges");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
+    public static class Range {
 
-    public static class Range implements Writeable<Range>, ToXContent {
+        public String key;
+        public double from = Double.NEGATIVE_INFINITY;
+        String fromAsStr;
+        public double to = Double.POSITIVE_INFINITY;
+        String toAsStr;
 
-        public static final Range PROTOTYPE = new Range(null, -1, null, -1, null);
-        public static final ParseField KEY_FIELD = new ParseField("key");
-        public static final ParseField FROM_FIELD = new ParseField("from");
-        public static final ParseField TO_FIELD = new ParseField("to");
-
-        protected String key;
-        protected double from = Double.NEGATIVE_INFINITY;
-        protected String fromAsStr;
-        protected double to = Double.POSITIVE_INFINITY;
-        protected String toAsStr;
-
-        public Range(String key, Double from, Double to) {
-            this(key, from == null ? Double.NEGATIVE_INFINITY : from, null, to == null ? Double.POSITIVE_INFINITY : to, null);
-        }
-
-        public Range(String key, String from, String to) {
-            this(key, Double.NEGATIVE_INFINITY, from, Double.POSITIVE_INFINITY, to);
-        }
-
-        protected Range(String key, double from, String fromAsStr, double to, String toAsStr) {
+        public Range(String key, double from, String fromAsStr, double to, String toAsStr) {
             this.key = key;
             this.from = from;
             this.fromAsStr = fromAsStr;
@@ -113,99 +83,6 @@ public class RangeAggregator extends BucketsAggregator {
                 to = parser.parseDouble(toAsStr, context);
             }
         }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (Double.isFinite(from)) {
-                builder.field(FROM_FIELD.getPreferredName(), from);
-            }
-            if (Double.isFinite(to)) {
-                builder.field(TO_FIELD.getPreferredName(), to);
-            }
-            if (fromAsStr != null) {
-                builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-            }
-            if (toAsStr != null) {
-                builder.field(TO_FIELD.getPreferredName(), toAsStr);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Range other = (Range) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(fromAsStr, other.fromAsStr)
-                    && Objects.equals(to, other.to)
-                    && Objects.equals(toAsStr, other.toAsStr);
-        }
     }
 
     final ValuesSource.Numeric valuesSource;
@@ -217,7 +94,7 @@ public class RangeAggregator extends BucketsAggregator {
     final double[] maxTo;
 
     public RangeAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, ValueFormat format,
-            InternalRange.Factory rangeFactory, List<? extends Range> ranges, boolean keyed, AggregationContext aggregationContext,
+            InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed, AggregationContext aggregationContext,
             Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
 
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
@@ -368,13 +245,12 @@ public class RangeAggregator extends BucketsAggregator {
 
     public static class Unmapped extends NonCollectingAggregator {
 
-        private final List<? extends RangeAggregator.Range> ranges;
+        private final List<RangeAggregator.Range> ranges;
         private final boolean keyed;
         private final InternalRange.Factory factory;
         private final ValueFormatter formatter;
 
-        public Unmapped(String name, List<? extends RangeAggregator.Range> ranges, boolean keyed, ValueFormat format,
-                AggregationContext context,
+        public Unmapped(String name, List<RangeAggregator.Range> ranges, boolean keyed, ValueFormat format, AggregationContext context,
                 Aggregator parent, InternalRange.Factory factory, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
 
@@ -400,162 +276,29 @@ public class RangeAggregator extends BucketsAggregator {
         }
     }
 
-    public static abstract class AbstractBuilder<AB extends AbstractBuilder<AB, R>, R extends Range>
-            extends ValuesSourceAggregatorBuilder<ValuesSource.Numeric, AB> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
-        protected final InternalRange.Factory<?, ?> rangeFactory;
-        protected List<R> ranges = new ArrayList<>();
-        protected boolean keyed = false;
+        private final InternalRange.Factory rangeFactory;
+        private final List<Range> ranges;
+        private final boolean keyed;
 
-        protected AbstractBuilder(String name, InternalRange.Factory<?, ?> rangeFactory) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valueSourceConfig, InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
             this.rangeFactory = rangeFactory;
-        }
-
-        public AB addRange(R range) {
-            ranges.add(range);
-            return (AB) this;
-        }
-
-        public List<R> ranges() {
-            return ranges;
-        }
-
-        public AB keyed(boolean keyed) {
+            this.ranges = ranges;
             this.keyed = keyed;
-            return (AB) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-            return builder;
-        }
-
-        @Override
-        protected AB innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            AbstractBuilder<AB, R> factory = createFactoryFromStream(name, in);
-            factory.keyed = in.readBoolean();
-            return (AB) factory;
-        }
-
-        protected abstract AbstractBuilder<AB, R> createFactoryFromStream(String name, StreamInput in) throws IOException;
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
-            }
-            out.writeBoolean(keyed);
         }
 
         @Override
-        protected int innerHashCode() {
-            return Objects.hash(ranges, keyed);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new Unmapped(name, ranges, keyed, config.format(), aggregationContext, parent, rangeFactory, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            AbstractBuilder<AB, R> other = (AbstractBuilder<AB, R>) obj;
-            return Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed);
-        }
-    }
-
-    public static class RangeAggregatorBuilder extends AbstractBuilder<RangeAggregatorBuilder, Range> {
-
-        public RangeAggregatorBuilder(String name) {
-            super(name, InternalRange.FACTORY);
-        }
-
-        /**
-         * Add a new range to this aggregation.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public RangeAggregatorBuilder addRange(String key, double from, double to) {
-            addRange(new Range(key, from, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addRange(String, double, double)} but the key will be
-         * automatically generated based on <code>from</code> and
-         * <code>to</code>.
-         */
-        public RangeAggregatorBuilder addRange(double from, double to) {
-            return addRange(null, from, to);
-        }
-
-        /**
-         * Add a new range with no lower bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public RangeAggregatorBuilder addUnboundedTo(String key, double to) {
-            addRange(new Range(key, null, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedTo(String, double)} but the key will be
-         * computed automatically.
-         */
-        public RangeAggregatorBuilder addUnboundedTo(double to) {
-            return addUnboundedTo(null, to);
-        }
-
-        /**
-         * Add a new range with no upper bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         */
-        public RangeAggregatorBuilder addUnboundedFrom(String key, double from) {
-            addRange(new Range(key, from, null));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-         * computed automatically.
-         */
-        public RangeAggregatorBuilder addUnboundedFrom(double from) {
-            return addUnboundedFrom(null, from);
-        }
-
-        @Override
-        protected RangeAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new RangeAggregatorFactory(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder,
-                    metaData);
-        }
-
-        @Override
-        protected RangeAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-            int size = in.readVInt();
-            RangeAggregatorBuilder factory = new RangeAggregatorBuilder(name);
-            for (int i = 0; i < size; i++) {
-                factory.addRange(Range.PROTOTYPE.readFrom(in));
-            }
-            return factory;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new RangeAggregator(name, factories, valuesSource, config.format(), rangeFactory, ranges, keyed, aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregatorFactory.java
deleted file mode 100644
index 5dec4c4..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregatorFactory.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class RangeAggregatorFactory extends AbstractRangeAggregatorFactory<RangeAggregatorFactory, RangeAggregator.Range> {
-
-    public RangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, List<Range> ranges, boolean keyed,
-            Factory<?, ?> rangeFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
new file mode 100644
index 0000000..c772397
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.range;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder.Range} aggregation.
+ */
+public class RangeBuilder extends AbstractRangeBuilder<RangeBuilder> {
+
+    private String format;
+
+    /**
+     * Sole constructor.
+     */
+    public RangeBuilder(String name) {
+        super(name, InternalRange.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public RangeBuilder addRange(String key, double from, double to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, double, double)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public RangeBuilder addRange(double from, double to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public RangeBuilder addUnboundedTo(String key, double to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * computed automatically.
+     */
+    public RangeBuilder addUnboundedTo(double to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public RangeBuilder addUnboundedFrom(String key, double from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * computed automatically.
+     */
+    public RangeBuilder addUnboundedFrom(double from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    /**
+     * Set the format to use to display values.
+     */
+    public RangeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.doInternalXContent(builder, params);
+        if (format != null) {
+            builder.field("format", format);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
index 084939c..e30b84b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
@@ -18,33 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class RangeParser extends NumericValuesSourceParser {
-
-    public RangeParser() {
-        this(true, true, false);
-    }
-
-    protected RangeParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-        super(scriptable, formattable, timezoneAware);
-    }
+public class RangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -52,49 +41,75 @@ public class RangeParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected RangeAggregator.AbstractBuilder<?, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        RangeAggregator.RangeAggregatorBuilder factory = new RangeAggregator.RangeAggregatorBuilder(aggregationName);
-        List<? extends Range> ranges = (List<? extends Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    Range range = parseRange(parser, parseFieldMatcher);
-                    ranges.add(range);
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalRange.TYPE, context)
+                .formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
-    }
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
+        }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new RangeAggregator.RangeAggregatorBuilder(null);
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorBuilder.java
deleted file mode 100644
index f07e251..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorBuilder.java
+++ /dev/null
@@ -1,266 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.date;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.AbstractBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.joda.time.DateTime;
-
-import java.io.IOException;
-
-public class DateRangeAggregatorBuilder extends AbstractBuilder<DateRangeAggregatorBuilder, RangeAggregator.Range> {
-
-    public DateRangeAggregatorBuilder(String name) {
-        super(name, InternalDateRange.FACTORY);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalDateRange.TYPE.name();
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addRange(String key, String from, String to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorBuilder addRange(String from, String to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String key, String to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, String)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String key, String from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addRange(String key, double from, double to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addRange(String, double, double)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorBuilder addRange(double from, double to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String key, double to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, double)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(double to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String key, double from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(double from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addRange(String key, DateTime from, DateTime to) {
-        addRange(new Range(key, convertDateTime(from), convertDateTime(to)));
-        return this;
-    }
-
-    private Double convertDateTime(DateTime dateTime) {
-        if (dateTime == null) {
-            return null;
-        } else {
-            return (double) dateTime.getMillis();
-        }
-    }
-
-    /**
-     * Same as {@link #addRange(String, DateTime, DateTime)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorBuilder addRange(DateTime from, DateTime to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String key, DateTime to) {
-        addRange(new Range(key, null, convertDateTime(to)));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, DateTime)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(DateTime to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String key, DateTime from) {
-        addRange(new Range(key, convertDateTime(from), null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, DateTime)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(DateTime from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    @Override
-    protected DateRangeAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new DateRangeAggregatorFactory(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder,
-                metaData);
-    }
-
-    @Override
-    protected DateRangeAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        DateRangeAggregatorBuilder factory = new DateRangeAggregatorBuilder(name);
-        for (int i = 0; i < size; i++) {
-            factory.addRange(Range.PROTOTYPE.readFrom(in));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
deleted file mode 100644
index d3bb7ac..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.date;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class DateRangeAggregatorFactory extends AbstractRangeAggregatorFactory<DateRangeAggregatorFactory, Range> {
-
-    public DateRangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, List<Range> ranges, boolean keyed,
-            Factory<?, ?> rangeFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
new file mode 100644
index 0000000..4bd5758
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.range.date;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@code DateRange} aggregation.
+ */
+public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
+
+    private String format;
+
+    /**
+     * Sole constructor.
+     */
+    public DateRangeBuilder(String name) {
+        super(name, InternalDateRange.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public DateRangeBuilder addRange(String key, Object from, Object to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, Object, Object)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public DateRangeBuilder addRange(Object from, Object to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public DateRangeBuilder addUnboundedTo(String key, Object to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, Object)} but the key will be
+     * computed automatically.
+     */
+    public DateRangeBuilder addUnboundedTo(Object to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public DateRangeBuilder addUnboundedFrom(String key, Object from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, Object)} but the key will be
+     * computed automatically.
+     */
+    public DateRangeBuilder addUnboundedFrom(Object from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    /**
+     * Set the format to use to display values.
+     */
+    public DateRangeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.doInternalXContent(builder, params);
+        if (format != null) {
+            builder.field("format", format);
+        }
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
index 928cfa1..940e20a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
@@ -18,25 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.date;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
+import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class DateRangeParser extends RangeParser {
-
-    public DateRangeParser() {
-        super(true, true, true);
-    }
+public class DateRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -44,22 +43,78 @@ public class DateRangeParser extends RangeParser {
     }
 
     @Override
-    protected DateRangeAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        DateRangeAggregatorBuilder factory = new DateRangeAggregatorBuilder(aggregationName);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateRange.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new DateRangeAggregatorBuilder(null);
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalDateRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
index 88568bc..ac2c18e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -116,13 +115,8 @@ public class InternalDateRange extends InternalRange<InternalDateRange.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalDateRange.Bucket, InternalDateRange> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java
new file mode 100644
index 0000000..ae8fc22
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java
@@ -0,0 +1,260 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range.geodistance;
+
+import org.elasticsearch.common.geo.GeoDistance;
+import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.unit.DistanceUnit;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+
+/**
+ * Builder for the {@link GeoDistance} aggregation.
+ */
+public class GeoDistanceBuilder extends AggregationBuilder<GeoDistanceBuilder> {
+
+    /**
+     * A range of values.
+     */
+    public static class Range implements ToXContent {
+
+        private String key;
+        private Double from;
+        private Double to;
+
+        /**
+         * Create a new range.
+         * @param key   the identifier of this range
+         * @param from  the lower bound (inclusive)
+         * @param to    the upper bound (exclusive)
+         */
+        public Range(String key, Double from, Double to) {
+            this.key = key;
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            if (from != null) {
+                builder.field("from", from.doubleValue());
+            }
+            if (to != null) {
+                builder.field("to", to.doubleValue());
+            }
+            if (key != null) {
+                builder.field("key", key);
+            }
+            return builder.endObject();
+        }
+
+    }
+
+    private String field;
+    private DistanceUnit unit;
+    private GeoDistance distanceType;
+    private GeoPoint point;
+
+    private List<Range> ranges = new ArrayList<>();
+
+    /**
+     * Sole constructor.
+     */
+    public GeoDistanceBuilder(String name) {
+        super(name, InternalGeoDistance.TYPE.name());
+    }
+
+    /**
+     * Set the field to use to compute distances.
+     */
+    public GeoDistanceBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the unit to use for distances, default is kilometers.
+     */
+    public GeoDistanceBuilder unit(DistanceUnit unit) {
+        this.unit = unit;
+        return this;
+    }
+
+    /**
+     * Set the {@link GeoDistance distance type} to use, defaults to
+     * {@link GeoDistance#SLOPPY_ARC}.
+     */
+    public GeoDistanceBuilder distanceType(GeoDistance distanceType) {
+        this.distanceType = distanceType;
+        return this;
+    }
+
+    /**
+     * Set the point to calculate distances from using a
+     * <code>lat,lon</code> notation or geohash.
+     */
+    public GeoDistanceBuilder point(String latLon) {
+        return point(GeoPoint.parseFromLatLon(latLon));
+    }
+
+    /**
+     * Set the point to calculate distances from.
+     */
+    public GeoDistanceBuilder point(GeoPoint point) {
+        this.point = point;
+        return this;
+    }
+
+    /**
+     * Set the point to calculate distances from using its geohash.
+     */
+    public GeoDistanceBuilder geohash(String geohash) {
+        if (this.point == null) {
+            this.point = new GeoPoint();
+        }
+        this.point.resetFromGeoHash(geohash);
+        return this;
+    }
+
+    /**
+     * Set the latitude of the point to calculate distances from.
+     */
+    public GeoDistanceBuilder lat(double lat) {
+        if (this.point == null) {
+            point = new GeoPoint();
+        }
+        point.resetLat(lat);
+        return this;
+    }
+
+    /**
+     * Set the longitude of the point to calculate distances from.
+     */
+    public GeoDistanceBuilder lon(double lon) {
+        if (this.point == null) {
+            point = new GeoPoint();
+        }
+        point.resetLon(lon);
+        return this;
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public GeoDistanceBuilder addRange(String key, double from, double to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, double, double)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public GeoDistanceBuilder addRange(double from, double to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public GeoDistanceBuilder addUnboundedTo(String key, double to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * computed automatically.
+     */
+    public GeoDistanceBuilder addUnboundedTo(double to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public GeoDistanceBuilder addUnboundedFrom(String key, double from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * computed automatically.
+     */
+    public GeoDistanceBuilder addUnboundedFrom(double from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (ranges.isEmpty()) {
+            throw new SearchSourceBuilderException("at least one range must be defined for geo_distance aggregation [" + getName() + "]");
+        }
+        if (point == null) {
+            throw new SearchSourceBuilderException("center point must be defined for geo_distance aggregation [" + getName() + "]");
+        }
+
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (unit != null) {
+            builder.field("unit", unit);
+        }
+
+        if (distanceType != null) {
+            builder.field("distance_type", distanceType.name().toLowerCase(Locale.ROOT));
+        }
+
+        builder.startObject("center")
+                .field("lat", point.lat())
+                .field("lon", point.lon())
+                .endObject();
+
+        builder.startArray("ranges");
+        for (Range range : ranges) {
+            range.toXContent(builder, params);
+        }
+        builder.endArray();
+
+        return builder.endObject();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
index c89160c..e110cc1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
@@ -18,372 +18,230 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.geodistance;
 
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.SortedNumericDocValues;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoDistance;
+import org.elasticsearch.common.geo.GeoDistance.FixedSourceDistance;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.index.fielddata.MultiGeoPointValues;
+import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
+import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.aggregations.support.GeoPointParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public class GeoDistanceParser extends GeoPointValuesSourceParser {
+public class GeoDistanceParser implements Aggregator.Parser {
 
     private static final ParseField ORIGIN_FIELD = new ParseField("origin", "center", "point", "por");
-    private static final ParseField UNIT_FIELD = new ParseField("unit");
-    private static final ParseField DISTANCE_TYPE_FIELD = new ParseField("distance_type");
-
-    private GeoPointParser geoPointParser = new GeoPointParser(InternalGeoDistance.TYPE, ORIGIN_FIELD);
-
-    public GeoDistanceParser() {
-        super(true, false);
-    }
 
     @Override
     public String type() {
         return InternalGeoDistance.TYPE.name();
     }
 
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, null, null);
-
-        public Range(String key, Double from, Double to) {
-            super(key(key, from, to), from == null ? 0 : from, to);
-        }
-
-        private static String key(String key, Double from, Double to) {
-            if (key != null) {
-                return key;
-            }
-            StringBuilder sb = new StringBuilder();
-            sb.append((from == null || from == 0) ? "*" : from);
-            sb.append("-");
-            sb.append((to == null || Double.isInfinite(to)) ? "*" : to);
-            return sb.toString();
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, to);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
-    }
-
-    @Override
-    protected GeoDistanceAggregatorBuilder createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoPoint origin = (GeoPoint) otherOptions.get(ORIGIN_FIELD);
-        GeoDistanceAggregatorBuilder factory = new GeoDistanceAggregatorBuilder(aggregationName, origin);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        DistanceUnit unit = (DistanceUnit) otherOptions.get(UNIT_FIELD);
-        if (unit != null) {
-            factory.unit(unit);
-        }
-        GeoDistance distanceType = (GeoDistance) otherOptions.get(DISTANCE_TYPE_FIELD);
-        if (distanceType != null) {
-            factory.distanceType(distanceType);
+    private static String key(String key, double from, double to) {
+        if (key != null) {
+            return key;
         }
-        return factory;
+        StringBuilder sb = new StringBuilder();
+        sb.append(from == 0 ? "*" : from);
+        sb.append("-");
+        sb.append(Double.isInfinite(to) ? "*" : to);
+        return sb.toString();
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (geoPointParser.token(aggregationName, currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, UNIT_FIELD)) {
-                DistanceUnit unit = DistanceUnit.fromString(parser.text());
-                otherOptions.put(UNIT_FIELD, unit);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, DISTANCE_TYPE_FIELD)) {
-                GeoDistance distanceType = GeoDistance.fromString(parser.text());
-                otherOptions.put(DISTANCE_TYPE_FIELD, distanceType);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    String fromAsStr = null;
-                    String toAsStr = null;
-                    double from = 0.0;
-                    double to = Double.POSITIVE_INFINITY;
-                    String key = null;
-                    String toOrFromOrKey = null;
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            toOrFromOrKey = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                from = parser.doubleValue();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                to = parser.doubleValue();
-                            }
-                        } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.KEY_FIELD)) {
-                                key = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                fromAsStr = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                toAsStr = parser.text();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoDistance.TYPE, context).build();
+
+        GeoPointParser geoPointParser = new GeoPointParser(aggregationName, InternalGeoDistance.TYPE, context, ORIGIN_FIELD);
+
+        List<RangeAggregator.Range> ranges = null;
+        DistanceUnit unit = DistanceUnit.DEFAULT;
+        GeoDistance distanceType = GeoDistance.DEFAULT;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (geoPointParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if ("unit".equals(currentFieldName)) {
+                    unit = DistanceUnit.fromString(parser.text());
+                } else if ("distance_type".equals(currentFieldName) || "distanceType".equals(currentFieldName)) {
+                    distanceType = GeoDistance.fromString(parser.text());
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        String fromAsStr = null;
+                        String toAsStr = null;
+                        double from = 0.0;
+                        double to = Double.POSITIVE_INFINITY;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                }
                             }
                         }
+                        ranges.add(new RangeAggregator.Range(key(key, from, to), from, fromAsStr, to, toAsStr));
                     }
-                    if (fromAsStr != null || toAsStr != null) {
-                        ranges.add(new Range(key, Double.parseDouble(fromAsStr), Double.parseDouble(toAsStr)));
-                    } else {
-                        ranges.add(new Range(key, from, to));
-                    }
+                } else  {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    public static class GeoDistanceAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource.GeoPoint, GeoDistanceAggregatorBuilder> {
-
-        private final GeoPoint origin;
-        private final InternalRange.Factory rangeFactory;
-        private List<Range> ranges = new ArrayList<>();
-        private DistanceUnit unit = DistanceUnit.DEFAULT;
-        private GeoDistance distanceType = GeoDistance.DEFAULT;
-        private boolean keyed = false;
-
-        public GeoDistanceAggregatorBuilder(String name, GeoPoint origin) {
-            this(name, origin, InternalGeoDistance.FACTORY);
-        }
-
-        private GeoDistanceAggregatorBuilder(String name, GeoPoint origin, InternalRange.Factory rangeFactory) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
-            this.origin = origin;
-            this.rangeFactory = rangeFactory;
-        }
 
-        public GeoDistanceAggregatorBuilder addRange(Range range) {
-            ranges.add(range);
-            return this;
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        /**
-         * Add a new range to this aggregation.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public GeoDistanceAggregatorBuilder addRange(String key, double from, double to) {
-            ranges.add(new Range(key, from, to));
-            return this;
+        GeoPoint origin = geoPointParser.geoPoint();
+        if (origin == null) {
+            throw new SearchParseException(context, "Missing [origin] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        /**
-         * Same as {@link #addRange(String, double, double)} but the key will be
-         * automatically generated based on <code>from</code> and
-         * <code>to</code>.
-         */
-        public GeoDistanceAggregatorBuilder addRange(double from, double to) {
-            return addRange(null, from, to);
-        }
-
-        /**
-         * Add a new range with no lower bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedTo(String key, double to) {
-            ranges.add(new Range(key, null, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedTo(String, double)} but the key will be
-         * computed automatically.
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedTo(double to) {
-            return addUnboundedTo(null, to);
-        }
-
-        /**
-         * Add a new range with no upper bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedFrom(String key, double from) {
-            addRange(new Range(key, from, null));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-         * computed automatically.
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedFrom(double from) {
-            return addUnboundedFrom(null, from);
-        }
+        return new GeoDistanceFactory(aggregationName, vsParser.config(), InternalGeoDistance.FACTORY, origin, unit, distanceType, ranges, keyed);
+    }
 
-        public List<Range> range() {
-            return ranges;
-        }
+    private static class GeoDistanceFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        @Override
-        public String getWriteableName() {
-            return InternalGeoDistance.TYPE.name();
-        }
+        private final GeoPoint origin;
+        private final DistanceUnit unit;
+        private final GeoDistance distanceType;
+        private final InternalRange.Factory rangeFactory;
+        private final List<RangeAggregator.Range> ranges;
+        private final boolean keyed;
 
-        public GeoDistanceAggregatorBuilder unit(DistanceUnit unit) {
+        public GeoDistanceFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> valueSourceConfig,
+                                  InternalRange.Factory rangeFactory, GeoPoint origin, DistanceUnit unit, GeoDistance distanceType,
+                                  List<RangeAggregator.Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
+            this.origin = origin;
             this.unit = unit;
-            return this;
-        }
-
-        public DistanceUnit unit() {
-            return unit;
-        }
-
-        public GeoDistanceAggregatorBuilder distanceType(GeoDistance distanceType) {
             this.distanceType = distanceType;
-            return this;
-        }
-
-        public GeoDistance distanceType() {
-            return distanceType;
-        }
-
-        public GeoDistanceAggregatorBuilder keyed(boolean keyed) {
+            this.rangeFactory = rangeFactory;
+            this.ranges = ranges;
             this.keyed = keyed;
-            return this;
-        }
-
-        public boolean keyed() {
-            return keyed;
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource.GeoPoint> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                        throws IOException {
-            return new GeoDistanceRangeAggregatorFactory(name, type, config, origin, ranges, unit, distanceType, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new Unmapped(name, ranges, keyed, config.format(), aggregationContext, parent, rangeFactory, pipelineAggregators,
+                    metaData);
         }
 
         @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ORIGIN_FIELD.getPreferredName(), origin);
-            builder.field(RangeAggregator.RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(RangeAggregator.KEYED_FIELD.getPreferredName(), keyed);
-            builder.field(UNIT_FIELD.getPreferredName(), unit);
-            builder.field(DISTANCE_TYPE_FIELD.getPreferredName(), distanceType);
-            return builder;
-        }
+        protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
+                Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData)
+                throws IOException {
+            DistanceSource distanceSource = new DistanceSource(valuesSource, distanceType, origin, unit);
+            return new RangeAggregator(name, factories, distanceSource, config.format(), rangeFactory, ranges, keyed, aggregationContext,
+                    parent,
+                    pipelineAggregators, metaData);
+        }
+
+        private static class DistanceSource extends ValuesSource.Numeric {
+
+            private final ValuesSource.GeoPoint source;
+            private final GeoDistance distanceType;
+            private final DistanceUnit unit;
+            private final org.elasticsearch.common.geo.GeoPoint origin;
+
+            public DistanceSource(ValuesSource.GeoPoint source, GeoDistance distanceType, org.elasticsearch.common.geo.GeoPoint origin, DistanceUnit unit) {
+                this.source = source;
+                // even if the geo points are unique, there's no guarantee the distances are
+                this.distanceType = distanceType;
+                this.unit = unit;
+                this.origin = origin;
+            }
 
-        @Override
-        protected GeoDistanceAggregatorBuilder innerReadFrom(
-                String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in) throws IOException {
-            GeoPoint origin = new GeoPoint(in.readDouble(), in.readDouble());
-            int size = in.readVInt();
-            GeoDistanceAggregatorBuilder factory = new GeoDistanceAggregatorBuilder(name, origin);
-            for (int i = 0; i < size; i++) {
-                factory.addRange(Range.PROTOTYPE.readFrom(in));
+            @Override
+            public boolean isFloatingPoint() {
+                return true;
             }
-            factory.keyed = in.readBoolean();
-            factory.distanceType = GeoDistance.readGeoDistanceFrom(in);
-            factory.unit = DistanceUnit.readDistanceUnit(in);
-            return factory;
-        }
 
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(origin.lat());
-            out.writeDouble(origin.lon());
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
+            @Override
+            public SortedNumericDocValues longValues(LeafReaderContext ctx) {
+                throw new UnsupportedOperationException();
             }
-            out.writeBoolean(keyed);
-            distanceType.writeTo(out);
-            DistanceUnit.writeDistanceUnit(out, unit);
-        }
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(origin, ranges, keyed, distanceType, unit);
-        }
+            @Override
+            public SortedNumericDoubleValues doubleValues(LeafReaderContext ctx) {
+                final MultiGeoPointValues geoValues = source.geoPointValues(ctx);
+                final FixedSourceDistance distance = distanceType.fixedSourceDistance(origin.getLat(), origin.getLon(), unit);
+                return GeoDistance.distanceValues(geoValues, distance);
+            }
 
-        @Override
-        protected boolean innerEquals(Object obj) {
-            GeoDistanceAggregatorBuilder other = (GeoDistanceAggregatorBuilder) obj;
-            return Objects.equals(origin, other.origin)
-                    && Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(distanceType, other.distanceType)
-                    && Objects.equals(unit, other.unit);
-        }
+            @Override
+            public SortedBinaryDocValues bytesValues(LeafReaderContext ctx) {
+                throw new UnsupportedOperationException();
+            }
 
-    }
+        }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoDistanceAggregatorBuilder(null, null);
     }
 
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceRangeAggregatorFactory.java
deleted file mode 100644
index 7572e6c..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceRangeAggregatorFactory.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.geodistance;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoDistance.FixedSourceDistance;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.index.fielddata.MultiGeoPointValues;
-import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
-import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.Range;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GeoDistanceRangeAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoDistanceRangeAggregatorFactory> {
-
-    private final InternalRange.Factory<InternalGeoDistance.Bucket, InternalGeoDistance> rangeFactory = InternalGeoDistance.FACTORY;
-    private final GeoPoint origin;
-    private final List<Range> ranges;
-    private final DistanceUnit unit;
-    private final GeoDistance distanceType;
-    private final boolean keyed;
-
-    public GeoDistanceRangeAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource.GeoPoint> config, GeoPoint origin,
-            List<Range> ranges, DistanceUnit unit, GeoDistance distanceType, boolean keyed, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.origin = origin;
-        this.ranges = ranges;
-        this.unit = unit;
-        this.distanceType = distanceType;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new Unmapped(name, ranges, keyed, config.format(), context, parent, rangeFactory, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        DistanceSource distanceSource = new DistanceSource(valuesSource, distanceType, origin, unit);
-        return new RangeAggregator(name, factories, distanceSource, config.format(), rangeFactory, ranges, keyed, context,
-                parent,
-                pipelineAggregators, metaData);
-    }
-
-    private static class DistanceSource extends ValuesSource.Numeric {
-
-        private final ValuesSource.GeoPoint source;
-        private final GeoDistance distanceType;
-        private final DistanceUnit unit;
-        private final org.elasticsearch.common.geo.GeoPoint origin;
-
-        public DistanceSource(ValuesSource.GeoPoint source, GeoDistance distanceType, org.elasticsearch.common.geo.GeoPoint origin,
-                DistanceUnit unit) {
-            this.source = source;
-            // even if the geo points are unique, there's no guarantee the
-            // distances are
-            this.distanceType = distanceType;
-            this.unit = unit;
-            this.origin = origin;
-        }
-
-        @Override
-        public boolean isFloatingPoint() {
-            return true;
-        }
-
-        @Override
-        public SortedNumericDocValues longValues(LeafReaderContext ctx) {
-            throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public SortedNumericDoubleValues doubleValues(LeafReaderContext ctx) {
-            final MultiGeoPointValues geoValues = source.geoPointValues(ctx);
-            final FixedSourceDistance distance = distanceType.fixedSourceDistance(origin.getLat(), origin.getLon(), unit);
-            return GeoDistance.distanceValues(geoValues, distance);
-        }
-
-        @Override
-        public SortedBinaryDocValues bytesValues(LeafReaderContext ctx) {
-            throw new UnsupportedOperationException();
-        }
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
index 2c16d93..c5c67df 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
@@ -26,8 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -106,18 +104,8 @@ public class InternalGeoDistance extends InternalRange<InternalGeoDistance.Bucke
     public static class Factory extends InternalRange.Factory<InternalGeoDistance.Bucket, InternalGeoDistance> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.GEOPOINT;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.GEOPOINT;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorBuilder.java
deleted file mode 100644
index f4b7a78..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorBuilder.java
+++ /dev/null
@@ -1,280 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.ipv4;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.AbstractBuilder;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public class IPv4RangeAggregatorBuilder extends AbstractBuilder<IPv4RangeAggregatorBuilder, IPv4RangeAggregatorBuilder.Range> {
-
-    public IPv4RangeAggregatorBuilder(String name) {
-        super(name, InternalIPv4Range.FACTORY);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalIPv4Range.TYPE.name();
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     * @param to
-     *            the upper bound on the distances, exclusive
-     */
-    public IPv4RangeAggregatorBuilder addRange(String key, String from, String to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as
-     * a key.
-     */
-    public IPv4RangeAggregatorBuilder addMaskRange(String key, String mask) {
-        return addRange(new Range(key, mask));
-    }
-
-    /**
-     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as
-     * a key.
-     */
-    public IPv4RangeAggregatorBuilder addMaskRange(String mask) {
-        return addRange(new Range(mask, mask));
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but the key will be
-     * automatically generated.
-     */
-    public IPv4RangeAggregatorBuilder addRange(String from, String to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but there will be no
-     * lower bound.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedTo(String key, String to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, String)} but the key will be
-     * generated automatically.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedTo(String to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but there will be no
-     * upper bound.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedFrom(String key, String from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
-     * generated automatically.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedFrom(String from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    @Override
-    protected Ipv4RangeAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new Ipv4RangeAggregatorFactory(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder,
-                metaData);
-    }
-
-    @Override
-    protected IPv4RangeAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        IPv4RangeAggregatorBuilder factory = new IPv4RangeAggregatorBuilder(name);
-        for (int i = 0; i < size; i++) {
-            factory.addRange(Range.PROTOTYPE.readFrom(in));
-        }
-        return factory;
-    }
-
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, -1, null, -1, null, null);
-        static final ParseField MASK_FIELD = new ParseField("mask");
-
-        private String cidr;
-
-        public Range(String key, double from, double to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String from, String to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String cidr) {
-            super(key, -1, null, -1, null);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        private Range(String key, double from, String fromAsStr, double to, String toAsStr, String cidr) {
-            super(key, from, fromAsStr, to, toAsStr);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        public String mask() {
-            return cidr;
-        }
-
-        private void parseMaskRange() throws IllegalArgumentException {
-            long[] fromTo = Cidrs.cidrMaskToMinMax(cidr);
-            from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
-            to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
-            if (key == null) {
-                key = cidr;
-            }
-        }
-
-        @Override
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            String cidr = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, MASK_FIELD)) {
-                        cidr = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr, cidr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (cidr != null) {
-                builder.field(MASK_FIELD.getPreferredName(), cidr);
-            } else {
-                if (Double.isFinite(from)) {
-                    builder.field(FROM_FIELD.getPreferredName(), from);
-                }
-                if (Double.isFinite(to)) {
-                    builder.field(TO_FIELD.getPreferredName(), to);
-                }
-                if (fromAsStr != null) {
-                    builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-                }
-                if (toAsStr != null) {
-                    builder.field(TO_FIELD.getPreferredName(), toAsStr);
-                }
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            String mask = in.readOptionalString();
-            return new Range(key, from, fromAsStr, to, toAsStr, mask);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-            out.writeOptionalString(cidr);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(super.hashCode(), cidr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            return super.equals(obj)
-                    && Objects.equals(cidr, ((Range) obj).cidr);
-        }
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
new file mode 100644
index 0000000..5ac3f2a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
@@ -0,0 +1,110 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range.ipv4;
+
+import org.elasticsearch.common.network.Cidrs;
+import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+/**
+ * Builder for the {@code IPv4Range} aggregation.
+ */
+public class IPv4RangeBuilder extends AbstractRangeBuilder<IPv4RangeBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public IPv4RangeBuilder(String name) {
+        super(name, InternalIPv4Range.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public IPv4RangeBuilder addRange(String key, String from, String to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as a key.
+     */
+    public IPv4RangeBuilder addMaskRange(String mask) {
+        return addMaskRange(mask, mask);
+    }
+
+    /**
+     * Add a range based on a CIDR mask.
+     */
+    public IPv4RangeBuilder addMaskRange(String key, String mask) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(mask);
+        } catch (IllegalArgumentException e) {
+            throw new SearchSourceBuilderException("invalid CIDR mask [" + mask + "] in ip_range aggregation [" + getName() + "]", e);
+        }
+        ranges.add(new Range(key, fromTo[0] == 0 ? null : fromTo[0], fromTo[1] == InternalIPv4Range.MAX_IP ? null : fromTo[1]));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but the key will be
+     * automatically generated.
+     */
+    public IPv4RangeBuilder addRange(String from, String to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but there will be no lower bound.
+     */
+    public IPv4RangeBuilder addUnboundedTo(String key, String to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, String)} but the key will be
+     * generated automatically.
+     */
+    public IPv4RangeBuilder addUnboundedTo(String to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but there will be no upper bound.
+     */
+    public IPv4RangeBuilder addUnboundedFrom(String key, String from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
+     * generated automatically.
+     */
+    public IPv4RangeBuilder addUnboundedFrom(String from) {
+        return addUnboundedFrom(null, from);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
index a6c3ed3..e20d1ac 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -112,13 +111,8 @@ public class InternalIPv4Range extends InternalRange<InternalIPv4Range.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalIPv4Range.Bucket, InternalIPv4Range> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.IP;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
index 0381ff0..dc1e2a6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
@@ -18,28 +18,25 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.ipv4;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.network.Cidrs;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class IpRangeParser extends RangeParser {
-
-    public IpRangeParser() {
-        super(true, false, false);
-    }
+public class IpRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -47,29 +44,99 @@ public class IpRangeParser extends RangeParser {
     }
 
     @Override
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return IPv4RangeAggregatorBuilder.Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalIPv4Range.TYPE, context)
+                .targetValueType(ValueType.IP)
+                .formattable(false)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String mask = null;
+                        String toOrFromOrMaskOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrMaskOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrMaskOrKey)) {
+                                    key = parser.text();
+                                } else if ("mask".equals(toOrFromOrMaskOrKey)) {
+                                    mask = parser.text();
+                                }
+                            }
+                        }
+                        RangeAggregator.Range range = new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr);
+                        if (mask != null) {
+                            parseMaskRange(mask, range, aggregationName, context);
+                        }
+                        ranges.add(range);
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
+        }
 
-    @Override
-    protected IPv4RangeAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        IPv4RangeAggregatorBuilder factory = new IPv4RangeAggregatorBuilder(aggregationName);
-        List<IPv4RangeAggregatorBuilder.Range> ranges = (List<IPv4RangeAggregatorBuilder.Range>) otherOptions
-                .get(RangeAggregator.RANGES_FIELD);
-        for (IPv4RangeAggregatorBuilder.Range range : ranges) {
-            factory.addRange(range);
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalIPv4Range.FACTORY, ranges, keyed);
+    }
+
+    private static void parseMaskRange(String cidr, RangeAggregator.Range range, String aggregationName, SearchContext ctx) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(cidr);
+        } catch (IllegalArgumentException e) {
+            throw new SearchParseException(ctx, "invalid CIDR mask [" + cidr + "] in aggregation [" + aggregationName + "]",
+                    null, e);
         }
-        return factory;
+        range.from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
+        range.to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
+        if (range.key == null) {
+            range.key = cidr;
         }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new IPv4RangeAggregatorBuilder(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/Ipv4RangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/Ipv4RangeAggregatorFactory.java
deleted file mode 100644
index 1c05935..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/Ipv4RangeAggregatorFactory.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.ipv4;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange.Factory;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class Ipv4RangeAggregatorFactory
-        extends AbstractRangeAggregatorFactory<Ipv4RangeAggregatorFactory, IPv4RangeAggregatorBuilder.Range> {
-
-    public Ipv4RangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config,
-            List<IPv4RangeAggregatorBuilder.Range> ranges, boolean keyed, Factory<?, ?> rangeFactory, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedAggregatorFactory.java
deleted file mode 100644
index a391351..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedAggregatorFactory.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.ExecutionMode;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class DiversifiedAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, DiversifiedAggregatorFactory> {
-
-    private final int shardSize;
-    private final int maxDocsPerValue;
-    private final String executionHint;
-
-    public DiversifiedAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, int shardSize, int maxDocsPerValue,
-            String executionHint, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.shardSize = shardSize;
-        this.maxDocsPerValue = maxDocsPerValue;
-        this.executionHint = executionHint;
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-
-        if (valuesSource instanceof ValuesSource.Numeric) {
-            return new DiversifiedNumericSamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData,
-                    (Numeric) valuesSource, maxDocsPerValue);
-        }
-
-        if (valuesSource instanceof ValuesSource.Bytes) {
-            ExecutionMode execution = null;
-            if (executionHint != null) {
-                execution = ExecutionMode.fromString(executionHint, context.searchContext().parseFieldMatcher());
-            }
-
-            // In some cases using ordinals is just not supported: override
-            // it
-            if (execution == null) {
-                execution = ExecutionMode.GLOBAL_ORDINALS;
-            }
-            if ((execution.needsGlobalOrdinals()) && (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals))) {
-                execution = ExecutionMode.MAP;
-            }
-            return execution.create(name, factories, shardSize, maxDocsPerValue, valuesSource, context, parent, pipelineAggregators,
-                    metaData);
-        }
-
-        throw new AggregationExecutionException("Sampler aggregation cannot be applied to field [" + config.fieldContext().field()
-                + "]. It can only be applied to numeric or string fields.");
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        final UnmappedSampler aggregation = new UnmappedSampler(name, pipelineAggregators, metaData);
-
-        return new NonCollectingAggregator(name, context, parent, factories, pipelineAggregators, metaData) {
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return aggregation;
-            }
-        };
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
deleted file mode 100644
index fa48ab2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- *
- */
-public class DiversifiedSamplerParser extends AnyValuesSourceParser {
-
-    public DiversifiedSamplerParser() {
-        super(true, false);
-    }
-
-    @Override
-    public String type() {
-        return SamplerAggregator.DiversifiedAggregatorBuilder.TYPE.name();
-    }
-
-    @Override
-    protected SamplerAggregator.DiversifiedAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        SamplerAggregator.DiversifiedAggregatorBuilder factory = new SamplerAggregator.DiversifiedAggregatorBuilder(aggregationName);
-        Integer shardSize = (Integer) otherOptions.get(SamplerAggregator.SHARD_SIZE_FIELD);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
-        }
-        Integer maxDocsPerValue = (Integer) otherOptions.get(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD);
-        if (maxDocsPerValue != null) {
-            factory.maxDocsPerValue(maxDocsPerValue);
-        }
-        String executionHint = (String) otherOptions.get(SamplerAggregator.EXECUTION_HINT_FIELD);
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
-                int shardSize = parser.intValue();
-                otherOptions.put(SamplerAggregator.SHARD_SIZE_FIELD, shardSize);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD)) {
-                int maxDocsPerValue = parser.intValue();
-                otherOptions.put(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD, maxDocsPerValue);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.EXECUTION_HINT_FIELD)) {
-                String executionHint = parser.text();
-                otherOptions.put(SamplerAggregator.EXECUTION_HINT_FIELD, executionHint);
-                return true;
-            }
-        }
-        return false;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SamplerAggregator.DiversifiedAggregatorBuilder(null);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
new file mode 100644
index 0000000..a623735
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.sampler;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Sampler} aggregation.
+ */
+public class SamplerAggregationBuilder extends ValuesSourceAggregationBuilder<SamplerAggregationBuilder> {
+
+    private int shardSize = SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE;
+
+    int maxDocsPerValue = SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT;
+    String executionHint = null;
+
+    /**
+     * Sole constructor.
+     */
+    public SamplerAggregationBuilder(String name) {
+        super(name, InternalSampler.TYPE.name());
+    }
+
+    /**
+     * Set the max num docs to be returned from each shard.
+     */
+    public SamplerAggregationBuilder shardSize(int shardSize) {
+        this.shardSize = shardSize;
+        return this;
+    }
+
+    public SamplerAggregationBuilder maxDocsPerValue(int maxDocsPerValue) {
+        this.maxDocsPerValue = maxDocsPerValue;
+        return this;
+    }
+
+    public SamplerAggregationBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        // builder.startObject();
+        if (shardSize != SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE) {
+            builder.field(SamplerParser.SHARD_SIZE_FIELD.getPreferredName(), shardSize);
+        }
+
+        if (maxDocsPerValue != SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT) {
+            builder.field(SamplerParser.MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
+        }
+        if (executionHint != null) {
+            builder.field(SamplerParser.EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
+        }
+
+        return builder;
+    }
+
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
index f9c156e..8cb9809 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
@@ -21,34 +21,27 @@ package org.elasticsearch.search.aggregations.bucket.sampler;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BestDocsDeferringCollector;
 import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate on only the top-scoring docs on a shard.
@@ -62,10 +55,6 @@ import java.util.Objects;
  */
 public class SamplerAggregator extends SingleBucketAggregator {
 
-    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
-    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
-    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
-
 
     public enum ExecutionMode {
 
@@ -191,175 +180,82 @@ public class SamplerAggregator extends SingleBucketAggregator {
         return new InternalSampler(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class SamplerAggregatorBuilder extends AggregatorBuilder<SamplerAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
-        public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+        private int shardSize;
 
-        private int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
-
-        public SamplerAggregatorBuilder(String name) {
-            super(name, InternalSampler.TYPE);
-        }
-
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public SamplerAggregatorBuilder shardSize(int shardSize) {
+        public Factory(String name, int shardSize) {
+            super(name, InternalSampler.TYPE.name());
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
-        }
-
-        @Override
-        protected SamplerAggregatorFactory doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new SamplerAggregatorFactory(name, type, shardSize, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected SamplerAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            SamplerAggregatorBuilder factory = new SamplerAggregatorBuilder(name);
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
         }
 
         @Override
-        protected int doHashCode() {
-            return Objects.hash(shardSize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            SamplerAggregatorBuilder other = (SamplerAggregatorBuilder) obj;
-            return Objects.equals(shardSize, other.shardSize);
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new SamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData);
         }
 
     }
 
-    public static class DiversifiedAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, DiversifiedAggregatorBuilder> {
-
-        public static final Type TYPE = new Type("diversified_sampler");
-
-        public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
-
-        private int shardSize = SamplerAggregatorBuilder.DEFAULT_SHARD_SAMPLE_SIZE;
-        private int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
-        private String executionHint = null;
+    public static class DiversifiedFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
-        public DiversifiedAggregatorBuilder(String name) {
-            super(name, TYPE, ValuesSourceType.ANY, null);
-        }
+        private int shardSize;
+        private int maxDocsPerValue;
+        private String executionHint;
 
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public DiversifiedAggregatorBuilder shardSize(int shardSize) {
+        public DiversifiedFactory(String name, int shardSize, String executionHint, ValuesSourceConfig vsConfig, int maxDocsPerValue) {
+            super(name, InternalSampler.TYPE.name(), vsConfig);
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
-        }
-
-        /**
-         * Set the max num docs to be returned per value.
-         */
-        public DiversifiedAggregatorBuilder maxDocsPerValue(int maxDocsPerValue) {
             this.maxDocsPerValue = maxDocsPerValue;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned per value.
-         */
-        public int maxDocsPerValue() {
-            return maxDocsPerValue;
-        }
-
-        /**
-         * Set the execution hint.
-         */
-        public DiversifiedAggregatorBuilder executionHint(String executionHint) {
             this.executionHint = executionHint;
-            return this;
-        }
-
-        /**
-         * Get the execution hint.
-         */
-        public String executionHint() {
-            return executionHint;
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new DiversifiedAggregatorFactory(name, TYPE, config, shardSize, maxDocsPerValue, executionHint, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
+        protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext context, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.field(MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
-            if (executionHint != null) {
-                builder.field(EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
+            if (valuesSource instanceof ValuesSource.Numeric) {
+                return new DiversifiedNumericSamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData,
+                        (Numeric) valuesSource, maxDocsPerValue);
             }
-            return builder;
-        }
 
-        @Override
-        protected DiversifiedAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            DiversifiedAggregatorBuilder factory = new DiversifiedAggregatorBuilder(name);
-            factory.shardSize = in.readVInt();
-            factory.maxDocsPerValue = in.readVInt();
-            factory.executionHint = in.readOptionalString();
-            return factory;
-        }
+            if (valuesSource instanceof ValuesSource.Bytes) {
+                ExecutionMode execution = null;
+                if (executionHint != null) {
+                    execution = ExecutionMode.fromString(executionHint, context.searchContext().parseFieldMatcher());
+                }
 
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
-            out.writeVInt(maxDocsPerValue);
-            out.writeOptionalString(executionHint);
-        }
+                // In some cases using ordinals is just not supported: override
+                // it
+                if(execution==null){
+                    execution = ExecutionMode.GLOBAL_ORDINALS;
+                }
+                if ((execution.needsGlobalOrdinals()) && (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals))) {
+                    execution = ExecutionMode.MAP;
+                }
+                return execution.create(name, factories, shardSize, maxDocsPerValue, valuesSource, context, parent, pipelineAggregators,
+                        metaData);
+            }
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(shardSize, maxDocsPerValue, executionHint);
+            throw new AggregationExecutionException("Sampler aggregation cannot be applied to field [" + config.fieldContext().field() +
+                    "]. It can only be applied to numeric or string fields.");
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            DiversifiedAggregatorBuilder other = (DiversifiedAggregatorBuilder) obj;
-            return Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(maxDocsPerValue, other.maxDocsPerValue)
-                    && Objects.equals(executionHint, other.executionHint);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            final UnmappedSampler aggregation = new UnmappedSampler(name, pipelineAggregators, metaData);
+
+            return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
+                @Override
+                public InternalAggregation buildEmptyAggregation() {
+                    return aggregation;
+                }
+            };
         }
+
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregatorFactory.java
deleted file mode 100644
index 09f510f..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregatorFactory.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class SamplerAggregatorFactory extends AggregatorFactory<SamplerAggregatorFactory> {
-
-    private final int shardSize;
-
-    public SamplerAggregatorFactory(String name, Type type, int shardSize, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.shardSize = shardSize;
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        return new SamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
index 0f96eb3..498a7cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
@@ -18,12 +18,14 @@
  */
 package org.elasticsearch.search.aggregations.bucket.sampler;
 
-
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -32,44 +34,73 @@ import java.io.IOException;
  */
 public class SamplerParser implements Aggregator.Parser {
 
+    public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
+    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
+    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
+    public static final boolean DEFAULT_USE_GLOBAL_ORDINALS = false;
+    public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
+
+
     @Override
     public String type() {
         return InternalSampler.TYPE.name();
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
         XContentParser.Token token;
         String currentFieldName = null;
-        Integer shardSize = null;
+        String executionHint = null;
+        int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
+        int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
+        ValuesSourceParser vsParser = null;
+        boolean diversityChoiceMade = false;
+
+        vsParser = ValuesSourceParser.any(aggregationName, InternalSampler.TYPE, context).scriptable(true).formattable(false).build();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (context.parseFieldMatcher().match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
+                if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD)) {
                     shardSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, MAX_DOCS_PER_VALUE_FIELD)) {
+                    diversityChoiceMade = true;
+                    maxDocsPerValue = parser.intValue();
+                } else {
+                    throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                            + aggregationName, parser.getTokenLocation());
+                }
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD)) {
+                    executionHint = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                        + aggregationName, parser.getTokenLocation());
             }
         }
 
-        SamplerAggregator.SamplerAggregatorBuilder factory = new SamplerAggregator.SamplerAggregatorBuilder(aggregationName);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
+        ValuesSourceConfig vsConfig = vsParser.config();
+        if (vsConfig.valid()) {
+            return new SamplerAggregator.DiversifiedFactory(aggregationName, shardSize, executionHint, vsConfig, maxDocsPerValue);
+        } else {
+            if (diversityChoiceMade) {
+                throw new SearchParseException(context, "Sampler aggregation has " + MAX_DOCS_PER_VALUE_FIELD.getPreferredName()
+                        + " setting but no \"field\" or \"script\" setting to provide values for aggregation \"" + aggregationName + "\"",
+                        parser.getTokenLocation());
+
+            }
+            return new SamplerAggregator.Factory(aggregationName, shardSize);
         }
-        return factory;
     }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SamplerAggregator.SamplerAggregatorBuilder(null);
-    }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java
index 7409a61..e489799 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java
@@ -27,7 +27,6 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -48,17 +47,15 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
 
     protected long numCollectedDocs;
     protected final SignificantTermsAggregatorFactory termsAggFactory;
-    private final SignificanceHeuristic significanceHeuristic;
 
     public GlobalOrdinalsSignificantTermsAggregator(String name, AggregatorFactories factories,
             ValuesSource.Bytes.WithOrdinals.FieldData valuesSource, BucketCountThresholds bucketCountThresholds,
             IncludeExclude.OrdinalsFilter includeExclude, AggregationContext aggregationContext, Aggregator parent,
-            SignificanceHeuristic significanceHeuristic, SignificantTermsAggregatorFactory termsAggFactory,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            SignificantTermsAggregatorFactory termsAggFactory, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+            throws IOException {
 
         super(name, factories, valuesSource, null, bucketCountThresholds, includeExclude, aggregationContext, parent,
                 SubAggCollectionMode.DEPTH_FIRST, false, pipelineAggregators, metaData);
-        this.significanceHeuristic = significanceHeuristic;
         this.termsAggFactory = termsAggFactory;
     }
 
@@ -120,7 +117,7 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
             // that are for this shard only
             // Back at the central reducer these properties will be updated with
             // global stats
-            spare.updateScore(significanceHeuristic);
+            spare.updateScore(termsAggFactory.getSignificanceHeuristic());
             spare = (SignificantStringTerms.Bucket) ordered.insertWithOverflow(spare);
         }
 
@@ -134,7 +131,7 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
         }
 
         return new SignificantStringTerms(subsetSize, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic, Arrays.asList(list), pipelineAggregators(),
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(), Arrays.asList(list), pipelineAggregators(),
                 metaData());
     }
 
@@ -145,7 +142,7 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
         IndexReader topReader = searcher.getIndexReader();
         int supersetSize = topReader.numDocs();
         return new SignificantStringTerms(0, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic,
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(),
                 Collections.<InternalSignificantTerms.Bucket> emptyList(), pipelineAggregators(), metaData());
     }
 
@@ -160,11 +157,10 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
 
         public WithHash(String name, AggregatorFactories factories, ValuesSource.Bytes.WithOrdinals.FieldData valuesSource,
                 BucketCountThresholds bucketCountThresholds, IncludeExclude.OrdinalsFilter includeExclude,
-                AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                SignificantTermsAggregatorFactory termsAggFactory, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException {
-            super(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent, significanceHeuristic,
-                    termsAggFactory, pipelineAggregators, metaData);
+                AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggFactory,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            super(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent, termsAggFactory,
+                    pipelineAggregators, metaData);
             bucketOrds = new LongHash(1, aggregationContext.bigArrays());
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
index 226c6d5..9e8ad33 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
@@ -228,7 +228,7 @@ public class SignificantLongTerms extends InternalSignificantTerms<SignificantLo
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java
index abf9bf6..5e102ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.LongTermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -47,19 +46,16 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
 
     public SignificantLongTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource,
             ValueFormat format, BucketCountThresholds bucketCountThresholds, AggregationContext aggregationContext, Aggregator parent,
-            SignificanceHeuristic significanceHeuristic, SignificantTermsAggregatorFactory termsAggFactory,
-            IncludeExclude.LongFilter includeExclude,
+            SignificantTermsAggregatorFactory termsAggFactory, IncludeExclude.LongFilter includeExclude,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
 
         super(name, factories, valuesSource, format, null, bucketCountThresholds, aggregationContext, parent,
                 SubAggCollectionMode.DEPTH_FIRST, false, includeExclude, pipelineAggregators, metaData);
-        this.significanceHeuristic = significanceHeuristic;
         this.termsAggFactory = termsAggFactory;
     }
 
     protected long numCollectedDocs;
     private final SignificantTermsAggregatorFactory termsAggFactory;
-    private final SignificanceHeuristic significanceHeuristic;
 
     @Override
     public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,
@@ -99,7 +95,7 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
             spare.supersetSize = supersetSize;
             // During shard-local down-selection we use subset/superset stats that are for this shard only
             // Back at the central reducer these properties will be updated with global stats
-            spare.updateScore(significanceHeuristic);
+            spare.updateScore(termsAggFactory.getSignificanceHeuristic());
 
             spare.bucketOrd = i;
             spare = (SignificantLongTerms.Bucket) ordered.insertWithOverflow(spare);
@@ -112,7 +108,7 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
             list[i] = bucket;
         }
         return new SignificantLongTerms(subsetSize, supersetSize, name, formatter, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic, Arrays.asList(list), pipelineAggregators(),
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(), Arrays.asList(list), pipelineAggregators(),
                 metaData());
     }
 
@@ -123,7 +119,7 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
         IndexReader topReader = searcher.getIndexReader();
         int supersetSize = topReader.numDocs();
         return new SignificantLongTerms(0, supersetSize, name, formatter, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic,
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(),
                 Collections.<InternalSignificantTerms.Bucket> emptyList(), pipelineAggregators(), metaData());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
index b047947..6c1ca0a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
@@ -214,7 +214,7 @@ public class SignificantStringTerms extends InternalSignificantTerms<Significant
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
             bucket.writeTo(out);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java
index d818fb0..1218bba 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -47,16 +46,16 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
 
     protected long numCollectedDocs;
     protected final SignificantTermsAggregatorFactory termsAggFactory;
-    private final SignificanceHeuristic significanceHeuristic;
 
     public SignificantStringTermsAggregator(String name, AggregatorFactories factories, ValuesSource valuesSource,
-            BucketCountThresholds bucketCountThresholds, IncludeExclude.StringFilter includeExclude, AggregationContext aggregationContext,
-            Aggregator parent, SignificanceHeuristic significanceHeuristic, SignificantTermsAggregatorFactory termsAggFactory,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            BucketCountThresholds bucketCountThresholds,
+            IncludeExclude.StringFilter includeExclude, AggregationContext aggregationContext, Aggregator parent,
+ SignificantTermsAggregatorFactory termsAggFactory, List<PipelineAggregator> pipelineAggregators,
+            Map<String, Object> metaData)
+            throws IOException {
 
         super(name, factories, valuesSource, null, bucketCountThresholds, includeExclude, aggregationContext, parent,
                 SubAggCollectionMode.DEPTH_FIRST, false, pipelineAggregators, metaData);
-        this.significanceHeuristic = significanceHeuristic;
         this.termsAggFactory = termsAggFactory;
     }
 
@@ -101,7 +100,7 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
             // that are for this shard only
             // Back at the central reducer these properties will be updated with
             // global stats
-            spare.updateScore(significanceHeuristic);
+            spare.updateScore(termsAggFactory.getSignificanceHeuristic());
 
             spare.bucketOrd = i;
             spare = (SignificantStringTerms.Bucket) ordered.insertWithOverflow(spare);
@@ -117,7 +116,7 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
         }
 
         return new SignificantStringTerms(subsetSize, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic, Arrays.asList(list), pipelineAggregators(),
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(), Arrays.asList(list), pipelineAggregators(),
                 metaData());
     }
 
@@ -128,7 +127,7 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
         IndexReader topReader = searcher.getIndexReader();
         int supersetSize = topReader.numDocs();
         return new SignificantStringTerms(0, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic,
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(),
                 Collections.<InternalSignificantTerms.Bucket> emptyList(), pipelineAggregators(), metaData());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorBuilder.java
deleted file mode 100644
index ce2edfd..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorBuilder.java
+++ /dev/null
@@ -1,241 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.significant;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.index.FilterableTermsEnum;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- *
- */
-public class SignificantTermsAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, SignificantTermsAggregatorBuilder> {
-
-    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
-    static final ParseField HEURISTIC = new ParseField("significance_heuristic");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(
-            3, 0, 10, -1);
-
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
-    private String indexedFieldName;
-    private MappedFieldType fieldType;
-    private FilterableTermsEnum termsEnum;
-    private int numberOfAggregatorsCreated = 0;
-    private QueryBuilder<?> filterBuilder = null;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private SignificanceHeuristic significanceHeuristic = JLHScore.PROTOTYPE;
-
-    protected TermsAggregator.BucketCountThresholds getBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
-    }
-
-    public SignificantTermsAggregatorBuilder(String name, ValueType valueType) {
-        super(name, SignificantStringTerms.TYPE, ValuesSourceType.ANY, valueType);
-    }
-
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public SignificantTermsAggregatorBuilder bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
-        this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the size - indicating how many term buckets should be returned
-     * (defaults to 10)
-     */
-    public SignificantTermsAggregatorBuilder size(int size) {
-        bucketCountThresholds.setRequiredSize(size);
-        return this;
-    }
-
-    /**
-     * Sets the shard_size - indicating the number of term buckets each shard
-     * will return to the coordinating node (the node that coordinates the
-     * search execution). The higher the shard size is, the more accurate the
-     * results are.
-     */
-    public SignificantTermsAggregatorBuilder shardSize(int shardSize) {
-        bucketCountThresholds.setShardSize(shardSize);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have in order to appear in
-     * the response.
-     */
-    public SignificantTermsAggregatorBuilder minDocCount(long minDocCount) {
-        bucketCountThresholds.setMinDocCount(minDocCount);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have on the shard in order to
-     * appear in the response.
-     */
-    public SignificantTermsAggregatorBuilder shardMinDocCount(long shardMinDocCount) {
-        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        return this;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public SignificantTermsAggregatorBuilder executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    public SignificantTermsAggregatorBuilder backgroundFilter(QueryBuilder<?> filterBuilder) {
-        this.filterBuilder = filterBuilder;
-        return this;
-    }
-
-    public QueryBuilder<?> backgroundFilter() {
-        return filterBuilder;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public SignificantTermsAggregatorBuilder includeExclude(IncludeExclude includeExclude) {
-        this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    public SignificantTermsAggregatorBuilder significanceHeuristic(SignificanceHeuristic significanceHeuristic) {
-        this.significanceHeuristic = significanceHeuristic;
-        return this;
-    }
-
-    public SignificanceHeuristic significanceHeuristic() {
-        return significanceHeuristic;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new SignificantTermsAggregatorFactory(name, type, config, includeExclude, executionHint, filterBuilder,
-                bucketCountThresholds, significanceHeuristic, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorBuilder.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        if (filterBuilder != null) {
-            builder.field(BACKGROUND_FILTER.getPreferredName(), filterBuilder);
-        }
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        significanceHeuristic.toXContent(builder, params);
-        return builder;
-    }
-
-    @Override
-    protected SignificantTermsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        SignificantTermsAggregatorBuilder factory = new SignificantTermsAggregatorBuilder(name, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.filterBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.significanceHeuristic = SignificanceHeuristicStreams.read(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasfilterBuilder = filterBuilder != null;
-        out.writeBoolean(hasfilterBuilder);
-        if (hasfilterBuilder) {
-            out.writeQuery(filterBuilder);
-        }
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, executionHint, filterBuilder, includeExclude, significanceHeuristic);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        SignificantTermsAggregatorBuilder other = (SignificantTermsAggregatorBuilder) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(filterBuilder, other.filterBuilder)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(significanceHeuristic, other.significanceHeuristic);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
index 5ed1f95..399e857 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
@@ -16,7 +16,6 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations.bucket.significant;
 
 import org.apache.lucene.index.IndexReader;
@@ -31,18 +30,13 @@ import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lucene.index.FilterableTermsEnum;
 import org.elasticsearch.common.lucene.index.FreqTermsEnum;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -55,107 +49,122 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, SignificantTermsAggregatorFactory>
-        implements Releasable {
+/**
+ *
+ */
+public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> implements Releasable {
+
+    public SignificanceHeuristic getSignificanceHeuristic() {
+        return significanceHeuristic;
+    }
+
+    public enum ExecutionMode {
+
+        MAP(new ParseField("map")) {
+
+            @Override
+            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                    AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                    List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                final IncludeExclude.StringFilter filter = includeExclude == null ? null : includeExclude.convertToStringFilter();
+                return new SignificantStringTermsAggregator(name, factories, valuesSource, bucketCountThresholds, filter,
+                        aggregationContext, parent, termsAggregatorFactory, pipelineAggregators, metaData);
+            }
+
+        },
+        GLOBAL_ORDINALS(new ParseField("global_ordinals")) {
+
+            @Override
+            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                              TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                    AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                    List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
+                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
+                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
+                return new GlobalOrdinalsSignificantTermsAggregator(name, factories,
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
+            }
+
+        },
+        GLOBAL_ORDINALS_HASH(new ParseField("global_ordinals_hash")) {
+
+            @Override
+            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                              TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                    AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                    List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
+                return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter,
+ aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
+            }
+        };
+
+        public static ExecutionMode fromString(String value, ParseFieldMatcher parseFieldMatcher) {
+            for (ExecutionMode mode : values()) {
+                if (parseFieldMatcher.match(value, mode.parseField)) {
+                    return mode;
+                }
+            }
+            throw new IllegalArgumentException("Unknown `execution_hint`: [" + value + "], expected any of " + values());
+        }
+
+        private final ParseField parseField;
+
+        ExecutionMode(ParseField parseField) {
+            this.parseField = parseField;
+        }
 
+        abstract Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                                   TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException;
+
+        @Override
+        public String toString() {
+            return parseField.getPreferredName();
+        }
+    }
     private final IncludeExclude includeExclude;
     private final String executionHint;
     private String indexedFieldName;
     private MappedFieldType fieldType;
     private FilterableTermsEnum termsEnum;
-    private int numberOfAggregatorsCreated;
-    private final QueryBuilder<?> filterBuilder;
+    private int numberOfAggregatorsCreated = 0;
+    private final Query filter;
     private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
     private final SignificanceHeuristic significanceHeuristic;
 
-    public SignificantTermsAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, IncludeExclude includeExclude,
-            String executionHint, QueryBuilder<?> filterBuilder, TermsAggregator.BucketCountThresholds bucketCountThresholds,
-            SignificanceHeuristic significanceHeuristic, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
+    protected TermsAggregator.BucketCountThresholds getBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
+    }
+
+    public SignificantTermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                                             String executionHint, Query filter, SignificanceHeuristic significanceHeuristic) {
+
+        super(name, SignificantStringTerms.TYPE.name(), valueSourceConfig);
+        this.bucketCountThresholds = bucketCountThresholds;
         this.includeExclude = includeExclude;
         this.executionHint = executionHint;
-        this.filterBuilder = filterBuilder;
-        this.bucketCountThresholds = bucketCountThresholds;
         this.significanceHeuristic = significanceHeuristic;
-        this.significanceHeuristic.initialize(context.searchContext());
-        setFieldInfo();
-
-    }
-
-    private void setFieldInfo() {
-        if (!config.unmapped()) {
+        if (!valueSourceConfig.unmapped()) {
             this.indexedFieldName = config.fieldContext().field();
             fieldType = SearchContext.current().smartNameFieldType(indexedFieldName);
         }
-    }
-
-    /**
-     * Creates the TermsEnum (if not already created) and must be called before
-     * any calls to getBackgroundFrequency
-     *
-     * @param context
-     *            The aggregation context
-     * @return The number of documents in the index (after an optional filter
-     *         might have been applied)
-     */
-    public long prepareBackground(AggregationContext context) {
-        if (termsEnum != null) {
-            // already prepared - return
-            return termsEnum.getNumDocs();
-        }
-        SearchContext searchContext = context.searchContext();
-        IndexReader reader = searchContext.searcher().getIndexReader();
-        Query filter = null;
-        try {
-            if (filterBuilder != null) {
-                filter = filterBuilder.toFilter(context.searchContext().getQueryShardContext());
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to create filter: " + filterBuilder.toString(), e);
-        }
-        try {
-            if (numberOfAggregatorsCreated == 1) {
-                // Setup a termsEnum for sole use by one aggregator
-                termsEnum = new FilterableTermsEnum(reader, indexedFieldName, PostingsEnum.NONE, filter);
-            } else {
-                // When we have > 1 agg we have possibility of duplicate term
-                // frequency lookups
-                // and so use a TermsEnum that caches results of all term
-                // lookups
-                termsEnum = new FreqTermsEnum(reader, indexedFieldName, true, false, filter, searchContext.bigArrays());
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to build terms enumeration", e);
-        }
-        return termsEnum.getNumDocs();
-    }
-
-    public long getBackgroundFrequency(BytesRef termBytes) {
-        assert termsEnum != null; // having failed to find a field in the index
-                                  // we don't expect any calls for frequencies
-        long result = 0;
-        try {
-            if (termsEnum.seekExact(termBytes)) {
-                result = termsEnum.docFreq();
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("IOException loading background document frequency info", e);
-        }
-        return result;
-    }
-
-    public long getBackgroundFrequency(long term) {
-        BytesRef indexedVal = fieldType.indexedValueForSearch(term);
-        return getBackgroundFrequency(indexedVal);
+        this.filter = filter;
     }
 
     @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
+    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+            List<PipelineAggregator> pipelineAggregators,
+            Map<String, Object> metaData) throws IOException {
         final InternalAggregation aggregation = new UnmappedSignificantTerms(name, bucketCountThresholds.getRequiredSize(),
                 bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
-        return new NonCollectingAggregator(name, context, parent, pipelineAggregators, metaData) {
+        return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
             @Override
             public InternalAggregation buildEmptyAggregation() {
                 return aggregation;
@@ -164,35 +173,19 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
     }
 
     @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+            throws IOException {
         if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
+            return asMultiBucketAggregator(this, aggregationContext, parent);
         }
 
         numberOfAggregatorsCreated++;
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (bucketCountThresholds.getShardSize() == SignificantTermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            // The user has not made a shardSize selection .
-            // Use default heuristic to avoid any wrong-ranking caused by
-            // distributed counting
-            // but request double the usual amount.
-            // We typically need more than the number of "top" terms requested
-            // by other aggregations
-            // as the significance algorithm is in less of a position to
-            // down-select at shard-level -
-            // some of the things we want to find have only one occurrence on
-            // each shard and as
-            // such are impossible to differentiate from non-significant terms
-            // at that early stage.
-            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    context.searchContext().numberOfShards()));
-        }
 
         if (valuesSource instanceof ValuesSource.Bytes) {
             ExecutionMode execution = null;
             if (executionHint != null) {
-                execution = ExecutionMode.fromString(executionHint, context.searchContext().parseFieldMatcher());
+                execution = ExecutionMode.fromString(executionHint, aggregationContext.searchContext().parseFieldMatcher());
             }
             if (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals)) {
                 execution = ExecutionMode.MAP;
@@ -205,13 +198,14 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                 }
             }
             assert execution != null;
-            return execution.create(name, factories, valuesSource, bucketCountThresholds, includeExclude, context, parent,
-                    significanceHeuristic, this, pipelineAggregators, metaData);
+            return execution.create(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent, this,
+                    pipelineAggregators, metaData);
         }
 
+
         if ((includeExclude != null) && (includeExclude.isRegexBased())) {
-            throw new AggregationExecutionException("Aggregation [" + name + "] cannot support regular expression style include/exclude "
-                    + "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
+            throw new AggregationExecutionException("Aggregation [" + name + "] cannot support regular expression style include/exclude " +
+                    "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
         }
 
         if (valuesSource instanceof ValuesSource.Numeric) {
@@ -224,87 +218,57 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                 longFilter = includeExclude.convertToLongFilter();
             }
             return new SignificantLongTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(),
-                    bucketCountThresholds, context, parent, significanceHeuristic, this, longFilter, pipelineAggregators,
-                    metaData);
+                    bucketCountThresholds, aggregationContext, parent, this, longFilter, pipelineAggregators, metaData);
         }
 
-        throw new AggregationExecutionException("sigfnificant_terms aggregation cannot be applied to field ["
-                + config.fieldContext().field() + "]. It can only be applied to numeric or string fields.");
+        throw new AggregationExecutionException("sigfnificant_terms aggregation cannot be applied to field [" + config.fieldContext().field() +
+                "]. It can only be applied to numeric or string fields.");
     }
 
-    public enum ExecutionMode {
-
-        MAP(new ParseField("map")) {
-
-            @Override
-            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                    AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                    SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                    Map<String, Object> metaData) throws IOException {
-                final IncludeExclude.StringFilter filter = includeExclude == null ? null : includeExclude.convertToStringFilter();
-                return new SignificantStringTermsAggregator(name, factories, valuesSource, bucketCountThresholds, filter,
-                        aggregationContext, parent, significanceHeuristic, termsAggregatorFactory, pipelineAggregators, metaData);
-            }
-
-        },
-        GLOBAL_ORDINALS(new ParseField("global_ordinals")) {
-
-            @Override
-            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                    AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                    SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                    Map<String, Object> metaData) throws IOException {
-                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
-                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
-                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
-                return new GlobalOrdinalsSignificantTermsAggregator(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        significanceHeuristic, termsAggregatorFactory, pipelineAggregators, metaData);
-            }
-
-        },
-        GLOBAL_ORDINALS_HASH(new ParseField("global_ordinals_hash")) {
-
-            @Override
-            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                    AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                    SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                    Map<String, Object> metaData) throws IOException {
-                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
-                return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        significanceHeuristic, termsAggregatorFactory, pipelineAggregators, metaData);
-            }
-        };
-
-        public static ExecutionMode fromString(String value, ParseFieldMatcher parseFieldMatcher) {
-            for (ExecutionMode mode : values()) {
-                if (parseFieldMatcher.match(value, mode.parseField)) {
-                    return mode;
-                }
+    /**
+     * Creates the TermsEnum (if not already created) and must be called before any calls to getBackgroundFrequency
+     * @param context The aggregation context
+     * @return The number of documents in the index (after an optional filter might have been applied)
+     */
+    public long prepareBackground(AggregationContext context) {
+        if (termsEnum != null) {
+            // already prepared - return
+            return termsEnum.getNumDocs();
+        }
+        SearchContext searchContext = context.searchContext();
+        IndexReader reader = searchContext.searcher().getIndexReader();
+        try {
+            if (numberOfAggregatorsCreated == 1) {
+                // Setup a termsEnum for sole use by one aggregator
+                termsEnum = new FilterableTermsEnum(reader, indexedFieldName, PostingsEnum.NONE, filter);
+            } else {
+                // When we have > 1 agg we have possibility of duplicate term frequency lookups
+                // and so use a TermsEnum that caches results of all term lookups
+                termsEnum = new FreqTermsEnum(reader, indexedFieldName, true, false, filter, searchContext.bigArrays());
             }
-            throw new IllegalArgumentException("Unknown `execution_hint`: [" + value + "], expected any of " + values());
+        } catch (IOException e) {
+            throw new ElasticsearchException("failed to build terms enumeration", e);
         }
+        return termsEnum.getNumDocs();
+    }
 
-        private final ParseField parseField;
-
-        ExecutionMode(ParseField parseField) {
-            this.parseField = parseField;
+    public long getBackgroundFrequency(BytesRef termBytes) {
+        assert termsEnum != null; // having failed to find a field in the index we don't expect any calls for frequencies
+        long result = 0;
+        try {
+            if (termsEnum.seekExact(termBytes)) {
+                result = termsEnum.docFreq();
+            }
+        } catch (IOException e) {
+            throw new ElasticsearchException("IOException loading background document frequency info", e);
         }
+        return result;
+    }
 
-        abstract Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException;
 
-        @Override
-        public String toString() {
-            return parseField.getPreferredName();
-        }
+    public long getBackgroundFrequency(long term) {
+        BytesRef indexedVal = fieldType.indexedValueForSearch(term);
+        return getBackgroundFrequency(indexedVal);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
new file mode 100644
index 0000000..b67ce2a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
@@ -0,0 +1,277 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+
+import java.io.IOException;
+
+/**
+ * Creates an aggregation that finds interesting or unusual occurrences of terms in a result set.
+ * <p>
+ * This feature is marked as experimental, and may be subject to change in the future.  If you
+ * use this feature, please let us know your experience with it!
+ */
+public class SignificantTermsBuilder extends AggregationBuilder<SignificantTermsBuilder> {
+
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(-1, -1, -1, -1);
+
+    private String field;
+    private String executionHint;
+    private String includePattern;
+    private int includeFlags;
+    private String excludePattern;
+    private int excludeFlags;
+    private String[] includeTerms = null;
+    private String[] excludeTerms = null;
+    private QueryBuilder filterBuilder;
+    private SignificanceHeuristicBuilder significanceHeuristicBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public SignificantTermsBuilder(String name) {
+        super(name, SignificantStringTerms.TYPE.name());
+    }
+
+    /**
+     * Set the field to fetch significant terms from.
+     */
+    public SignificantTermsBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the number of significant terms to retrieve.
+     */
+    public SignificantTermsBuilder size(int requiredSize) {
+        bucketCountThresholds.setRequiredSize(requiredSize);
+        return this;
+    }
+
+    /**
+     * Expert: Set the number of significant terms to retrieve on each shard.
+     */
+    public SignificantTermsBuilder shardSize(int shardSize) {
+        bucketCountThresholds.setShardSize(shardSize);
+        return this;
+    }
+
+    /**
+     * Only return significant terms that belong to at least <code>minDocCount</code> documents.
+     */
+    public SignificantTermsBuilder minDocCount(int minDocCount) {
+        bucketCountThresholds.setMinDocCount(minDocCount);
+        return this;
+    }
+    
+    /**
+     * Set the background filter to compare to. Defaults to the whole index.
+     */
+    public SignificantTermsBuilder backgroundFilter(QueryBuilder filter) {
+        this.filterBuilder = filter;
+        return this;
+    }
+    
+    /**
+     * Expert: set the minimum number of documents that a term should match to
+     * be retrieved from a shard.
+     */
+    public SignificantTermsBuilder shardMinDocCount(int shardMinDocCount) {
+        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
+        return this;
+    }
+
+    /**
+     * Expert: give an execution hint to this aggregation.
+     */
+    public SignificantTermsBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link java.util.regex.Pattern} class.
+     *
+     * @see #include(String, int)
+     */
+    public SignificantTermsBuilder include(String regex) {
+        return include(regex, 0);
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link java.util.regex.Pattern} class.
+     *
+     * @see java.util.regex.Pattern#compile(String, int)
+     */
+    public SignificantTermsBuilder include(String regex, int flags) {
+        if (includeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.includePattern = regex;
+        this.includeFlags = flags;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public SignificantTermsBuilder include(String [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = terms;
+        return this;
+    }    
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public SignificantTermsBuilder include(long [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = longsArrToStringArr(terms);
+        return this;
+    }     
+    
+    private String[] longsArrToStringArr(long[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Long.toString(terms[i]);
+        }
+        return termsAsString;
+    }      
+    
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link java.util.regex.Pattern} class.
+     *
+     * @see #exclude(String, int)
+     */
+    public SignificantTermsBuilder exclude(String regex) {
+        return exclude(regex, 0);
+    }
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link java.util.regex.Pattern} class.
+     *
+     * @see java.util.regex.Pattern#compile(String, int)
+     */
+    public SignificantTermsBuilder exclude(String regex, int flags) {
+        if (excludeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.excludePattern = regex;
+        this.excludeFlags = flags;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public SignificantTermsBuilder exclude(String [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.excludeTerms = terms;
+        return this;
+    }    
+    
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public SignificantTermsBuilder exclude(long [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of longs or a regex, not both");
+        }
+        this.excludeTerms = longsArrToStringArr(terms);
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        bucketCountThresholds.toXContent(builder);
+        if (executionHint != null) {
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+        }
+        if (includePattern != null) {
+            if (includeFlags == 0) {
+                builder.field("include", includePattern);
+            } else {
+                builder.startObject("include")
+                        .field("pattern", includePattern)
+                        .field("flags", includeFlags)
+                        .endObject();
+            }
+        }
+        if (includeTerms != null) {
+            builder.array("include", includeTerms);
+        }
+        
+        if (excludePattern != null) {
+            if (excludeFlags == 0) {
+                builder.field("exclude", excludePattern);
+            } else {
+                builder.startObject("exclude")
+                        .field("pattern", excludePattern)
+                        .field("flags", excludeFlags)
+                        .endObject();
+            }
+        }
+        if (excludeTerms != null) {
+            builder.array("exclude", excludeTerms);
+        }
+        
+        if (filterBuilder != null) {
+            builder.field(SignificantTermsParametersParser.BACKGROUND_FILTER.getPreferredName());
+            filterBuilder.toXContent(builder, params); 
+        }
+        if (significanceHeuristicBuilder != null) {
+            significanceHeuristicBuilder.toXContent(builder, params);
+        }
+
+        return builder.endObject();
+    }
+
+    /**
+     * Expert: set the {@link SignificanceHeuristic} to use.
+     */
+    public SignificantTermsBuilder significanceHeuristic(SignificanceHeuristicBuilder significanceHeuristicBuilder) {
+        this.significanceHeuristicBuilder = significanceHeuristicBuilder;
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
new file mode 100644
index 0000000..3019765
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.apache.lucene.search.Query;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+
+public class SignificantTermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(3, 0, 10, -1);
+    private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
+
+    public SignificantTermsParametersParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
+        this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
+    }
+
+    public Query getFilter() {
+        return filter;
+    }
+
+    private Query filter = null;
+
+    private SignificanceHeuristic significanceHeuristic;
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+
+    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+
+        if (token == XContentParser.Token.START_OBJECT) {
+            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
+            if (significanceHeuristicParser != null) {
+                significanceHeuristic = significanceHeuristicParser.parse(parser, context.parseFieldMatcher(), context);
+            } else if (context.parseFieldMatcher().match(currentFieldName, BACKGROUND_FILTER)) {
+                filter = context.getQueryShardContext().parseInnerFilter(parser).query();
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    public SignificanceHeuristic getSignificanceHeuristic() {
+        return significanceHeuristic;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
index 12b181a..28e0fb5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
@@ -18,41 +18,31 @@
  */
 package org.elasticsearch.search.aggregations.bucket.significant;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
-import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParser;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class SignificantTermsParser extends AbstractTermsParser {
+public class SignificantTermsParser implements Aggregator.Parser {
+
     private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
-    private final IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper,
-            IndicesQueriesRegistry queriesRegistry) {
+    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
         this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
@@ -61,58 +51,32 @@ public class SignificantTermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected SignificantTermsAggregatorBuilder doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        SignificantTermsAggregatorBuilder factory = new SignificantTermsAggregatorBuilder(aggregationName, targetValueType);
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        QueryBuilder<?> backgroundFilter = (QueryBuilder<?>) otherOptions.get(SignificantTermsAggregatorBuilder.BACKGROUND_FILTER);
-        if (backgroundFilter != null) {
-            factory.backgroundFilter(backgroundFilter);
-        }
-        SignificanceHeuristic significanceHeuristic = (SignificanceHeuristic) otherOptions.get(SignificantTermsAggregatorBuilder.HEURISTIC);
-        if (significanceHeuristic != null) {
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SignificantTermsParametersParser aggParser = new SignificantTermsParametersParser(significanceHeuristicParserMapper);
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, SignificantStringTerms.TYPE, context)
+                .scriptable(false)
+                .formattable(true)
+                .build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
 
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
-            if (significanceHeuristicParser != null) {
-                SignificanceHeuristic significanceHeuristic = significanceHeuristicParser.parse(parser, parseFieldMatcher);
-                otherOptions.put(SignificantTermsAggregatorBuilder.HEURISTIC, significanceHeuristic);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SignificantTermsAggregatorBuilder.BACKGROUND_FILTER)) {
-                QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                queryParseContext.reset(parser);
-                queryParseContext.parseFieldMatcher(parseFieldMatcher);
-                QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                otherOptions.put(SignificantTermsAggregatorBuilder.BACKGROUND_FILTER, filter);
-                return true;
-            }
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            //The user has not made a shardSize selection .
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            //but request double the usual amount.
+            //We typically need more than the number of "top" terms requested by other aggregations
+            //as the significance algorithm is in less of a position to down-select at shard-level -
+            //some of the things we want to find have only one occurrence on each shard and as
+            // such are impossible to differentiate from non-significant terms at that early stage.
+            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(), context.numberOfShards()));
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SignificantTermsAggregatorBuilder(null, null);
-    }
-
-    @Override
-    protected BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(SignificantTermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        SignificanceHeuristic significanceHeuristic = aggParser.getSignificanceHeuristic();
+        if (significanceHeuristic == null) {
+            significanceHeuristic = JLHScore.INSTANCE;
+        }
+        return new SignificantTermsAggregatorFactory(aggregationName, vsParser.config(), bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getFilter(), significanceHeuristic);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
index bcad058..c7569db 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
@@ -58,9 +58,9 @@ public class UnmappedSignificantTerms extends InternalSignificantTerms<UnmappedS
     UnmappedSignificantTerms() {} // for serialization
 
     public UnmappedSignificantTerms(String name, int requiredSize, long minDocCount, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) {
-        //We pass zero for index/subset sizes because for the purpose of significant term analysis
-        // we assume an unmapped index's size is irrelevant to the proceedings.
-        super(0, 0, name, requiredSize, minDocCount, JLHScore.PROTOTYPE, BUCKETS, pipelineAggregators, metaData);
+        //We pass zero for index/subset sizes because for the purpose of significant term analysis 
+        // we assume an unmapped index's size is irrelevant to the proceedings. 
+        super(0, 0, name, requiredSize, minDocCount, JLHScore.INSTANCE, BUCKETS, pipelineAggregators, metaData);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
index c68e47a..cc9303a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class ChiSquare extends NXYSignificanceHeuristic {
 
-    static final ChiSquare PROTOTYPE = new ChiSquare(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("chi_square");
 
     public ChiSquare(boolean includeNegatives, boolean backgroundIsSuperset) {
@@ -52,6 +51,18 @@ public class ChiSquare extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new ChiSquare(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Chi^2
      * see "Information Retrieval", Manning et al., Eq. 13.19
@@ -69,21 +80,9 @@ public class ChiSquare extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new ChiSquare(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class ChiSquareParser extends NXYParser {
@@ -107,7 +106,7 @@ public class ChiSquare extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index 6da05ed..99ee7c7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -29,13 +29,12 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class GND extends NXYSignificanceHeuristic {
 
-    static final GND PROTOTYPE = new GND(false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("gnd");
 
     public GND(boolean backgroundIsSuperset) {
@@ -58,6 +57,18 @@ public class GND extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new GND(in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Google Normalized Distance, as described in "The Google Similarity Distance", Cilibrasi and Vitanyi, 2007
      * link: http://arxiv.org/pdf/cs/0412098v3.pdf
@@ -87,28 +98,11 @@ public class GND extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new GND(in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         out.writeBoolean(backgroundIsSuperset);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
-        builder.endObject();
-        return builder;
-    }
-
     public static class GNDParser extends NXYParser {
 
         @Override
@@ -122,7 +116,7 @@ public class GND extends NXYSignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
@@ -149,7 +143,7 @@ public class GND extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index c4327cd..97264e7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -22,43 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class JLHScore extends SignificanceHeuristic {
 
-    public static final JLHScore PROTOTYPE = new JLHScore();
+    public static final JLHScore INSTANCE = new JLHScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("jlh");
+    protected static final String[] NAMES = {"jlh"};
 
-    public JLHScore() {
-    }
+    private JLHScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -106,21 +101,26 @@ public class JLHScore extends SignificanceHeuristic {
         return absoluteProbabilityChange * relativeProbabilityChange;
     }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class JLHScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
-                throw new ElasticsearchParseException("failed to parse [jlh] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
+                throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new JLHScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -128,7 +128,7 @@ public class JLHScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
index d20b5f3..b4529b8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class MutualInformation extends NXYSignificanceHeuristic {
 
-    static final MutualInformation PROTOTYPE = new MutualInformation(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("mutual_information");
 
     private static final double log2 = Math.log(2.0);
@@ -54,6 +53,18 @@ public class MutualInformation extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new MutualInformation(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates mutual information
      * see "Information Retrieval", Manning et al., Eq. 13.17
@@ -102,21 +113,9 @@ public class MutualInformation extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new MutualInformation(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class MutualInformationParser extends NXYParser {
@@ -140,7 +139,7 @@ public class MutualInformation extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index d3f98f2..c6a6924 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -135,15 +136,10 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
         }
     }
 
-    protected void build(XContentBuilder builder) throws IOException {
-        builder.field(INCLUDE_NEGATIVES_FIELD.getPreferredName(), includeNegatives).field(BACKGROUND_IS_SUPERSET.getPreferredName(),
-                backgroundIsSuperset);
-    }
-
     public static abstract class NXYParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index 648c126..aceae8c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -22,43 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class PercentageScore extends SignificanceHeuristic {
 
-    public static final PercentageScore PROTOTYPE = new PercentageScore();
+    public static final PercentageScore INSTANCE = new PercentageScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("percentage");
+    protected static final String[] NAMES = {"percentage"};
 
-    public PercentageScore() {
-    }
+    private PercentageScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -75,21 +70,26 @@ public class PercentageScore extends SignificanceHeuristic {
         return (double) subsetFreq / (double) supersetFreq;
    }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class PercentageScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new PercentageScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -97,7 +97,7 @@ public class PercentageScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index 64a437d..a160451 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -44,12 +44,9 @@ import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Objects;
 
 public class ScriptHeuristic extends SignificanceHeuristic {
 
-    static final ScriptHeuristic PROTOTYPE = new ScriptHeuristic(null);
-
     protected static final ParseField NAMES_FIELD = new ParseField("script_heuristic");
     private final LongAccessor subsetSizeHolder;
     private final LongAccessor supersetSizeHolder;
@@ -58,11 +55,31 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     ExecutableScript searchScript = null;
     Script script;
 
-    public ScriptHeuristic(Script script) {
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            Script script = Script.readScript(in);
+            return new ScriptHeuristic(null, script);
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
+    public ScriptHeuristic(ExecutableScript searchScript, Script script) {
         subsetSizeHolder = new LongAccessor();
         supersetSizeHolder = new LongAccessor();
         subsetDfHolder = new LongAccessor();
         supersetDfHolder = new LongAccessor();
+        this.searchScript = searchScript;
+        if (searchScript != null) {
+            searchScript.setNextVar("_subset_freq", subsetDfHolder);
+            searchScript.setNextVar("_subset_size", subsetSizeHolder);
+            searchScript.setNextVar("_superset_freq", supersetDfHolder);
+            searchScript.setNextVar("_superset_size", supersetSizeHolder);
+        }
         this.script = script;
 
 
@@ -70,16 +87,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
     @Override
     public void initialize(InternalAggregation.ReduceContext context) {
-        initialize(context.scriptService());
-    }
-
-    @Override
-    public void initialize(SearchContext context) {
-        initialize(context.scriptService());
-    }
-
-    public void initialize(ScriptService scriptService) {
-        searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+        searchScript = context.scriptService().executable(script, ScriptContext.Standard.AGGS, Collections.emptyMap());
         searchScript.setNextVar("_subset_freq", subsetDfHolder);
         searchScript.setNextVar("_subset_size", subsetSizeHolder);
         searchScript.setNextVar("_superset_freq", supersetDfHolder);
@@ -113,54 +121,20 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        Script script = Script.readScript(in);
-        return new ScriptHeuristic(script);
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         script.writeTo(out);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(ScriptField.SCRIPT.getPreferredName());
-        script.toXContent(builder, builderParams);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ScriptHeuristic other = (ScriptHeuristic) obj;
-        return Objects.equals(script, other.script);
-    }
-
     public static class ScriptHeuristicParser implements SignificanceHeuristicParser {
+        private final ScriptService scriptService;
 
-        public ScriptHeuristicParser() {
+        public ScriptHeuristicParser(ScriptService scriptService) {
+            this.scriptService = scriptService;
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String heuristicName = parser.currentName();
             Script script = null;
@@ -199,7 +173,13 @@ public class ScriptHeuristic extends SignificanceHeuristic {
             if (script == null) {
                 throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. no script found in script_heuristic", heuristicName);
             }
-            return new ScriptHeuristic(script);
+            ExecutableScript searchScript;
+            try {
+                searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+            } catch (Exception e) {
+                throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. the script [{}] could not be loaded", e, script, heuristicName);
+            }
+            return new ScriptHeuristic(searchScript, script);
         }
 
         @Override
@@ -219,7 +199,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(ScriptField.SCRIPT.getPreferredName());
             script.toXContent(builder, builderParams);
             builder.endObject();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
index 972696b..4f12277 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
@@ -20,12 +20,12 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.internal.SearchContext;
 
-public abstract class SignificanceHeuristic implements NamedWriteable<SignificanceHeuristic>, ToXContent {
+import java.io.IOException;
+
+public abstract class SignificanceHeuristic {
     /**
      * @param subsetFreq   The frequency of the term in the selected sample
      * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -35,6 +35,8 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
      */
     public abstract double getScore(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize);
 
+    abstract public void writeTo(StreamOutput out) throws IOException;
+
     protected void checkFrequencyValidity(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize, String scoreFunctionName) {
         if (subsetFreq < 0 || subsetSize < 0 || supersetFreq < 0 || supersetSize < 0) {
             throw new IllegalArgumentException("Frequencies of subset and superset must be positive in " + scoreFunctionName + ".getScore()");
@@ -50,8 +52,4 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
     public void initialize(InternalAggregation.ReduceContext reduceContext) {
 
     }
-
-    public void initialize(SearchContext context) {
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
index aa430dc..92baa43 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
@@ -23,12 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public interface SignificanceHeuristicParser {
 
-    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException,
+    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context) throws IOException,
             ParsingException;
 
     String[] getNames();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
index a9b385c..07e74ec 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
@@ -21,6 +21,8 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.script.ScriptService;
+
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
@@ -31,14 +33,14 @@ public class SignificanceHeuristicParserMapper {
     protected final Map<String, SignificanceHeuristicParser> significanceHeuristicParsers;
 
     @Inject
-    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers) {
+    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers, ScriptService scriptService) {
         Map<String, SignificanceHeuristicParser> map = new HashMap<>();
         add(map, new JLHScore.JLHScoreParser());
         add(map, new PercentageScore.PercentageScoreParser());
         add(map, new MutualInformation.MutualInformationParser());
         add(map, new ChiSquare.ChiSquareParser());
         add(map, new GND.GNDParser());
-        add(map, new ScriptHeuristic.ScriptHeuristicParser());
+        add(map, new ScriptHeuristic.ScriptHeuristicParser(scriptService));
         for (SignificanceHeuristicParser parser : parsers) {
             add(map, parser);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
index 2ffe5ec..198f129 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 import java.util.Collections;
@@ -32,26 +31,21 @@ import java.util.Map;
  */
 public class SignificanceHeuristicStreams {
 
-    private static Map<String, SignificanceHeuristic> STREAMS = Collections.emptyMap();
+    private static Map<String, Stream> STREAMS = Collections.emptyMap();
 
     static {
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
-        map.put(JLHScore.NAMES_FIELD.getPreferredName(), JLHScore.PROTOTYPE);
-        map.put(PercentageScore.NAMES_FIELD.getPreferredName(), PercentageScore.PROTOTYPE);
-        map.put(MutualInformation.NAMES_FIELD.getPreferredName(), MutualInformation.PROTOTYPE);
-        map.put(GND.NAMES_FIELD.getPreferredName(), GND.PROTOTYPE);
-        map.put(ChiSquare.NAMES_FIELD.getPreferredName(), ChiSquare.PROTOTYPE);
-        map.put(ScriptHeuristic.NAMES_FIELD.getPreferredName(), ScriptHeuristic.PROTOTYPE);
+        HashMap<String, Stream> map = new HashMap<>();
+        map.put(JLHScore.STREAM.getName(), JLHScore.STREAM);
+        map.put(PercentageScore.STREAM.getName(), PercentageScore.STREAM);
+        map.put(MutualInformation.STREAM.getName(), MutualInformation.STREAM);
+        map.put(GND.STREAM.getName(), GND.STREAM);
+        map.put(ChiSquare.STREAM.getName(), ChiSquare.STREAM);
+        map.put(ScriptHeuristic.STREAM.getName(), ScriptHeuristic.STREAM);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
     public static SignificanceHeuristic read(StreamInput in) throws IOException {
-        return stream(in.readString()).readFrom(in);
-    }
-
-    public static void writeTo(SignificanceHeuristic significanceHeuristic, StreamOutput out) throws IOException {
-        out.writeString(significanceHeuristic.getWriteableName());
-        significanceHeuristic.writeTo(out);
+        return stream(in.readString()).readResult(in);
     }
 
     /**
@@ -65,18 +59,17 @@ public class SignificanceHeuristicStreams {
     }
 
     /**
-     * Registers the given prototype.
+     * Registers the given stream and associate it with the given types.
      *
-     * @param prototype
-     *            The prototype to register
+     * @param stream The stream to register
      */
-    public static synchronized void registerPrototype(SignificanceHeuristic prototype) {
-        if (STREAMS.containsKey(prototype.getWriteableName())) {
-            throw new IllegalArgumentException("Can't register stream with name [" + prototype.getWriteableName() + "] more than once");
+    public static synchronized void registerStream(Stream stream) {
+        if (STREAMS.containsKey(stream.getName())) {
+            throw new IllegalArgumentException("Can't register stream with name [" + stream.getName() + "] more than once");
         }
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
+        HashMap<String, Stream> map = new HashMap<>();
         map.putAll(STREAMS);
-        map.put(prototype.getWriteableName(), prototype);
+        map.put(stream.getName(), stream);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
@@ -86,7 +79,7 @@ public class SignificanceHeuristicStreams {
      * @param name The given name
      * @return The associated stream
      */
-    private static synchronized SignificanceHeuristic stream(String name) {
+    private static synchronized Stream stream(String name) {
         return STREAMS.get(name);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
new file mode 100644
index 0000000..891526c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+public abstract class AbstractTermsParametersParser {
+
+    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
+    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
+    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
+    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
+    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
+    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
+    
+
+    //These are the results of the parsing.
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds();
+
+    private String executionHint = null;
+    
+    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
+
+
+    IncludeExclude includeExclude;
+
+    public TermsAggregator.BucketCountThresholds getBucketCountThresholds() {return bucketCountThresholds;}
+
+    //These are the results of the parsing.
+
+    public String getExecutionHint() {
+        return executionHint;
+    }
+
+    public IncludeExclude getIncludeExclude() {
+        return includeExclude;
+    }
+    
+    public SubAggCollectionMode getCollectionMode() {
+        return collectMode;
+    }
+
+    public void parse(String aggregationName, XContentParser parser, SearchContext context, ValuesSourceParser vsParser, IncludeExclude.Parser incExcParser) throws IOException {
+        bucketCountThresholds = getDefaultBucketCountThresholds();
+        XContentParser.Token token;
+        String currentFieldName = null;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (incExcParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
+                    executionHint = parser.text();
+                } else if(context.parseFieldMatcher().match(currentFieldName, SubAggCollectionMode.KEY)){
+                    collectMode = SubAggCollectionMode.parse(parser.text(), context.parseFieldMatcher());
+                } else if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setShardSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setMinDocCount(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setShardMinDocCount(parser.longValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else {
+                parseSpecial(aggregationName, parser, context, token, currentFieldName);
+            }
+        }
+        includeExclude = incExcParser.includeExclude();
+    }
+
+    public abstract void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException;
+
+    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
deleted file mode 100644
index a15c7d2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.terms;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-public abstract class AbstractTermsParser extends AnyValuesSourceParser {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    public IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
-
-    protected AbstractTermsParser() {
-        super(true, true);
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorBuilder<ValuesSource, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        BucketCountThresholds bucketCountThresholds = getDefaultBucketCountThresholds();
-        Integer requiredSize = (Integer) otherOptions.get(REQUIRED_SIZE_FIELD_NAME);
-        if (requiredSize != null && requiredSize != -1) {
-            bucketCountThresholds.setRequiredSize(requiredSize);
-        }
-        Integer shardSize = (Integer) otherOptions.get(SHARD_SIZE_FIELD_NAME);
-        if (shardSize != null && shardSize != -1) {
-            bucketCountThresholds.setShardSize(shardSize);
-        }
-        Long minDocCount = (Long) otherOptions.get(MIN_DOC_COUNT_FIELD_NAME);
-        if (minDocCount != null && minDocCount != -1) {
-            bucketCountThresholds.setMinDocCount(minDocCount);
-        }
-        Long shardMinDocCount = (Long) otherOptions.get(SHARD_MIN_DOC_COUNT_FIELD_NAME);
-        if (shardMinDocCount != null && shardMinDocCount != -1) {
-            bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        }
-        SubAggCollectionMode collectMode = (SubAggCollectionMode) otherOptions.get(SubAggCollectionMode.KEY);
-        String executionHint = (String) otherOptions.get(EXECUTION_HINT_FIELD_NAME);
-        IncludeExclude incExc = incExcParser.createIncludeExclude(otherOptions);
-        return doCreateFactory(aggregationName, valuesSourceType, targetValueType, bucketCountThresholds, collectMode, executionHint,
-                incExc,
-                otherOptions);
-    }
-
-    protected abstract ValuesSourceAggregatorBuilder<ValuesSource, ?> doCreateFactory(String aggregationName,
-            ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions);
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (incExcParser.token(currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
-                otherOptions.put(EXECUTION_HINT_FIELD_NAME, parser.text());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SubAggCollectionMode.KEY)) {
-                otherOptions.put(SubAggCollectionMode.KEY, SubAggCollectionMode.parse(parser.text(), parseFieldMatcher));
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
-                otherOptions.put(SHARD_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(SHARD_MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-            return true;
-        }
-        return false;
-    }
-
-    public abstract boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher,
-            XContentParser.Token token, String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException;
-
-    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
index 91e949e..1e7a004 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
@@ -347,7 +347,7 @@ public class GlobalOrdinalsStringTermsAggregator extends AbstractStringTermsAggr
                 Map<String, Object> metaData) throws IOException {
             super(name, factories, valuesSource, order, bucketCountThresholds, null, aggregationContext, parent, collectionMode,
                     showTermDocCountError, pipelineAggregators, metaData);
-            assert factories == null || factories.countAggregators() == 0;
+            assert factories == null || factories.count() == 0;
             this.segmentDocCounts = context.bigArrays().newIntArray(1, true);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
index f3f87c0..4e3e28a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
@@ -38,7 +38,6 @@ import java.util.Comparator;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
-import java.util.Objects;
 
 /**
  *
@@ -96,7 +95,7 @@ class InternalOrder extends Terms.Order {
     public static boolean isCountDesc(Terms.Order order) {
         if (order == COUNT_DESC) {
             return true;
-        } else if (order instanceof CompoundOrder) {
+        }else if (order instanceof CompoundOrder) {
             // check if its a compound order with count desc and the tie breaker (term asc)
             CompoundOrder compoundOrder = (CompoundOrder) order;
             if (compoundOrder.orderElements.size() == 2 && compoundOrder.orderElements.get(0) == COUNT_DESC && compoundOrder.orderElements.get(1) == TERM_ASC) {
@@ -106,23 +105,6 @@ class InternalOrder extends Terms.Order {
         return false;
     }
 
-    public static boolean isTermOrder(Terms.Order order) {
-        if (order == TERM_ASC) {
-            return true;
-        } else if (order == TERM_DESC) {
-            return true;
-        } else if (order instanceof CompoundOrder) {
-            // check if its a compound order with only a single element ordering
-            // by term
-            CompoundOrder compoundOrder = (CompoundOrder) order;
-            if (compoundOrder.orderElements.size() == 1 && compoundOrder.orderElements.get(0) == TERM_ASC
-                    || compoundOrder.orderElements.get(0) == TERM_DESC) {
-                return true;
-            }
-        }
-        return false;
-    }
-
     final byte id;
     final String key;
     final boolean asc;
@@ -281,23 +263,6 @@ class InternalOrder extends Terms.Order {
             return new CompoundOrderComparator(orderElements, aggregator);
         }
 
-        @Override
-        public int hashCode() {
-            return Objects.hash(orderElements);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            CompoundOrder other = (CompoundOrder) obj;
-            return Objects.equals(orderElements, other.orderElements);
-        }
-
         public static class CompoundOrderComparator implements Comparator<Terms.Bucket> {
 
             private List<Terms.Order> compoundOrder;
@@ -341,7 +306,7 @@ class InternalOrder extends Terms.Order {
         }
 
         public static Terms.Order readOrder(StreamInput in) throws IOException {
-            return readOrder(in, false);
+            return readOrder(in, true);
         }
 
         public static Terms.Order readOrder(StreamInput in, boolean absoluteOrder) throws IOException {
@@ -367,22 +332,4 @@ class InternalOrder extends Terms.Order {
             }
         }
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, asc);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(asc, other.asc);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
index 6e5aaac..31285d2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
@@ -188,7 +188,7 @@ public abstract class InternalTerms<A extends InternalTerms, B extends InternalT
             }
             otherDocCount += terms.getSumOfOtherDocCounts();
             final long thisAggDocCountError;
-            if (terms.buckets.size() < this.shardSize || InternalOrder.isTermOrder(order)) {
+            if (terms.buckets.size() < this.shardSize || this.order == InternalOrder.TERM_ASC || this.order == InternalOrder.TERM_DESC) {
                 thisAggDocCountError = 0;
             } else if (InternalOrder.isCountDesc(this.order)) {
                 thisAggDocCountError = terms.buckets.get(terms.buckets.size() - 1).docCount;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
index 16fb7f1..224e42c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
@@ -82,7 +82,7 @@ public interface Terms extends MultiBucketsAggregation {
      * Get the bucket for the given term, or null if there is no such bucket.
      */
     Bucket getBucketByKey(String term);
-
+    
     /**
      * Get an upper bound of the error on document counts in this aggregation.
      */
@@ -166,11 +166,5 @@ public interface Terms extends MultiBucketsAggregation {
 
         abstract byte id();
 
-        @Override
-        public abstract int hashCode();
-
-        @Override
-        public abstract boolean equals(Object obj);
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
index 699793e..7971d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
@@ -21,10 +21,7 @@
 package org.elasticsearch.search.aggregations.bucket.terms;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -39,136 +36,99 @@ import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 public abstract class TermsAggregator extends BucketsAggregator {
 
-    public static class BucketCountThresholds implements Writeable<BucketCountThresholds>, ToXContent {
-
-        private static final BucketCountThresholds PROTOTYPE = new BucketCountThresholds(-1, -1, -1, -1);
-
-        private long minDocCount;
-        private long shardMinDocCount;
-        private int requiredSize;
-        private int shardSize;
-
-        public static BucketCountThresholds readFromStream(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
+    public static class BucketCountThresholds {
+        private Explicit<Long> minDocCount;
+        private Explicit<Long> shardMinDocCount;
+        private Explicit<Integer> requiredSize;
+        private Explicit<Integer> shardSize;
 
         public BucketCountThresholds(long minDocCount, long shardMinDocCount, int requiredSize, int shardSize) {
-            this.minDocCount = minDocCount;
-            this.shardMinDocCount = shardMinDocCount;
-            this.requiredSize = requiredSize;
-            this.shardSize = shardSize;
+            this.minDocCount = new Explicit<>(minDocCount, false);
+            this.shardMinDocCount =  new Explicit<>(shardMinDocCount, false);
+            this.requiredSize = new Explicit<>(requiredSize, false);
+            this.shardSize = new Explicit<>(shardSize, false);
+        }
+        public BucketCountThresholds() {
+            this(-1, -1, -1, -1);
         }
 
         public BucketCountThresholds(BucketCountThresholds bucketCountThresholds) {
-            this(bucketCountThresholds.minDocCount, bucketCountThresholds.shardMinDocCount, bucketCountThresholds.requiredSize,
-                    bucketCountThresholds.shardSize);
+            this(bucketCountThresholds.minDocCount.value(), bucketCountThresholds.shardMinDocCount.value(), bucketCountThresholds.requiredSize.value(), bucketCountThresholds.shardSize.value());
         }
 
         public void ensureValidity() {
 
-            if (shardSize == 0) {
+            if (shardSize.value() == 0) {
                 setShardSize(Integer.MAX_VALUE);
             }
 
-            if (requiredSize == 0) {
+            if (requiredSize.value() == 0) {
                 setRequiredSize(Integer.MAX_VALUE);
             }
             // shard_size cannot be smaller than size as we need to at least fetch <size> entries from every shards in order to return <size>
-            if (shardSize < requiredSize) {
-                setShardSize(requiredSize);
+            if (shardSize.value() < requiredSize.value()) {
+                setShardSize(requiredSize.value());
             }
 
             // shard_min_doc_count should not be larger than min_doc_count because this can cause buckets to be removed that would match the min_doc_count criteria
-            if (shardMinDocCount > minDocCount) {
-                setShardMinDocCount(minDocCount);
+            if (shardMinDocCount.value() > minDocCount.value()) {
+                setShardMinDocCount(minDocCount.value());
             }
 
-            if (requiredSize < 0 || minDocCount < 0) {
+            if (requiredSize.value() < 0 || minDocCount.value() < 0) {
                 throw new ElasticsearchException("parameters [requiredSize] and [minDocCount] must be >=0 in terms aggregation.");
             }
         }
 
         public long getShardMinDocCount() {
-            return shardMinDocCount;
+            return shardMinDocCount.value();
         }
 
         public void setShardMinDocCount(long shardMinDocCount) {
-            this.shardMinDocCount = shardMinDocCount;
+            this.shardMinDocCount = new Explicit<>(shardMinDocCount, true);
         }
 
         public long getMinDocCount() {
-            return minDocCount;
+            return minDocCount.value();
         }
 
         public void setMinDocCount(long minDocCount) {
-            this.minDocCount = minDocCount;
+            this.minDocCount = new Explicit<>(minDocCount, true);
         }
 
         public int getRequiredSize() {
-            return requiredSize;
+            return requiredSize.value();
         }
 
         public void setRequiredSize(int requiredSize) {
-            this.requiredSize = requiredSize;
+            this.requiredSize = new Explicit<>(requiredSize, true);
         }
 
         public int getShardSize() {
-            return shardSize;
+            return shardSize.value();
         }
 
         public void setShardSize(int shardSize) {
-            this.shardSize = shardSize;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TermsAggregatorBuilder.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize);
-            builder.field(TermsAggregatorBuilder.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize);
-            builder.field(TermsAggregatorBuilder.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount);
-            builder.field(TermsAggregatorBuilder.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount);
-            return builder;
+            this.shardSize = new Explicit<>(shardSize, true);
         }
 
-        @Override
-        public BucketCountThresholds readFrom(StreamInput in) throws IOException {
-            int requiredSize = in.readInt();
-            int shardSize = in.readInt();
-            long minDocCount = in.readLong();
-            long shardMinDocCount = in.readLong();
-            return new BucketCountThresholds(minDocCount, shardMinDocCount, requiredSize, shardSize);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeInt(requiredSize);
-            out.writeInt(shardSize);
-            out.writeLong(minDocCount);
-            out.writeLong(shardMinDocCount);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(requiredSize, shardSize, minDocCount, shardMinDocCount);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
+        public void toXContent(XContentBuilder builder) throws IOException {
+            if (requiredSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize.value());
+            }
+            if (shardSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize.value());
+            }
+            if (minDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount.value());
             }
-            if (getClass() != obj.getClass()) {
-                return false;
+            if (shardMinDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount.value());
             }
-            BucketCountThresholds other = (BucketCountThresholds) obj;
-            return Objects.equals(requiredSize, other.requiredSize)
-                    && Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(shardMinDocCount, other.shardMinDocCount);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorBuilder.java
deleted file mode 100644
index 2bfdcfe..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorBuilder.java
+++ /dev/null
@@ -1,271 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.terms;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import java.io.IOException;
-import java.util.List;
-import java.util.Objects;
-
-/**
- *
- */
-/**
- *
- */
-public class TermsAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, TermsAggregatorBuilder> {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10,
-            -1);
-    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
-    public static final ParseField ORDER_FIELD = new ParseField("order");
-
-    private Terms.Order order = Terms.Order.compound(Terms.Order.count(false), Terms.Order.term(true));
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
-    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(
-            DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private boolean showTermDocCountError = false;
-
-    public TermsAggregatorBuilder(String name, ValueType valueType) {
-        super(name, StringTerms.TYPE, ValuesSourceType.ANY, valueType);
-    }
-
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public TermsAggregatorBuilder bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
-        this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the size - indicating how many term buckets should be returned
-     * (defaults to 10)
-     */
-    public TermsAggregatorBuilder size(int size) {
-        bucketCountThresholds.setRequiredSize(size);
-        return this;
-    }
-
-    /**
-     * Sets the shard_size - indicating the number of term buckets each shard
-     * will return to the coordinating node (the node that coordinates the
-     * search execution). The higher the shard size is, the more accurate the
-     * results are.
-     */
-    public TermsAggregatorBuilder shardSize(int shardSize) {
-        bucketCountThresholds.setShardSize(shardSize);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have in order to appear in
-     * the response.
-     */
-    public TermsAggregatorBuilder minDocCount(long minDocCount) {
-        bucketCountThresholds.setMinDocCount(minDocCount);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have on the shard in order to
-     * appear in the response.
-     */
-    public TermsAggregatorBuilder shardMinDocCount(long shardMinDocCount) {
-        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorBuilder order(Terms.Order order) {
-        this.order = order;
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorBuilder order(List<Terms.Order> orders) {
-        order(Terms.Order.compound(orders));
-        return this;
-    }
-
-    /**
-     * Gets the order in which the buckets will be returned.
-     */
-    public Terms.Order order() {
-        return order;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public TermsAggregatorBuilder executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    /**
-     * Expert: set the collection mode.
-     */
-    public TermsAggregatorBuilder collectMode(SubAggCollectionMode mode) {
-        this.collectMode = mode;
-        return this;
-    }
-
-    /**
-     * Expert: get the collection mode.
-     */
-    public SubAggCollectionMode collectMode() {
-        return collectMode;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public TermsAggregatorBuilder includeExclude(IncludeExclude includeExclude) {
-        this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    /**
-     * Get whether doc count error will be return for individual terms
-     */
-    public boolean showTermDocCountError() {
-        return showTermDocCountError;
-    }
-
-    /**
-     * Set whether doc count error will be return for individual terms
-     */
-    public TermsAggregatorBuilder showTermDocCountError(boolean showTermDocCountError) {
-        this.showTermDocCountError = showTermDocCountError;
-        return this;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new TermsAggregatorFactory(name, type, config, order, includeExclude, executionHint, collectMode,
- bucketCountThresholds,
-                showTermDocCountError, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        builder.field(SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorBuilder.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        builder.field(ORDER_FIELD.getPreferredName());
-        order.toXContent(builder, params);
-        builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectMode.parseField().getPreferredName());
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        return builder;
-    }
-
-    @Override
-    protected TermsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        TermsAggregatorBuilder factory = new TermsAggregatorBuilder(name, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.collectMode = SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.order = InternalOrder.Streams.readOrder(in);
-        factory.showTermDocCountError = in.readBoolean();
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        collectMode.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        InternalOrder.Streams.writeOrder(order, out);
-        out.writeBoolean(showTermDocCountError);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, collectMode, executionHint, includeExclude, order, showTermDocCountError);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        TermsAggregatorBuilder other = (TermsAggregatorBuilder) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(collectMode, other.collectMode)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(order, other.order)
-                && Objects.equals(showTermDocCountError, other.showTermDocCountError);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
index 53a7b11..270dc00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
@@ -16,7 +16,6 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations.bucket.terms;
 
 import org.apache.lucene.search.IndexSearcher;
@@ -24,14 +23,10 @@ import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -43,143 +38,10 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, TermsAggregatorFactory> {
-
-    private final Terms.Order order;
-    private final IncludeExclude includeExclude;
-    private final String executionHint;
-    private final SubAggCollectionMode collectMode;
-    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
-    private boolean showTermDocCountError;
-
-    public TermsAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, Terms.Order order,
-            IncludeExclude includeExclude, String executionHint, SubAggCollectionMode collectMode,
-            TermsAggregator.BucketCountThresholds bucketCountThresholds, boolean showTermDocCountError, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.order = order;
-        this.includeExclude = includeExclude;
-        this.executionHint = executionHint;
-        this.collectMode = collectMode;
-        this.bucketCountThresholds = bucketCountThresholds;
-        this.showTermDocCountError = showTermDocCountError;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        final InternalAggregation aggregation = new UnmappedTerms(name, order, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getShardSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
-        return new NonCollectingAggregator(name, context, parent, factories, pipelineAggregators, metaData) {
-            {
-                // even in the case of an unmapped aggregator, validate the
-                // order
-                InternalOrder.validate(order, this);
-            }
-
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return aggregation;
-            }
-        };
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
-                && bucketCountThresholds.getShardSize() == TermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            // The user has not made a shardSize selection. Use default
-            // heuristic to avoid any wrong-ranking caused by distributed
-            // counting
-            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    context.searchContext().numberOfShards()));
-        }
-        bucketCountThresholds.ensureValidity();
-        if (valuesSource instanceof ValuesSource.Bytes) {
-            ExecutionMode execution = null;
-            if (executionHint != null) {
-                execution = ExecutionMode.fromString(executionHint, context.searchContext().parseFieldMatcher());
-            }
-
-            // In some cases, using ordinals is just not supported: override it
-            if (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals)) {
-                execution = ExecutionMode.MAP;
-            }
-
-            final long maxOrd;
-            final double ratio;
-            if (execution == null || execution.needsGlobalOrdinals()) {
-                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
-                IndexSearcher indexSearcher = context.searchContext().searcher();
-                maxOrd = valueSourceWithOrdinals.globalMaxOrd(indexSearcher);
-                ratio = maxOrd / ((double) indexSearcher.getIndexReader().numDocs());
-            } else {
-                maxOrd = -1;
-                ratio = -1;
-            }
-
-            // Let's try to use a good default
-            if (execution == null) {
-                // if there is a parent bucket aggregator the number of
-                // instances of this aggregator is going
-                // to be unbounded and most instances may only aggregate few
-                // documents, so use hashed based
-                // global ordinals to keep the bucket ords dense.
-                if (Aggregator.descendsFromBucketAggregator(parent)) {
-                    execution = ExecutionMode.GLOBAL_ORDINALS_HASH;
-                } else {
-                    if (factories == AggregatorFactories.EMPTY) {
-                        if (ratio <= 0.5 && maxOrd <= 2048) {
-                            // 0.5: At least we need reduce the number of global
-                            // ordinals look-ups by half
-                            // 2048: GLOBAL_ORDINALS_LOW_CARDINALITY has
-                            // additional memory usage, which directly linked to
-                            // maxOrd, so we need to limit.
-                            execution = ExecutionMode.GLOBAL_ORDINALS_LOW_CARDINALITY;
-                        } else {
-                            execution = ExecutionMode.GLOBAL_ORDINALS;
-                        }
-                    } else {
-                        execution = ExecutionMode.GLOBAL_ORDINALS;
-                    }
-                }
-            }
-
-            return execution.create(name, factories, valuesSource, order, bucketCountThresholds, includeExclude, context, parent,
-                    collectMode, showTermDocCountError, pipelineAggregators, metaData);
-        }
-
-        if ((includeExclude != null) && (includeExclude.isRegexBased())) {
-            throw new AggregationExecutionException("Aggregation [" + name + "] cannot support regular expression style include/exclude "
-                    + "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
-        }
-
-        if (valuesSource instanceof ValuesSource.Numeric) {
-            IncludeExclude.LongFilter longFilter = null;
-            if (((ValuesSource.Numeric) valuesSource).isFloatingPoint()) {
-                if (includeExclude != null) {
-                    longFilter = includeExclude.convertToDoubleFilter();
-                }
-                return new DoubleTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
-                        bucketCountThresholds, context, parent, collectMode, showTermDocCountError, longFilter,
-                        pipelineAggregators, metaData);
-            }
-            if (includeExclude != null) {
-                longFilter = includeExclude.convertToLongFilter();
-            }
-            return new LongTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
-                    bucketCountThresholds, context, parent, collectMode, showTermDocCountError, longFilter, pipelineAggregators,
-                    metaData);
-        }
-
-        throw new AggregationExecutionException("terms aggregation cannot be applied to field [" + config.fieldContext().field()
-                + "]. It can only be applied to numeric or string fields.");
-    }
+/**
+ *
+ */
+public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
     public enum ExecutionMode {
 
@@ -190,7 +52,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
+                    throws IOException {
                 final IncludeExclude.StringFilter filter = includeExclude == null ? null : includeExclude.convertToStringFilter();
                 return new StringTermsAggregator(name, factories, valuesSource, order, bucketCountThresholds, filter, aggregationContext,
                         parent, subAggCollectMode, showTermDocCountError, pipelineAggregators, metaData);
@@ -209,7 +71,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
+                    throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
                 return new GlobalOrdinalsStringTermsAggregator(name, factories, (ValuesSource.Bytes.WithOrdinals) valuesSource, order,
                         bucketCountThresholds, filter, aggregationContext, parent, subAggCollectMode, showTermDocCountError,
@@ -229,11 +91,11 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
+                    throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
-                return new GlobalOrdinalsStringTermsAggregator.WithHash(name, factories, (ValuesSource.Bytes.WithOrdinals) valuesSource,
-                        order, bucketCountThresholds, filter, aggregationContext, parent, subAggCollectMode, showTermDocCountError,
-                        pipelineAggregators, metaData);
+                return new GlobalOrdinalsStringTermsAggregator.WithHash(name, factories,
+                        (ValuesSource.Bytes.WithOrdinals) valuesSource, order, bucketCountThresholds, filter, aggregationContext,
+                        parent, subAggCollectMode, showTermDocCountError, pipelineAggregators, metaData);
             }
 
             @Override
@@ -248,10 +110,10 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
-                if (includeExclude != null || factories.countAggregators() > 0
-                // we need the FieldData impl to be able to extract the
-                // segment to global ord mapping
+                    throws IOException {
+                if (includeExclude != null || factories.count() > 0
+                        // we need the FieldData impl to be able to extract the
+                        // segment to global ord mapping
                         || valuesSource.getClass() != ValuesSource.Bytes.FieldData.class) {
                     return GLOBAL_ORDINALS.create(name, factories, valuesSource, order, bucketCountThresholds, includeExclude,
                             aggregationContext, parent, subAggCollectMode, showTermDocCountError, pipelineAggregators, metaData);
@@ -286,7 +148,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                 TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                 AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                 boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                        throws IOException;
+                throws IOException;
 
         abstract boolean needsGlobalOrdinals();
 
@@ -296,4 +158,127 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         }
     }
 
+    private final Terms.Order order;
+    private final IncludeExclude includeExclude;
+    private final String executionHint;
+    private final SubAggCollectionMode collectMode;
+    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
+    private final boolean showTermDocCountError;
+
+    public TermsAggregatorFactory(String name, ValuesSourceConfig config, Terms.Order order,
+            TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude, String executionHint,
+            SubAggCollectionMode executionMode, boolean showTermDocCountError) {
+        super(name, StringTerms.TYPE.name(), config);
+        this.order = order;
+        this.includeExclude = includeExclude;
+        this.executionHint = executionHint;
+        this.bucketCountThresholds = bucketCountThresholds;
+        this.collectMode = executionMode;
+        this.showTermDocCountError = showTermDocCountError;
+    }
+
+    @Override
+    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+        final InternalAggregation aggregation = new UnmappedTerms(name, order, bucketCountThresholds.getRequiredSize(),
+                bucketCountThresholds.getShardSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
+        return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
+            {
+                // even in the case of an unmapped aggregator, validate the order
+                InternalOrder.validate(order, this);
+            }
+
+            @Override
+            public InternalAggregation buildEmptyAggregation() {
+                return aggregation;
+            }
+        };
+    }
+
+    @Override
+    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+            throws IOException {
+        if (collectsFromSingleBucket == false) {
+            return asMultiBucketAggregator(this, aggregationContext, parent);
+        }
+        if (valuesSource instanceof ValuesSource.Bytes) {
+            ExecutionMode execution = null;
+            if (executionHint != null) {
+                execution = ExecutionMode.fromString(executionHint, aggregationContext.searchContext().parseFieldMatcher());
+            }
+
+            // In some cases, using ordinals is just not supported: override it
+            if (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals)) {
+                execution = ExecutionMode.MAP;
+            }
+
+            final long maxOrd;
+            final double ratio;
+            if (execution == null || execution.needsGlobalOrdinals()) {
+                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
+                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
+                maxOrd = valueSourceWithOrdinals.globalMaxOrd(indexSearcher);
+                ratio = maxOrd / ((double) indexSearcher.getIndexReader().numDocs());
+            } else {
+                maxOrd = -1;
+                ratio = -1;
+            }
+
+            // Let's try to use a good default
+            if (execution == null) {
+                // if there is a parent bucket aggregator the number of instances of this aggregator is going
+                // to be unbounded and most instances may only aggregate few documents, so use hashed based
+                // global ordinals to keep the bucket ords dense.
+                if (Aggregator.descendsFromBucketAggregator(parent)) {
+                    execution = ExecutionMode.GLOBAL_ORDINALS_HASH;
+                } else {
+                    if (factories == AggregatorFactories.EMPTY) {
+                        if (ratio <= 0.5 && maxOrd <= 2048) {
+                            // 0.5: At least we need reduce the number of global ordinals look-ups by half
+                            // 2048: GLOBAL_ORDINALS_LOW_CARDINALITY has additional memory usage, which directly linked to maxOrd, so we need to limit.
+                            execution = ExecutionMode.GLOBAL_ORDINALS_LOW_CARDINALITY;
+                        } else {
+                            execution = ExecutionMode.GLOBAL_ORDINALS;
+                        }
+                    } else {
+                        execution = ExecutionMode.GLOBAL_ORDINALS;
+                    }
+                }
+            }
+
+            return execution.create(name, factories, valuesSource, order, bucketCountThresholds, includeExclude, aggregationContext,
+                    parent, collectMode, showTermDocCountError, pipelineAggregators, metaData);
+        }
+
+        if ((includeExclude != null) && (includeExclude.isRegexBased())) {
+            throw new AggregationExecutionException(
+                    "Aggregation ["
+                            + name
+                            + "] cannot support regular expression style include/exclude "
+                            + "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
+        }
+
+        if (valuesSource instanceof ValuesSource.Numeric) {
+            IncludeExclude.LongFilter longFilter = null;
+            if (((ValuesSource.Numeric) valuesSource).isFloatingPoint()) {
+                if (includeExclude != null) {
+                    longFilter = includeExclude.convertToDoubleFilter();
+                }
+                return new DoubleTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
+                        bucketCountThresholds, aggregationContext, parent, collectMode, showTermDocCountError, longFilter,
+                        pipelineAggregators, metaData);
+            }
+            if (includeExclude != null) {
+                longFilter = includeExclude.convertToLongFilter();
+            }
+            return new LongTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
+                    bucketCountThresholds, aggregationContext, parent, collectMode, showTermDocCountError, longFilter, pipelineAggregators,
+                    metaData);
+        }
+
+        throw new AggregationExecutionException("terms aggregation cannot be applied to field [" + config.fieldContext().field()
+                + "]. It can only be applied to numeric or string fields.");
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
new file mode 100644
index 0000000..9bc1f7a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.apache.lucene.util.automaton.RegExp;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Locale;
+
+/**
+ * Builder for the {@link Terms} aggregation.
+ */
+public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
+
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(-1, -1, -1, -1);
+
+    private Terms.ValueType valueType;
+    private Terms.Order order;
+    private String includePattern;
+    private String excludePattern;
+    private String executionHint;
+    private SubAggCollectionMode collectionMode;
+    private Boolean showTermDocCountError;
+    private String[] includeTerms = null;
+    private String[] excludeTerms = null;
+
+    /**
+     * Sole constructor.
+     */
+    public TermsBuilder(String name) {
+        super(name, "terms");
+    }
+
+    /**
+     * Sets the size - indicating how many term buckets should be returned (defaults to 10)
+     */
+    public TermsBuilder size(int size) {
+        bucketCountThresholds.setRequiredSize(size);
+        return this;
+    }
+
+    /**
+     * Sets the shard_size - indicating the number of term buckets each shard will return to the coordinating node (the
+     * node that coordinates the search execution). The higher the shard size is, the more accurate the results are.
+     */
+    public TermsBuilder shardSize(int shardSize) {
+        bucketCountThresholds.setShardSize(shardSize);
+        return this;
+    }
+
+    /**
+     * Set the minimum document count terms should have in order to appear in the response.
+     */
+    public TermsBuilder minDocCount(long minDocCount) {
+        bucketCountThresholds.setMinDocCount(minDocCount);
+        return this;
+    }
+
+    /**
+     * Set the minimum document count terms should have on the shard in order to appear in the response.
+     */
+    public TermsBuilder shardMinDocCount(long shardMinDocCount) {
+        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
+        return this;
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link RegExp} class.
+     *
+     * @see RegExp#RegExp(String)
+     */
+    public TermsBuilder include(String regex) {
+        if (includeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.includePattern = regex;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(String [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = terms;
+        return this;
+    }    
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(long [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = longsArrToStringArr(terms);
+        return this;
+    }     
+    
+    private String[] longsArrToStringArr(long[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Long.toString(terms[i]);
+        }
+        return termsAsString;
+    }      
+    
+
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(double [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = doubleArrToStringArr(terms);
+        return this;
+    }
+
+    private String[] doubleArrToStringArr(double[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Double.toString(terms[i]);
+        }
+        return termsAsString;
+    }    
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link RegExp} class.
+     *
+     * @see RegExp#RegExp(String)
+     */
+    public TermsBuilder exclude(String regex) {
+        if (excludeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludePattern = regex;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(String [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = terms;
+        return this;
+    }    
+    
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(long [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = longsArrToStringArr(terms);
+        return this;
+    }
+
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(double [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = doubleArrToStringArr(terms);
+        return this;
+    }    
+    
+    
+
+    /**
+     * When using scripts, the value type indicates the types of the values the script is generating.
+     */
+    public TermsBuilder valueType(Terms.ValueType valueType) {
+        this.valueType = valueType;
+        return this;
+    }
+
+    /**
+     * Defines the order in which the buckets will be returned.
+     */
+    public TermsBuilder order(Terms.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Expert: provide an execution hint to the aggregation.
+     */
+    public TermsBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    /**
+     * Expert: set the collection mode.
+     */
+    public TermsBuilder collectMode(SubAggCollectionMode mode) {
+        this.collectionMode = mode;
+        return this;
+    }
+
+    /**
+     * Expert: return document count errors per term in the response.
+     */
+    public TermsBuilder showTermDocCountError(boolean showTermDocCountError) {
+        this.showTermDocCountError = showTermDocCountError;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+
+        bucketCountThresholds.toXContent(builder);
+
+        if (showTermDocCountError != null) {
+            builder.field(AbstractTermsParametersParser.SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
+        }
+        if (executionHint != null) {
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+        }
+        if (valueType != null) {
+            builder.field("value_type", valueType.name().toLowerCase(Locale.ROOT));
+        }
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+        if (collectionMode != null) {
+            builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectionMode.parseField().getPreferredName());
+        }
+        if (includeTerms != null) {
+            builder.array("include", includeTerms);
+        }
+        if (includePattern != null) {
+            builder.field("include", includePattern);
+        }
+        if (excludeTerms != null) {
+            builder.array("exclude", excludeTerms);
+        }
+        if (excludePattern != null) {
+            builder.field("exclude", excludePattern);
+        }
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
new file mode 100644
index 0000000..c8138b7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+
+public class TermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10, -1);
+
+    public List<OrderElement> getOrderElements() {
+        return orderElements;
+    }
+    
+    public boolean showTermDocCountError() {
+        return showTermDocCountError;
+    }
+
+    List<OrderElement> orderElements;
+    private boolean showTermDocCountError = false;
+
+    public TermsParametersParser() {
+        orderElements = new ArrayList<>(1);
+        orderElements.add(new OrderElement("_count", false));
+    }
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+        if (token == XContentParser.Token.START_OBJECT) {
+            if ("order".equals(currentFieldName)) {
+                this.orderElements = Collections.singletonList(parseOrderParam(aggregationName, parser, context));
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.START_ARRAY) {
+            if ("order".equals(currentFieldName)) {
+                orderElements = new ArrayList<>();
+                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                    if (token == XContentParser.Token.START_OBJECT) {
+                        OrderElement orderParam = parseOrderParam(aggregationName, parser, context);
+                        orderElements.add(orderParam);
+                    } else {
+                        throw new SearchParseException(context, "Order elements must be of type object in [" + aggregationName + "].",
+                                parser.getTokenLocation());
+                    }
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+            if (context.parseFieldMatcher().match(currentFieldName, SHOW_TERM_DOC_COUNT_ERROR)) {
+                showTermDocCountError = parser.booleanValue();
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    private OrderElement parseOrderParam(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        XContentParser.Token token;
+        OrderElement orderParam = null;
+        String orderKey = null;
+        boolean orderAsc = false;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                orderKey = parser.currentName();
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                String dir = parser.text();
+                if ("asc".equalsIgnoreCase(dir)) {
+                    orderAsc = true;
+                } else if ("desc".equalsIgnoreCase(dir)) {
+                    orderAsc = false;
+                } else {
+                    throw new SearchParseException(context, "Unknown terms order direction [" + dir + "] in terms aggregation ["
+                            + aggregationName + "]", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " for [order] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+        if (orderKey == null) {
+            throw new SearchParseException(context, "Must specify at least one field for [order] in [" + aggregationName + "].",
+                    parser.getTokenLocation());
+        } else {
+            orderParam = new OrderElement(orderKey, orderAsc);
+        }
+        return orderParam;
+    }
+
+    static class OrderElement {
+        private final String key;
+        private final boolean asc;
+
+        public OrderElement(String key, boolean asc) {
+            this.key = key;
+            this.asc = asc;
+        }
+
+        public String key() {
+            return key;
+        }
+
+        public boolean asc() {
+            return asc;
+        }
+
+        
+    }
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
index 1a5f854..478309d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
@@ -18,30 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.terms;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsParametersParser.OrderElement;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class TermsParser extends AbstractTermsParser {
-
+public class TermsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -49,123 +43,37 @@ public class TermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected TermsAggregatorBuilder doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        TermsAggregatorBuilder factory = new TermsAggregatorBuilder(aggregationName, targetValueType);
-        List<OrderElement> orderElements = (List<OrderElement>) otherOptions.get(TermsAggregatorBuilder.ORDER_FIELD);
-        if (orderElements != null) {
-            List<Terms.Order> orders = new ArrayList<>(orderElements.size());
-            for (OrderElement orderElement : orderElements) {
-                orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
-            }
-            factory.order(orders);
-        }
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (collectMode != null) {
-            factory.collectMode(collectMode);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        Boolean showTermDocCountError = (Boolean) otherOptions.get(TermsAggregatorBuilder.SHOW_TERM_DOC_COUNT_ERROR);
-        if (showTermDocCountError != null) {
-            factory.showTermDocCountError(showTermDocCountError);
-        }
-        return factory;
-    }
-
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorBuilder.ORDER_FIELD)) {
-                otherOptions.put(TermsAggregatorBuilder.ORDER_FIELD, Collections.singletonList(parseOrderParam(aggregationName, parser)));
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorBuilder.ORDER_FIELD)) {
-                List<OrderElement> orderElements = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    if (token == XContentParser.Token.START_OBJECT) {
-                        OrderElement orderParam = parseOrderParam(aggregationName, parser);
-                        orderElements.add(orderParam);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Order elements must be of type object in [" + aggregationName + "] found token of type [" + token + "].");
-                    }
-                }
-                otherOptions.put(TermsAggregatorBuilder.ORDER_FIELD, orderElements);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorBuilder.SHOW_TERM_DOC_COUNT_ERROR)) {
-                otherOptions.put(TermsAggregatorBuilder.SHOW_TERM_DOC_COUNT_ERROR, parser.booleanValue());
-                return true;
-            }
-        }
-        return false;
-    }
-
-    private OrderElement parseOrderParam(String aggregationName, XContentParser parser) throws IOException {
-        XContentParser.Token token;
-        OrderElement orderParam = null;
-        String orderKey = null;
-        boolean orderAsc = false;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                orderKey = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                String dir = parser.text();
-                if ("asc".equalsIgnoreCase(dir)) {
-                    orderAsc = true;
-                } else if ("desc".equalsIgnoreCase(dir)) {
-                    orderAsc = false;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown terms order direction [" + dir + "] in terms aggregation [" + aggregationName + "]");
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " for [order] in [" + aggregationName + "].");
-            }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        TermsParametersParser aggParser = new TermsParametersParser();
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, StringTerms.TYPE, context).scriptable(true).formattable(true).build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
+
+        List<OrderElement> orderElements = aggParser.getOrderElements();
+        List<Terms.Order> orders = new ArrayList<>(orderElements.size());
+        for (OrderElement orderElement : orderElements) {
+            orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
         }
-        if (orderKey == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Must specify at least one field for [order] in [" + aggregationName + "].");
-        } else {
-            orderParam = new OrderElement(orderKey, orderAsc);
+        Terms.Order order;
+        if (orders.size() == 1 && (orders.get(0) == InternalOrder.TERM_ASC || orders.get(0) == InternalOrder.TERM_DESC))
+        {
+            // If order is only terms order then we don't need compound ordering
+            order = orders.get(0);
         }
-        return orderParam;
-    }
-
-    static class OrderElement {
-        private final String key;
-        private final boolean asc;
-
-        public OrderElement(String key, boolean asc) {
-            this.key = key;
-            this.asc = asc;
+        else
+        {
+            // for all other cases we need compound order so term order asc can be added to make the order deterministic
+            order = Order.compound(orders);
         }
-
-        public String key() {
-            return key;
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
+                && bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            // The user has not made a shardSize selection. Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
+                    context.numberOfShards()));
         }
-
-        public boolean asc() {
-            return asc;
-        }
-
-    }
-
-    @Override
-    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(TermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        return new TermsAggregatorFactory(aggregationName, vsParser.config(), order, bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getCollectionMode(), aggParser.showTermDocCountError());
     }
 
     static Terms.Order resolveOrder(String key, boolean asc) {
@@ -178,9 +86,4 @@ public class TermsParser extends AbstractTermsParser {
         return Order.aggregation(key, asc);
     }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new TermsAggregatorBuilder(null, null);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
index eee9d4c..9c33a98 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.terms.support;
 
 import com.carrotsearch.hppc.LongHashSet;
 import com.carrotsearch.hppc.LongSet;
-
 import org.apache.lucene.index.RandomAccessOrds;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Terms;
@@ -35,22 +34,12 @@ import org.apache.lucene.util.automaton.CompiledAutomaton;
 import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.WithOrdinals;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashSet;
-import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
@@ -59,16 +48,7 @@ import java.util.TreeSet;
  * Defines the include/exclude regular expression filtering for string terms aggregation. In this filtering logic,
  * exclusion has precedence, where the {@code include} is evaluated first and then the {@code exclude}.
  */
-public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
-
-    private static final IncludeExclude PROTOTYPE = new IncludeExclude(Collections.emptySortedSet(), Collections.emptySortedSet());
-    private static final ParseField INCLUDE_FIELD = new ParseField("include");
-    private static final ParseField EXCLUDE_FIELD = new ParseField("exclude");
-    private static final ParseField PATTERN_FIELD = new ParseField("pattern");
-
-    public static IncludeExclude readFromStream(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
+public class IncludeExclude {
 
     // The includeValue and excludeValue ByteRefs which are the result of the parsing
     // process are converted into a LongFilter when used on numeric fields
@@ -227,10 +207,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         this.excludeValues = null;
     }
 
-    public IncludeExclude(String include, String exclude) {
-        this(include == null ? null : new RegExp(include), exclude == null ? null : new RegExp(exclude));
-    }
-
     /**
      * @param includeValues   The terms to be included
      * @param excludeValues   The terms to be excluded
@@ -245,51 +221,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         this.excludeValues = excludeValues;
     }
 
-    public IncludeExclude(String[] includeValues, String[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    public IncludeExclude(double[] includeValues, double[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    public IncludeExclude(long[] includeValues, long[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(String[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (String value : values) {
-                returnSet.add(new BytesRef(value));
-            }
-        }
-        return returnSet;
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(double[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (double value : values) {
-                returnSet.add(new BytesRef(String.valueOf(value)));
-            }
-        }
-        return returnSet;
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(long[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (long value : values) {
-                returnSet.add(new BytesRef(String.valueOf(value)));
-            }
-        }
-        return returnSet;
-    }
-
     /**
      * Terms adapter around doc values.
      */
@@ -352,14 +283,18 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
     public static class Parser {
 
-        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser,
-                ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
+        String include = null;
+        String exclude = null;
+        SortedSet<BytesRef> includeValues;
+        SortedSet<BytesRef> excludeValues;
+
+        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
 
             if (token == XContentParser.Token.VALUE_STRING) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, parser.text());
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, parser.text());
+                if ("include".equals(currentFieldName)) {
+                    include = parser.text();
+                } else if ("exclude".equals(currentFieldName)) {
+                    exclude = parser.text();
                 } else {
                     return false;
                 }
@@ -367,35 +302,35 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             }
 
             if (token == XContentParser.Token.START_ARRAY) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("include".equals(currentFieldName)) {
+                     includeValues = new TreeSet<>(parseArrayToSet(parser));
                      return true;
                 }
-                if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("exclude".equals(currentFieldName)) {
+                      excludeValues = new TreeSet<>(parseArrayToSet(parser));
                       return true;
                 }
                 return false;
             }
 
             if (token == XContentParser.Token.START_OBJECT) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
+                if ("include".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(INCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                include = parser.text();
                             }
                         }
                     }
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
+                } else if ("exclude".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(EXCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                exclude = parser.text();
                             }
                         }
                     }
@@ -407,7 +342,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
             return false;
         }
-
         private Set<BytesRef> parseArrayToSet(XContentParser parser) throws IOException {
             final Set<BytesRef> set = new HashSet<>();
             if (parser.currentToken() != XContentParser.Token.START_ARRAY) {
@@ -422,27 +356,7 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             return set;
         }
 
-        public IncludeExclude createIncludeExclude(Map<ParseField, Object> otherOptions) {
-            Object includeObject = otherOptions.get(INCLUDE_FIELD);
-            String include = null;
-            SortedSet<BytesRef> includeValues = null;
-            if (includeObject != null) {
-                if (includeObject instanceof String) {
-                    include = (String) includeObject;
-                } else if (includeObject instanceof SortedSet) {
-                    includeValues = (SortedSet<BytesRef>) includeObject;
-                }
-            }
-            Object excludeObject = otherOptions.get(EXCLUDE_FIELD);
-            String exclude = null;
-            SortedSet<BytesRef> excludeValues = null;
-            if (excludeObject != null) {
-                if (excludeObject instanceof String) {
-                    exclude = (String) excludeObject;
-                } else if (excludeObject instanceof SortedSet) {
-                    excludeValues = (SortedSet<BytesRef>) excludeObject;
-                }
-            }
+        public IncludeExclude includeExclude() {
             RegExp includePattern =  include != null ? new RegExp(include) : null;
             RegExp excludePattern = exclude != null ? new RegExp(exclude) : null;
             if (includePattern != null || excludePattern != null) {
@@ -530,111 +444,4 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         return result;
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (include != null) {
-            builder.field(INCLUDE_FIELD.getPreferredName(), include.getOriginalString());
-        }
-        if (includeValues != null) {
-            builder.startArray(INCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : includeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        if (exclude != null) {
-            builder.field(EXCLUDE_FIELD.getPreferredName(), exclude.getOriginalString());
-        }
-        if (excludeValues != null) {
-            builder.startArray(EXCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : excludeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        return builder;
-    }
-
-    @Override
-    public IncludeExclude readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            String includeString = in.readOptionalString();
-            RegExp include = null;
-            if (includeString != null) {
-                include = new RegExp(includeString);
-            }
-            String excludeString = in.readOptionalString();
-            RegExp exclude = null;
-            if (excludeString != null) {
-                exclude = new RegExp(excludeString);
-            }
-            return new IncludeExclude(include, exclude);
-        } else {
-            SortedSet<BytesRef> includes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                includes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    includes.add(in.readBytesRef());
-                }
-            }
-            SortedSet<BytesRef> excludes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                excludes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    excludes.add(in.readBytesRef());
-                }
-            }
-            return new IncludeExclude(includes, excludes);
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean regexBased = isRegexBased();
-        out.writeBoolean(regexBased);
-        if (regexBased) {
-            out.writeOptionalString(include == null ? null : include.getOriginalString());
-            out.writeOptionalString(exclude == null ? null : exclude.getOriginalString());
-        } else {
-            boolean hasIncludes = includeValues != null;
-            out.writeBoolean(hasIncludes);
-            if (hasIncludes) {
-                out.writeVInt(includeValues.size());
-                for (BytesRef value : includeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-            boolean hasExcludes = excludeValues != null;
-            out.writeBoolean(hasExcludes);
-            if (hasExcludes) {
-                out.writeVInt(excludeValues.size());
-                for (BytesRef value : excludeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(include == null ? null : include.getOriginalString(), exclude == null ? null : exclude.getOriginalString(),
-                includeValues, excludeValues);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        } if (getClass() != obj.getClass()) {
-            return false;
-        }
-        IncludeExclude other = (IncludeExclude) obj;
-        return Objects.equals(include == null ? null : include.getOriginalString(), other.include == null ? null : other.include.getOriginalString())
-                && Objects.equals(exclude == null ? null : exclude.getOriginalString(), other.exclude == null ? null : other.exclude.getOriginalString())
-                && Objects.equals(includeValues, other.includeValues)
-                && Objects.equals(excludeValues, other.excludeValues);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java
new file mode 100644
index 0000000..56ae24b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Base builder for metrics aggregations.
+ */
+public abstract class MetricsAggregationBuilder<B extends MetricsAggregationBuilder<B>> extends AbstractAggregationBuilder {
+
+    public MetricsAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).startObject(type);
+        internalXContent(builder, params);
+        return builder.endObject().endObject();
+    }
+
+    protected abstract void internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
new file mode 100644
index 0000000..6847a9a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class NumericValuesSourceMetricsAggregatorParser<S extends InternalNumericMetricsAggregation> implements Aggregator.Parser {
+
+    protected final InternalAggregation.Type aggType;
+
+    protected NumericValuesSourceMetricsAggregatorParser(InternalAggregation.Type aggType) {
+        this.aggType = aggType;
+    }
+
+    @Override
+    public String type() {
+        return aggType.name();
+    }
+
+    @Override
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, aggType, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return createFactory(aggregationName, vsParser.config());
+    }
+
+    protected abstract AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config);
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
new file mode 100644
index 0000000..e675548
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class ValuesSourceMetricsAggregationBuilder<B extends ValuesSourceMetricsAggregationBuilder<B>> extends MetricsAggregationBuilder<B> {
+
+    private String field;
+    private Script script;
+    private String format;
+    private Object missing;
+
+    protected ValuesSourceMetricsAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @SuppressWarnings("unchecked")
+    public B field(String field) {
+        this.field = field;
+        return (B) this;
+    }
+
+    /**
+     * The script to use for this aggregation
+     */
+    @SuppressWarnings("unchecked")
+    public B script(Script script) {
+        this.script = script;
+        return (B) this;
+    }
+
+    @SuppressWarnings("unchecked")
+    public B format(String format) {
+        this.format = format;
+        return (B) this;
+    }
+
+    /**
+     * Configure the value to use when documents miss a value.
+     */
+    public B missing(Object missingValue) {
+        this.missing = missingValue;
+        return (B) this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (script != null) {
+            builder.field("script", script);
+        }
+
+        if (format != null) {
+            builder.field("format", format);
+        }
+
+        if (missing != null) {
+            builder.field("missing", missing);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
index 0f6657e..17506b7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
@@ -19,29 +19,21 @@
 package org.elasticsearch.search.aggregations.metrics.avg;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -121,42 +113,24 @@ public class AvgAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalAvg(name, 0.0, 0L, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class AvgAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, AvgAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public AvgAggregatorBuilder(String name) {
-            super(name, InternalAvg.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, String type, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
         }
 
         @Override
-        protected AvgAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new AvgAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new AvgAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected AvgAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new AvgAggregator.AvgAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new AvgAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregatorFactory.java
deleted file mode 100644
index e3728d0..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.avg;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class AvgAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AvgAggregatorFactory> {
-
-    public AvgAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new AvgAggregator(name, null, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new AvgAggregator(name, valuesSource, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
new file mode 100644
index 0000000..143f39b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.avg;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Avg} aggregation.
+ */
+public class AvgBuilder extends ValuesSourceMetricsAggregationBuilder<AvgBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public AvgBuilder(String name) {
+        super(name, InternalAvg.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
index 3897c7f..1c2b2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.avg;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class AvgParser extends NumericValuesSourceParser {
+public class AvgParser extends NumericValuesSourceMetricsAggregatorParser<InternalAvg> {
 
     public AvgParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalAvg.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected AvgAggregator.AvgAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new AvgAggregator.AvgAggregatorBuilder(aggregationName);
+        super(InternalAvg.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new AvgAggregator.AvgAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new AvgAggregator.Factory(aggregationName, type(), config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorBuilder.java
deleted file mode 100644
index 4f033c3..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorBuilder.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.cardinality;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import java.io.IOException;
-import java.util.Objects;
-
-public final class CardinalityAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource, CardinalityAggregatorBuilder> {
-
-    public static final ParseField PRECISION_THRESHOLD_FIELD = new ParseField("precision_threshold");
-
-    private Long precisionThreshold = null;
-
-    public CardinalityAggregatorBuilder(String name, ValueType targetValueType) {
-        super(name, InternalCardinality.TYPE, ValuesSourceType.ANY, targetValueType);
-    }
-
-    /**
-     * Set a precision threshold. Higher values improve accuracy but also
-     * increase memory usage.
-     */
-    public CardinalityAggregatorBuilder precisionThreshold(long precisionThreshold) {
-        this.precisionThreshold = precisionThreshold;
-        return this;
-    }
-
-    /**
-     * Get the precision threshold. Higher values improve accuracy but also
-     * increase memory usage. Will return <code>null</code> if the
-     * precisionThreshold has not been set yet.
-     */
-    public Long precisionThreshold() {
-        return precisionThreshold;
-    }
-
-    /**
-     * @deprecated no replacement - values will always be rehashed
-     */
-    @Deprecated
-    public void rehash(boolean rehash) {
-        // Deprecated all values are already rehashed so do nothing
-    }
-
-    @Override
-    protected CardinalityAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new CardinalityAggregatorFactory(name, type, config, precisionThreshold, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected CardinalityAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        CardinalityAggregatorBuilder factory = new CardinalityAggregatorBuilder(name, targetValueType);
-        if (in.readBoolean()) {
-            factory.precisionThreshold = in.readLong();
-        }
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        boolean hasPrecisionThreshold = precisionThreshold != null;
-        out.writeBoolean(hasPrecisionThreshold);
-        if (hasPrecisionThreshold) {
-            out.writeLong(precisionThreshold);
-        }
-    }
-
-    @Override
-    public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        if (precisionThreshold != null) {
-            builder.field(PRECISION_THRESHOLD_FIELD.getPreferredName(), precisionThreshold);
-        }
-        return builder;
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(precisionThreshold);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        CardinalityAggregatorBuilder other = (CardinalityAggregatorBuilder) obj;
-        return Objects.equals(precisionThreshold, other.precisionThreshold);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
index 326b23d..8fff043 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
@@ -20,9 +20,6 @@
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -34,34 +31,32 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, CardinalityAggregatorFactory> {
+final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource> {
 
-    private final Long precisionThreshold;
+    private final long precisionThreshold;
 
-    public CardinalityAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, Long precisionThreshold,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
+    CardinalityAggregatorFactory(String name, ValuesSourceConfig config, long precisionThreshold) {
+        super(name, InternalCardinality.TYPE.name(), config);
         this.precisionThreshold = precisionThreshold;
     }
 
+    private int precision(Aggregator parent) {
+        return precisionThreshold < 0 ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
+    }
+
     @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+    protected Aggregator createUnmapped(AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException {
         return new CardinalityAggregator(name, null, precision(parent), config.formatter(), context, parent, pipelineAggregators, metaData);
     }
 
     @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext context, Aggregator parent,
+            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
         return new CardinalityAggregator(name, valuesSource, precision(parent), config.formatter(), context, parent, pipelineAggregators,
                 metaData);
     }
 
-    private int precision(Aggregator parent) {
-        return precisionThreshold == null ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
-    }
-
     /*
      * If one of the parent aggregators is a MULTI_BUCKET one, we might want to lower the precision
      * because otherwise it might be memory-intensive. On the other hand, for top-level aggregators
@@ -71,7 +66,7 @@ public class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory<
         int precision = HyperLogLogPlusPlus.DEFAULT_PRECISION;
         while (parent != null) {
             if (parent instanceof SingleBucketAggregator == false) {
-                // if the parent creates buckets, we substract 5 to the precision,
+                // if the parent creates buckets, we subtract 5 to the precision,
                 // which will effectively divide the memory usage of each counter by 32
                 precision -= 5;
             }
@@ -80,4 +75,5 @@ public class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory<
 
         return Math.max(precision, HyperLogLogPlusPlus.MIN_PRECISION);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
new file mode 100644
index 0000000..7680aa0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.cardinality;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Cardinality} aggregation.
+ */
+public class CardinalityBuilder extends ValuesSourceMetricsAggregationBuilder<CardinalityBuilder> {
+
+    private Long precisionThreshold;
+    private Boolean rehash;
+
+    /**
+     * Sole constructor.
+     */
+    public CardinalityBuilder(String name) {
+        super(name, InternalCardinality.TYPE.name());
+    }
+
+    /**
+     * Set a precision threshold. Higher values improve accuracy but also
+     * increase memory usage.
+     */
+    public CardinalityBuilder precisionThreshold(long precisionThreshold) {
+        this.precisionThreshold = precisionThreshold;
+        return this;
+    }
+
+    /**
+     * Expert: set to false in case values of this field can already be treated
+     * as 64-bits hash values.
+     */
+    public CardinalityBuilder rehash(boolean rehash) {
+        this.rehash = rehash;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+        if (precisionThreshold != null) {
+            builder.field("precision_threshold", precisionThreshold);
+        }
+        if (rehash != null) {
+            builder.field("rehash", rehash);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
index 157d37e..6833945 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
@@ -20,59 +20,56 @@
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 
-public class CardinalityParser extends AnyValuesSourceParser {
+public class CardinalityParser implements Aggregator.Parser {
 
+    private static final ParseField PRECISION_THRESHOLD = new ParseField("precision_threshold");
     private static final ParseField REHASH = new ParseField("rehash").withAllDeprecated("no replacement - values will always be rehashed");
 
-    public CardinalityParser() {
-        super(true, false);
-    }
-
     @Override
     public String type() {
         return InternalCardinality.TYPE.name();
     }
 
     @Override
-    protected CardinalityAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        CardinalityAggregatorBuilder factory = new CardinalityAggregatorBuilder(aggregationName, targetValueType);
-        Long precisionThreshold = (Long) otherOptions.get(CardinalityAggregatorBuilder.PRECISION_THRESHOLD_FIELD);
-        if (precisionThreshold != null) {
-            factory.precisionThreshold(precisionThreshold);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String name, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, CardinalityAggregatorBuilder.PRECISION_THRESHOLD_FIELD)) {
-                otherOptions.put(CardinalityAggregatorBuilder.PRECISION_THRESHOLD_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REHASH)) {
-                // ignore
-                return true;
+        ValuesSourceParser<?> vsParser = ValuesSourceParser.any(name, InternalCardinality.TYPE, context).formattable(false).build();
+
+        long precisionThreshold = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if (context.parseFieldMatcher().match(currentFieldName, REHASH)) {
+                    // ignore
+                } else if (context.parseFieldMatcher().match(currentFieldName, PRECISION_THRESHOLD)) {
+                    precisionThreshold = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + name + "]: [" + currentFieldName
+                            + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + name + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new CardinalityAggregatorBuilder(null, null);
+        return new CardinalityAggregatorFactory(name, vsParser.config(), precisionThreshold);
+
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/HyperLogLogPlusPlus.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/HyperLogLogPlusPlus.java
index 6b38918..2e8ce45 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/HyperLogLogPlusPlus.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/HyperLogLogPlusPlus.java
@@ -47,7 +47,7 @@ import java.nio.ByteOrder;
  * requires more space and makes hyperloglog (which is less accurate) used sooner,
  * this is also considerably faster.
  *
- * Trying to understand what this class does whithout having read the paper is
+ * Trying to understand what this class does without having read the paper is
  * considered adventurous.
  */
 public final class HyperLogLogPlusPlus implements Releasable {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
index be2c773..8c6159e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
@@ -20,38 +20,28 @@
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public final class GeoBoundsAggregator extends MetricsAggregator {
 
-    static final ParseField WRAP_LONGITUDE_FIELD = new ParseField("wrap_longitude");
-
     private final ValuesSource.GeoPoint valuesSource;
     private final boolean wrapLongitude;
     DoubleArray tops;
@@ -170,69 +160,32 @@ public final class GeoBoundsAggregator extends MetricsAggregator {
         return new InternalGeoBounds(name, Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY,
                 Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, wrapLongitude, pipelineAggregators(), metaData());
     }
-
+    
     @Override
     public void doClose() {
         Releasables.close(tops, bottoms, posLefts, posRights, negLefts, negRights);
     }
 
-    public static class GeoBoundsAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource.GeoPoint, GeoBoundsAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        private boolean wrapLongitude = true;
-
-        public GeoBoundsAggregatorBuilder(String name) {
-            super(name, InternalGeoBounds.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
+        private final boolean wrapLongitude;
 
-        /**
-         * Set whether to wrap longitudes. Defaults to true.
-         */
-        public GeoBoundsAggregatorBuilder wrapLongitude(boolean wrapLongitude) {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, boolean wrapLongitude) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
             this.wrapLongitude = wrapLongitude;
-            return this;
-        }
-
-        /**
-         * Get whether to wrap longitudes.
-         */
-        public boolean wrapLongitude() {
-            return wrapLongitude;
-        }
-
-        @Override
-        protected GeoBoundsAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource.GeoPoint> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new GeoBoundsAggregatorFactory(name, type, config, wrapLongitude, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected GeoBoundsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            GeoBoundsAggregatorBuilder factory = new GeoBoundsAggregatorBuilder(name);
-            factory.wrapLongitude = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(wrapLongitude);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(WRAP_LONGITUDE_FIELD.getPreferredName(), wrapLongitude);
-            return builder;
         }
 
         @Override
-        protected int innerHashCode() {
-            return Objects.hash(wrapLongitude);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new GeoBoundsAggregator(name, aggregationContext, parent, null, wrapLongitude, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            GeoBoundsAggregatorBuilder other = (GeoBoundsAggregatorBuilder) obj;
-            return Objects.equals(wrapLongitude, other.wrapLongitude);
+        protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new GeoBoundsAggregator(name, aggregationContext, parent, valuesSource, wrapLongitude, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregatorFactory.java
deleted file mode 100644
index e7420a9..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregatorFactory.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.geobounds;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GeoBoundsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoBoundsAggregatorFactory> {
-
-    private final boolean wrapLongitude;
-
-    public GeoBoundsAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource.GeoPoint> config, boolean wrapLongitude,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.wrapLongitude = wrapLongitude;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new GeoBoundsAggregator(name, context, parent, null, wrapLongitude, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new GeoBoundsAggregator(name, context, parent, valuesSource, wrapLongitude, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
new file mode 100644
index 0000000..582ab8c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geobounds;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link GeoBounds} aggregation.
+ */
+public class GeoBoundsBuilder extends ValuesSourceAggregationBuilder<GeoBoundsBuilder> {
+
+    private Boolean wrapLongitude;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoBoundsBuilder(String name) {
+        super(name, InternalGeoBounds.TYPE.name());
+    }
+
+    /**
+     * Set whether to wrap longitudes. Defaults to true.
+     */
+    public GeoBoundsBuilder wrapLongitude(boolean wrapLongitude) {
+        this.wrapLongitude = wrapLongitude;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (wrapLongitude != null) {
+            builder.field("wrap_longitude", wrapLongitude);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
index 5c3112f..de1fea2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
@@ -19,23 +19,18 @@
 
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource.GeoPoint;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class GeoBoundsParser extends GeoPointValuesSourceParser {
-
-    public GeoBoundsParser() {
-        super(false, false);
-    }
+public class GeoBoundsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -43,31 +38,33 @@ public class GeoBoundsParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected GeoBoundsAggregator.GeoBoundsAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoBoundsAggregator.GeoBoundsAggregatorBuilder factory = new GeoBoundsAggregator.GeoBoundsAggregatorBuilder(aggregationName);
-        Boolean wrapLongitude = (Boolean) otherOptions.get(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD);
-        if (wrapLongitude != null) {
-            factory.wrapLongitude(wrapLongitude);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, GeoBoundsAggregator.WRAP_LONGITUDE_FIELD)) {
-                otherOptions.put(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD, parser.booleanValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoBounds.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        boolean wrapLongitude = true;
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+                
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("wrap_longitude".equals(currentFieldName) || "wrapLongitude".equals(currentFieldName)) {
+                    wrapLongitude = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoBoundsAggregator.GeoBoundsAggregatorBuilder(null);
+        return new GeoBoundsAggregator.Factory(aggregationName, vsParser.config(), wrapLongitude);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
index a4ef9f7..b5739f5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
@@ -22,27 +22,22 @@ package org.elasticsearch.search.aggregations.metrics.geocentroid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.GeoUtils;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
+import org.elasticsearch.search.aggregations.metrics.geobounds.InternalGeoBounds;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
@@ -127,43 +122,22 @@ public final class GeoCentroidAggregator extends MetricsAggregator {
         Releasables.close(centroids, counts);
     }
 
-    public static class GeoCentroidAggregatorBuilder
-            extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.GeoPoint, GeoCentroidAggregatorBuilder> {
-
-        public GeoCentroidAggregatorBuilder(String name) {
-            super(name, InternalGeoCentroid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
-
-        @Override
-        protected GeoCentroidAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource.GeoPoint> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new GeoCentroidAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected GeoCentroidAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            return new GeoCentroidAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.GeoPoint> {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
         }
 
         @Override
-        protected int innerHashCode() {
-            return 0;
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                                            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new GeoCentroidAggregator(name, aggregationContext, parent, null, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new GeoCentroidAggregator(name, aggregationContext, parent, valuesSource, pipelineAggregators, metaData);
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregatorFactory.java
deleted file mode 100644
index 6fa2d28..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.geocentroid;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GeoCentroidAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoCentroidAggregatorFactory> {
-
-    public GeoCentroidAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource.GeoPoint> config,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new GeoCentroidAggregator(name, context, parent, null, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new GeoCentroidAggregator(name, context, parent, valuesSource, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
new file mode 100644
index 0000000..9d6823c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geocentroid;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
+ */
+public class GeoCentroidBuilder extends ValuesSourceMetricsAggregationBuilder<GeoCentroidBuilder> {
+
+    public GeoCentroidBuilder(String name) {
+        super(name, InternalGeoCentroid.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
index 4bf7ccf..49a7bc8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
@@ -19,26 +19,21 @@
 
 package org.elasticsearch.search.aggregations.metrics.geocentroid;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parser class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
  */
-public class GeoCentroidParser extends GeoPointValuesSourceParser {
-
-    public GeoCentroidParser() {
-        super(true, false);
-    }
+public class GeoCentroidParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +41,23 @@ public class GeoCentroidParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected GeoCentroidAggregator.GeoCentroidAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new GeoCentroidAggregator.GeoCentroidAggregatorBuilder(aggregationName);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoCentroidAggregator.GeoCentroidAggregatorBuilder(null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoCentroid.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        }
+        return new GeoCentroidAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
index dd5e667..e70fc7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
@@ -19,30 +19,22 @@
 package org.elasticsearch.search.aggregations.metrics.max;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -122,44 +114,24 @@ public class MaxAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMax(name, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class MaxAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, MaxAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public MaxAggregatorBuilder(String name) {
-            super(name, InternalMax.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMax.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected MaxAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new MaxAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new MaxAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected MaxAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MaxAggregator.MaxAggregatorBuilder(name);
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new MaxAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
-
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregatorFactory.java
deleted file mode 100644
index 45acde7..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.max;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class MaxAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, MaxAggregatorFactory> {
-
-    public MaxAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new MaxAggregator(name, null, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new MaxAggregator(name, valuesSource, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
new file mode 100644
index 0000000..b433573
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.max;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Max} aggregation.
+ */
+public class MaxBuilder extends ValuesSourceMetricsAggregationBuilder<MaxBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MaxBuilder(String name) {
+        super(name, InternalMax.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
index ee517b7..a5cdac5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.max;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MaxParser extends NumericValuesSourceParser {
+public class MaxParser extends NumericValuesSourceMetricsAggregatorParser<InternalMax> {
 
     public MaxParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMax.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MaxAggregator.MaxAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MaxAggregator.MaxAggregatorBuilder(aggregationName);
+        super(InternalMax.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new MaxAggregator.MaxAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MaxAggregator.Factory(aggregationName, config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
index 70b7050..3a9bd40 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
@@ -19,30 +19,22 @@
 package org.elasticsearch.search.aggregations.metrics.min;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -121,42 +113,23 @@ public class MinAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMin(name, Double.POSITIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class MinAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, MinAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public MinAggregatorBuilder(String name) {
-            super(name, InternalMin.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMin.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected MinAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new MinAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new MinAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected MinAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MinAggregator.MinAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new MinAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregatorFactory.java
deleted file mode 100644
index c28a338..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.min;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class MinAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, MinAggregatorFactory> {
-
-    public MinAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new MinAggregator(name, null, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new MinAggregator(name, valuesSource, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
new file mode 100644
index 0000000..e5342de
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.min;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Min} aggregation.
+ */
+public class MinBuilder extends ValuesSourceMetricsAggregationBuilder<MinBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MinBuilder(String name) {
+        super(name, InternalMin.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
index e43b691..e7dde9c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
@@ -18,46 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.min;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MinParser extends NumericValuesSourceParser {
+public class MinParser extends NumericValuesSourceMetricsAggregatorParser<InternalMin> {
 
     public MinParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMin.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MinAggregator.MinAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MinAggregator.MinAggregatorBuilder(aggregationName);
+        super(InternalMin.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new MinAggregator.MinAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MinAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java
new file mode 100644
index 0000000..c587d64
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+abstract class AbstractPercentilesBuilder<PB extends AbstractPercentilesBuilder<PB>> extends
+        ValuesSourceMetricsAggregationBuilder<PB> {
+
+    private Double compression;
+    private PercentilesMethod method;
+    private Integer numberOfSignificantValueDigits;
+
+    public AbstractPercentilesBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Expert: Set the method to use to compute the percentiles.
+     */
+    public PB method(PercentilesMethod method) {
+        this.method = method;
+        return (PB) this;
+    }
+
+    /**
+     * Expert: set the compression. Higher values improve accuracy but also
+     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
+     */
+    public PB compression(double compression) {
+        this.compression = compression;
+        return (PB) this;
+    }
+
+    /**
+     * Expert: set the number of significant digits in the values. Only relevant
+     * when using {@link PercentilesMethod#HDR}.
+     */
+    public PB numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
+        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+        return (PB) this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        doInternalXContent(builder, params);
+
+        if (method != null) {
+            builder.startObject(method.getName());
+
+            if (compression != null) {
+                builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
+            }
+
+            if (numberOfSignificantValueDigits != null) {
+                builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
+            }
+
+            builder.endObject();
+        }
+    }
+
+    protected abstract void doInternalXContent(XContentBuilder builder, Params params) throws IOException;
+
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
index 0a22b03..0b30040 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
@@ -21,19 +21,21 @@ package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import com.carrotsearch.hppc.DoubleArrayList;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
+import java.util.Arrays;
 
-public abstract class AbstractPercentilesParser extends NumericValuesSourceParser {
+public abstract class AbstractPercentilesParser implements Aggregator.Parser {
 
     public static final ParseField KEYED_FIELD = new ParseField("keyed");
     public static final ParseField METHOD_FIELD = new ParseField("method");
@@ -43,95 +45,139 @@ public abstract class AbstractPercentilesParser extends NumericValuesSourceParse
     private boolean formattable;
 
     public AbstractPercentilesParser(boolean formattable) {
-        super(true, formattable, false);
+        this.formattable = formattable;
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, keysField())) {
-                DoubleArrayList values = new DoubleArrayList(10);
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    double value = parser.doubleValue();
-                    values.add(value);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalTDigestPercentiles.TYPE, context)
+                .formattable(formattable).build();
+
+        double[] keys = null;
+        boolean keyed = true;
+        Double compression = null;
+        Integer numberOfSignificantValueDigits = null;
+        PercentilesMethod method = null;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if (context.parseFieldMatcher().match(currentFieldName, keysField())) {
+                    DoubleArrayList values = new DoubleArrayList(10);
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double value = parser.doubleValue();
+                        values.add(value);
+                    }
+                    keys = values.toArray();
+                    Arrays.sort(keys);
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                double[] keys = values.toArray();
-                otherOptions.put(keysField(), keys);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(KEYED_FIELD, keyed);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            PercentilesMethod method = PercentilesMethod.resolveFromName(currentFieldName);
-            if (method == null) {
-                return false;
-            } else {
-                otherOptions.put(METHOD_FIELD, method);
-                switch (method) {
-                case TDIGEST:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, COMPRESSION_FIELD)) {
-                                double compression = parser.doubleValue();
-                                otherOptions.put(COMPRESSION_FIELD, compression);
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if (context.parseFieldMatcher().match(currentFieldName, KEYED_FIELD)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if (method != null) {
+                    throw new SearchParseException(context, "Found multiple methods in [" + aggregationName + "]: [" + currentFieldName
+                            + "]. only one of [" + PercentilesMethod.TDIGEST.getName() + "] and [" + PercentilesMethod.HDR.getName()
+                            + "] may be used.", parser.getTokenLocation());
+                }
+                method = PercentilesMethod.resolveFromName(currentFieldName);
+                if (method == null) {
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
+                } else {
+                    switch (method) {
+                    case TDIGEST:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, COMPRESSION_FIELD)) {
+                                    compression = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
-                    }
-                    break;
-                case HDR:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
-                                int numberOfSignificantValueDigits = parser.intValue();
-                                otherOptions.put(NUMBER_SIGNIFICANT_DIGITS_FIELD, numberOfSignificantValueDigits);
+                        break;
+                    case HDR:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
+                                    numberOfSignificantValueDigits = parser.intValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
+                        break;
                     }
-                    break;
                 }
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ValuesSourceAggregatorBuilder<Numeric, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        PercentilesMethod method = (PercentilesMethod) otherOptions.getOrDefault(METHOD_FIELD, PercentilesMethod.TDIGEST);
-
-        double[] cdfValues = (double[]) otherOptions.get(keysField());
-        Double compression = (Double) otherOptions.get(COMPRESSION_FIELD);
-        Integer numberOfSignificantValueDigits = (Integer) otherOptions.get(NUMBER_SIGNIFICANT_DIGITS_FIELD);
-        Boolean keyed = (Boolean) otherOptions.get(KEYED_FIELD);
-        return buildFactory(aggregationName, cdfValues, method, compression, numberOfSignificantValueDigits, keyed);
+        if (method == null) {
+            method = PercentilesMethod.TDIGEST;
+        }
+
+        switch (method) {
+        case TDIGEST:
+            if (numberOfSignificantValueDigits != null) {
+                throw new SearchParseException(context, "[number_of_significant_value_digits] cannot be used with method [tdigest] in ["
+                        + aggregationName + "].", parser.getTokenLocation());
+            }
+            if (compression == null) {
+                compression = 100.0;
+            }
+            break;
+        case HDR:
+            if (compression != null) {
+                throw new SearchParseException(context, "[compression] cannot be used with method [hdr] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+            if (numberOfSignificantValueDigits == null) {
+                numberOfSignificantValueDigits = 3;
+            }
+            break;
+
+        default:
+            // Shouldn't get here but if we do, throw a parse exception for
+            // invalid method
+            throw new SearchParseException(context, "Unknown value for [" + currentFieldName + "] in [" + aggregationName + "]: [" + method
+                    + "].", parser.getTokenLocation());
+        }
+
+        return buildFactory(context, aggregationName, vsParser.config(), keys, method, compression,
+                numberOfSignificantValueDigits, keyed);
     }
 
-    protected abstract ValuesSourceAggregatorBuilder<Numeric, ?> buildFactory(String aggregationName, double[] cdfValues,
-            PercentilesMethod method,
-            Double compression,
-            Integer numberOfSignificantValueDigits, Boolean keyed);
+    protected abstract AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> config,
+            double[] cdfValues, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed);
 
     protected abstract ParseField keysField();
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorBuilder.java
deleted file mode 100644
index d022c91..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorBuilder.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregatorFactory;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder.LeafOnly;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
-
-public class PercentileRanksAggregatorBuilder extends LeafOnly<ValuesSource.Numeric, PercentileRanksAggregatorBuilder> {
-
-    private double[] values;
-    private PercentilesMethod method = PercentilesMethod.TDIGEST;
-    private int numberOfSignificantValueDigits = 3;
-    private double compression = 100.0;
-    private boolean keyed = false;
-
-    public PercentileRanksAggregatorBuilder(String name) {
-        super(name, InternalTDigestPercentileRanks.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-    }
-
-    /**
-     * Set the values to compute percentiles from.
-     */
-    public PercentileRanksAggregatorBuilder values(double... values) {
-        double[] sortedValues = Arrays.copyOf(values, values.length);
-        Arrays.sort(sortedValues);
-        this.values = sortedValues;
-        return this;
-    }
-
-    /**
-     * Get the values to compute percentiles from.
-     */
-    public double[] values() {
-        return values;
-    }
-
-    /**
-     * Set whether the XContent response should be keyed
-     */
-    public PercentileRanksAggregatorBuilder keyed(boolean keyed) {
-        this.keyed = keyed;
-        return this;
-    }
-
-    /**
-     * Get whether the XContent response should be keyed
-     */
-    public boolean keyed() {
-        return keyed;
-    }
-
-    /**
-     * Expert: set the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public PercentileRanksAggregatorBuilder numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        return this;
-    }
-
-    /**
-     * Expert: get the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public int numberOfSignificantValueDigits() {
-        return numberOfSignificantValueDigits;
-    }
-
-    /**
-     * Expert: set the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public PercentileRanksAggregatorBuilder compression(double compression) {
-        this.compression = compression;
-        return this;
-    }
-
-    /**
-     * Expert: get the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public double compression() {
-        return compression;
-    }
-
-    public PercentileRanksAggregatorBuilder method(PercentilesMethod method) {
-        this.method = method;
-        return this;
-    }
-
-    public PercentilesMethod method() {
-        return method;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentileRanksAggregatorFactory(name, type, config, values, compression, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
-        case HDR:
-            return new HDRPercentileRanksAggregatorFactory(name, type, config, values, numberOfSignificantValueDigits, keyed, context,
-                    parent, subFactoriesBuilder, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected PercentileRanksAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        PercentileRanksAggregatorBuilder factory = new PercentileRanksAggregatorBuilder(name);
-        factory.values = in.readDoubleArray();
-        factory.keyed = in.readBoolean();
-        factory.numberOfSignificantValueDigits = in.readVInt();
-        factory.compression = in.readDouble();
-        factory.method = PercentilesMethod.TDIGEST.readFrom(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        out.writeDoubleArray(values);
-        out.writeBoolean(keyed);
-        out.writeVInt(numberOfSignificantValueDigits);
-        out.writeDouble(compression);
-        method.writeTo(out);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
-        builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-        builder.startObject(method.getName());
-        if (method == PercentilesMethod.TDIGEST) {
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-        } else {
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        PercentileRanksAggregatorBuilder other = (PercentileRanksAggregatorBuilder) obj;
-        if (!Objects.equals(method, other.method)) {
-            return false;
-        }
-        boolean equalSettings = false;
-        switch (method) {
-        case HDR:
-            equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-            break;
-        case TDIGEST:
-            equalSettings = Objects.equals(compression, other.compression);
-            break;
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-        return equalSettings
-                && Objects.deepEquals(values, other.values)
-                && Objects.equals(keyed, other.keyed)
-                && Objects.equals(method, other.method);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        switch (method) {
-        case HDR:
-            return Objects.hash(Arrays.hashCode(values), keyed, numberOfSignificantValueDigits, method);
-        case TDIGEST:
-            return Objects.hash(Arrays.hashCode(values), keyed, compression, method);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java
new file mode 100644
index 0000000..abb9bc5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link PercentileRanks} aggregation.
+ */
+public class PercentileRanksBuilder extends AbstractPercentilesBuilder<PercentileRanksBuilder> {
+
+    private double[] values;
+
+    /**
+     * Sole constructor.
+     */
+    public PercentileRanksBuilder(String name) {
+        super(name, PercentileRanks.TYPE_NAME);
+    }
+
+    /**
+     * Set the values to compute percentiles from.
+     */
+    public PercentileRanksBuilder percentiles(double... values) {
+        this.values = values;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+
+        if (values != null) {
+            builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
index 400ddb5..51e900a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
@@ -19,10 +19,14 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -46,30 +50,19 @@ public class PercentileRanksParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorBuilder<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
-        PercentileRanksAggregatorBuilder factory = new PercentileRanksAggregatorBuilder(aggregationName);
-        if (keys != null) {
-            factory.values(keys);
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            throw new SearchParseException(context, "Missing token values in [" + aggregationName + "].", null);
         }
-        if (method != null) {
-            factory.method(method);
+        if (method == PercentilesMethod.TDIGEST) {
+            return new TDigestPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
+        } else if (method == PercentilesMethod.HDR) {
+            return new HDRPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits,
+                    keyed);
+        } else {
+            throw new AssertionError();
         }
-        if (compression != null) {
-            factory.compression(compression);
-        }
-        if (numberOfSignificantValueDigits != null) {
-            factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-        }
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new PercentileRanksAggregatorBuilder(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorBuilder.java
deleted file mode 100644
index 243028a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorBuilder.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregatorFactory;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder.LeafOnly;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
-
-public class PercentilesAggregatorBuilder extends LeafOnly<ValuesSource.Numeric, PercentilesAggregatorBuilder> {
-
-    private double[] percents = PercentilesParser.DEFAULT_PERCENTS;
-    private PercentilesMethod method = PercentilesMethod.TDIGEST;
-    private int numberOfSignificantValueDigits = 3;
-    private double compression = 100.0;
-    private boolean keyed = false;
-
-    public PercentilesAggregatorBuilder(String name) {
-        super(name, InternalTDigestPercentiles.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-    }
-
-    /**
-     * Set the values to compute percentiles from.
-     */
-    public PercentilesAggregatorBuilder percentiles(double... percents) {
-        double[] sortedPercents = Arrays.copyOf(percents, percents.length);
-        Arrays.sort(sortedPercents);
-        this.percents = sortedPercents;
-        return this;
-    }
-
-    /**
-     * Get the values to compute percentiles from.
-     */
-    public double[] percentiles() {
-        return percents;
-    }
-
-    /**
-     * Set whether the XContent response should be keyed
-     */
-    public PercentilesAggregatorBuilder keyed(boolean keyed) {
-        this.keyed = keyed;
-        return this;
-    }
-
-    /**
-     * Get whether the XContent response should be keyed
-     */
-    public boolean keyed() {
-        return keyed;
-    }
-
-    /**
-     * Expert: set the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public PercentilesAggregatorBuilder numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        return this;
-    }
-
-    /**
-     * Expert: get the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public int numberOfSignificantValueDigits() {
-        return numberOfSignificantValueDigits;
-    }
-
-    /**
-     * Expert: set the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public PercentilesAggregatorBuilder compression(double compression) {
-        this.compression = compression;
-        return this;
-    }
-
-    /**
-     * Expert: get the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public double compression() {
-        return compression;
-    }
-
-    public PercentilesAggregatorBuilder method(PercentilesMethod method) {
-        this.method = method;
-        return this;
-    }
-
-    public PercentilesMethod method() {
-        return method;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentilesAggregatorFactory(name, type, config, percents, compression, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
-        case HDR:
-            return new HDRPercentilesAggregatorFactory(name, type, config, percents, numberOfSignificantValueDigits, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected PercentilesAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        PercentilesAggregatorBuilder factory = new PercentilesAggregatorBuilder(name);
-        factory.percents = in.readDoubleArray();
-        factory.keyed = in.readBoolean();
-        factory.numberOfSignificantValueDigits = in.readVInt();
-        factory.compression = in.readDouble();
-        factory.method = PercentilesMethod.TDIGEST.readFrom(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        out.writeDoubleArray(percents);
-        out.writeBoolean(keyed);
-        out.writeVInt(numberOfSignificantValueDigits);
-        out.writeDouble(compression);
-        method.writeTo(out);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percents);
-        builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-        builder.startObject(method.getName());
-        if (method == PercentilesMethod.TDIGEST) {
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-        } else {
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        PercentilesAggregatorBuilder other = (PercentilesAggregatorBuilder) obj;
-        if (!Objects.equals(method, other.method)) {
-            return false;
-        }
-        boolean equalSettings = false;
-        switch (method) {
-        case HDR:
-            equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-            break;
-        case TDIGEST:
-            equalSettings = Objects.equals(compression, other.compression);
-            break;
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-        return equalSettings
-                && Objects.deepEquals(percents, other.percents)
-                && Objects.equals(keyed, other.keyed)
-                && Objects.equals(method, other.method);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        switch (method) {
-        case HDR:
-            return Objects.hash(Arrays.hashCode(percents), keyed, numberOfSignificantValueDigits, method);
-        case TDIGEST:
-            return Objects.hash(Arrays.hashCode(percents), keyed, compression, method);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java
new file mode 100644
index 0000000..399f9ea
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+
+/**
+ * Builder for the {@link Percentiles} aggregation.
+ */
+public class PercentilesBuilder extends AbstractPercentilesBuilder<PercentilesBuilder> {
+
+    double[] percentiles;
+    /**
+     * Sole constructor.
+     */
+    public PercentilesBuilder(String name) {
+        super(name, Percentiles.TYPE_NAME);
+    }
+
+    /**
+     * Set the percentiles to compute.
+     */
+    public PercentilesBuilder percentiles(double... percentiles) {
+        for (int i = 0; i < percentiles.length; i++) {
+            if (percentiles[i] < 0 || percentiles[i] > 100) {
+                throw new IllegalArgumentException("the percents in the percentiles aggregation [" +
+                        getName() + "] must be in the [0, 100] range");
+            }
+        }
+        this.percentiles = percentiles;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (percentiles != null) {
+            builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percentiles);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
index 44316b6..c593b1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
@@ -19,16 +19,11 @@
 
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
 
 /**
  * An enum representing the methods for calculating percentiles
  */
-public enum PercentilesMethod implements Writeable<PercentilesMethod> {
+public enum PercentilesMethod {
     /**
      * The TDigest method for calculating percentiles
      */
@@ -51,20 +46,6 @@ public enum PercentilesMethod implements Writeable<PercentilesMethod> {
         return name;
     }
 
-    @Override
-    public PercentilesMethod readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown PercentilesMethod ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-
     /**
      * Returns the {@link PercentilesMethod} for this method name. returns
      * <code>null</code> if no {@link PercentilesMethod} exists for the name.
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
index 7299ccb..6fbb2cc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
@@ -19,10 +19,13 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -35,7 +38,7 @@ public class PercentilesParser extends AbstractPercentilesParser {
         super(true);
     }
 
-    public final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
+    private final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
 
     @Override
     public String type() {
@@ -48,30 +51,18 @@ public class PercentilesParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorBuilder<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
-        PercentilesAggregatorBuilder factory = new PercentilesAggregatorBuilder(aggregationName);
-        if (keys != null) {
-            factory.percentiles(keys);
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            keys = DEFAULT_PERCENTS;
         }
-        if (method != null) {
-            factory.method(method);
+        if (method == PercentilesMethod.TDIGEST) {
+            return new TDigestPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
+        } else if (method == PercentilesMethod.HDR) {
+            return new HDRPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits, keyed);
+        } else {
+            throw new AssertionError();
         }
-        if (compression != null) {
-            factory.compression(compression);
-        }
-        if (numberOfSignificantValueDigits != null) {
-            factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-        }
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new PercentilesAggregatorBuilder(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
index 603871e..d132fdb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
@@ -23,7 +23,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -70,4 +73,34 @@ public class HDRPercentileRanksAggregator extends AbstractHDRPercentilesAggregat
             return InternalHDRPercentileRanks.percentileRank(state, Double.valueOf(name));
         }
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] values;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] values,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalHDRPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
+            this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new HDRPercentileRanksAggregator(name, null, aggregationContext, parent, values, numberOfSignificantValueDigits, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new HDRPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, numberOfSignificantValueDigits,
+                    keyed, config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregatorFactory.java
deleted file mode 100644
index 5ee9a41..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregatorFactory.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class HDRPercentileRanksAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, HDRPercentileRanksAggregatorFactory> {
-
-    private final double[] values;
-    private final int numberOfSignificantValueDigits;
-    private final boolean keyed;
-
-    public HDRPercentileRanksAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] values,
-            int numberOfSignificantValueDigits, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.values = values;
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new HDRPercentileRanksAggregator(name, null, context, parent, values, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new HDRPercentileRanksAggregator(name, valuesSource, context, parent, values, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
index f51769d..d1c0a62 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
@@ -21,9 +21,13 @@ package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
 import org.HdrHistogram.DoubleHistogram;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -71,4 +75,34 @@ public class HDRPercentilesAggregator extends AbstractHDRPercentilesAggregator {
                 keyed,
                 formatter, pipelineAggregators(), metaData());
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] percents;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] percents,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
+            this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new HDRPercentilesAggregator(name, null, aggregationContext, parent, percents, numberOfSignificantValueDigits, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new HDRPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, numberOfSignificantValueDigits,
+                    keyed, config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregatorFactory.java
deleted file mode 100644
index 8d24536..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregatorFactory.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class HDRPercentilesAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, HDRPercentilesAggregatorFactory> {
-
-    private final double[] percents;
-    private final int numberOfSignificantValueDigits;
-    private final boolean keyed;
-
-    public HDRPercentilesAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] percents,
-            int numberOfSignificantValueDigits, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.percents = percents;
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new HDRPercentilesAggregator(name, null, context, parent, percents, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new HDRPercentilesAggregator(name, valuesSource, context, parent, percents, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
index 3db68ca..95c9f06 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
@@ -22,7 +22,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -65,4 +68,34 @@ public class TDigestPercentileRanksAggregator extends AbstractTDigestPercentiles
             return InternalTDigestPercentileRanks.percentileRank(state, Double.valueOf(name));
         }
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] values;
+        private final double compression;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] values, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
+            this.compression = compression;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TDigestPercentileRanksAggregator(name, null, aggregationContext, parent, values, compression, keyed, config.formatter(),
+                    pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new TDigestPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, compression, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregatorFactory.java
deleted file mode 100644
index f5bf60f..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregatorFactory.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class TDigestPercentileRanksAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, TDigestPercentileRanksAggregatorFactory> {
-
-    private final double[] percents;
-    private final double compression;
-    private final boolean keyed;
-
-    public TDigestPercentileRanksAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] percents,
-            double compression, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.percents = percents;
-        this.compression = compression;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new TDigestPercentileRanksAggregator(name, null, context, parent, percents, compression, keyed, config.formatter(),
-                pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new TDigestPercentileRanksAggregator(name, valuesSource, context, parent, percents, compression, keyed, config.formatter(),
-                pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
index 3bf7cff..43800bf 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
@@ -22,7 +22,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -65,4 +68,34 @@ public class TDigestPercentilesAggregator extends AbstractTDigestPercentilesAggr
     public InternalAggregation buildEmptyAggregation() {
         return new InternalTDigestPercentiles(name, keys, new TDigestState(compression), keyed, formatter, pipelineAggregators(), metaData());
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] percents;
+        private final double compression;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] percents, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
+            this.compression = compression;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TDigestPercentilesAggregator(name, null, aggregationContext, parent, percents, compression, keyed, config.formatter(),
+                    pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new TDigestPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregatorFactory.java
deleted file mode 100644
index 5cd3649..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregatorFactory.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class TDigestPercentilesAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, TDigestPercentilesAggregatorFactory> {
-
-    private final double[] percents;
-    private final double compression;
-    private final boolean keyed;
-
-    public TDigestPercentilesAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] percents,
-            double compression, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.percents = percents;
-        this.compression = compression;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new TDigestPercentilesAggregator(name, null, context, parent, percents, compression, keyed, config.formatter(),
-                pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new TDigestPercentilesAggregator(name, valuesSource, context, parent, percents, compression, keyed, config.formatter(),
-                pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
index d2d7ccb..68d886a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
@@ -20,30 +20,30 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
+
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
+import java.util.Map.Entry;
 
 public class ScriptedMetricAggregator extends MetricsAggregator {
 
@@ -106,7 +106,7 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         return new InternalScriptedMetric(name, null, reduceScript, pipelineAggregators(), metaData());
     }
 
-    public static class ScriptedMetricAggregatorBuilder extends AggregatorBuilder<ScriptedMetricAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
         private Script initScript;
         private Script mapScript;
@@ -114,159 +114,79 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         private Script reduceScript;
         private Map<String, Object> params;
 
-        public ScriptedMetricAggregatorBuilder(String name) {
-            super(name, InternalScriptedMetric.TYPE);
-        }
-
-        /**
-         * Set the <tt>init</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder initScript(Script initScript) {
+        public Factory(String name, Script initScript, Script mapScript, Script combineScript, Script reduceScript,
+                Map<String, Object> params) {
+            super(name, InternalScriptedMetric.TYPE.name());
             this.initScript = initScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>init</tt> script.
-         */
-        public Script initScript() {
-            return initScript;
-        }
-
-        /**
-         * Set the <tt>map</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder mapScript(Script mapScript) {
             this.mapScript = mapScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>map</tt> script.
-         */
-        public Script mapScript() {
-            return mapScript;
-        }
-
-        /**
-         * Set the <tt>combine</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder combineScript(Script combineScript) {
             this.combineScript = combineScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>combine</tt> script.
-         */
-        public Script combineScript() {
-            return combineScript;
-        }
-
-        /**
-         * Set the <tt>reduce</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder reduceScript(Script reduceScript) {
             this.reduceScript = reduceScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>reduce</tt> script.
-         */
-        public Script reduceScript() {
-            return reduceScript;
-        }
-
-        /**
-         * Set parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public ScriptedMetricAggregatorBuilder params(Map<String, Object> params) {
             this.params = params;
-            return this;
-        }
-
-        /**
-         * Get parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public Map<String, Object> params() {
-            return params;
         }
 
         @Override
-        protected ScriptedMetricAggregatorFactory doBuild(AggregationContext context, AggregatorFactory<?> parent,
-                Builder subfactoriesBuilder) throws IOException {
-            return new ScriptedMetricAggregatorFactory(name, type, initScript, mapScript, combineScript, reduceScript, params, context,
-                    parent, subfactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject();
-            if (initScript != null) {
-                builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, context, parent);
             }
-
-            if (mapScript != null) {
-                builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
+            Map<String, Object> params = this.params;
+            if (params != null) {
+                params = deepCopyParams(params, context.searchContext());
+            } else {
+                params = new HashMap<>();
+                params.put("_agg", new HashMap<String, Object>());
             }
-
-            if (combineScript != null) {
-                builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
+            return new ScriptedMetricAggregator(name, insertParams(initScript, params), insertParams(mapScript, params), insertParams(
+                    combineScript, params), deepCopyScript(reduceScript, context.searchContext()), params, context, parent, pipelineAggregators,
+                    metaData);
             }
 
-            if (reduceScript != null) {
-                builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
+        private static Script insertParams(Script script, Map<String, Object> params) {
+            if (script == null) {
+                return null;
             }
-            if (params != null) {
-                builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
-                builder.map(params);
+            return new Script(script.getScript(), script.getType(), script.getLang(), params);
+        }
+
+        private static Script deepCopyScript(Script script, SearchContext context) {
+            if (script != null) {
+                Map<String, Object> params = script.getParams();
+                if (params != null) {
+                    params = deepCopyParams(params, context);
+                }
+                return new Script(script.getScript(), script.getType(), script.getLang(), params);
+            } else {
+                return null;
             }
-            builder.endObject();
-            return builder;
         }
 
-        @Override
-        protected ScriptedMetricAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            ScriptedMetricAggregatorBuilder factory = new ScriptedMetricAggregatorBuilder(name);
-            factory.initScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.mapScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.combineScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.reduceScript = in.readOptionalStreamable(Script.SUPPLIER);
-            if (in.readBoolean()) {
-                factory.params = in.readMap();
+        @SuppressWarnings({ "unchecked" })
+        private static <T> T deepCopyParams(T original, SearchContext context) {
+            T clone;
+            if (original instanceof Map) {
+                Map<?, ?> originalMap = (Map<?, ?>) original;
+                Map<Object, Object> clonedMap = new HashMap<>();
+                for (Entry<?, ?> e : originalMap.entrySet()) {
+                    clonedMap.put(deepCopyParams(e.getKey(), context), deepCopyParams(e.getValue(), context));
+                }
+                clone = (T) clonedMap;
+            } else if (original instanceof List) {
+                List<?> originalList = (List<?>) original;
+                List<Object> clonedList = new ArrayList<Object>();
+                for (Object o : originalList) {
+                    clonedList.add(deepCopyParams(o, context));
+                }
+                clone = (T) clonedList;
+            } else if (original instanceof String || original instanceof Integer || original instanceof Long || original instanceof Short
+                    || original instanceof Byte || original instanceof Float || original instanceof Double || original instanceof Character
+                    || original instanceof Boolean) {
+                clone = original;
+            } else {
+                throw new SearchParseException(context, "Can only clone primitives, String, ArrayList, and HashMap. Found: "
+                        + original.getClass().getCanonicalName(), null);
             }
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalStreamable(initScript);
-            out.writeOptionalStreamable(mapScript);
-            out.writeOptionalStreamable(combineScript);
-            out.writeOptionalStreamable(reduceScript);
-            boolean hasParams = params != null;
-            out.writeBoolean(hasParams);
-            if (hasParams) {
-                out.writeMap(params);
-            }
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(initScript, mapScript, combineScript, reduceScript, params);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            ScriptedMetricAggregatorBuilder other = (ScriptedMetricAggregatorBuilder) obj;
-            return Objects.equals(initScript, other.initScript)
-                    && Objects.equals(mapScript, other.mapScript)
-                    && Objects.equals(combineScript, other.combineScript)
-                    && Objects.equals(reduceScript, other.reduceScript)
-                    && Objects.equals(params, other.params);
+            return clone;
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregatorFactory.java
deleted file mode 100644
index f89e99f..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregatorFactory.java
+++ /dev/null
@@ -1,123 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.scripted;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.SearchParseException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.internal.SearchContext;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-public class ScriptedMetricAggregatorFactory extends AggregatorFactory<ScriptedMetricAggregatorFactory> {
-
-    private final Script initScript;
-    private final Script mapScript;
-    private final Script combineScript;
-    private final Script reduceScript;
-    private final Map<String, Object> params;
-
-    public ScriptedMetricAggregatorFactory(String name, Type type, Script initScript, Script mapScript, Script combineScript,
-            Script reduceScript, Map<String, Object> params, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.initScript = initScript;
-        this.mapScript = mapScript;
-        this.combineScript = combineScript;
-        this.reduceScript = reduceScript;
-        this.params = params;
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        Map<String, Object> params = this.params;
-        if (params != null) {
-            params = deepCopyParams(params, context.searchContext());
-        } else {
-            params = new HashMap<>();
-            params.put("_agg", new HashMap<String, Object>());
-        }
-        return new ScriptedMetricAggregator(name, insertParams(initScript, params), insertParams(mapScript, params),
-                insertParams(combineScript, params), deepCopyScript(reduceScript, context.searchContext()), params, context, parent,
-                pipelineAggregators, metaData);
-    }
-
-    private static Script insertParams(Script script, Map<String, Object> params) {
-        if (script == null) {
-            return null;
-        }
-        return new Script(script.getScript(), script.getType(), script.getLang(), params);
-    }
-
-    private static Script deepCopyScript(Script script, SearchContext context) {
-        if (script != null) {
-            Map<String, Object> params = script.getParams();
-            if (params != null) {
-                params = deepCopyParams(params, context);
-            }
-            return new Script(script.getScript(), script.getType(), script.getLang(), params);
-        } else {
-            return null;
-        }
-    }
-
-    @SuppressWarnings({ "unchecked" })
-    private static <T> T deepCopyParams(T original, SearchContext context) {
-        T clone;
-        if (original instanceof Map) {
-            Map<?, ?> originalMap = (Map<?, ?>) original;
-            Map<Object, Object> clonedMap = new HashMap<>();
-            for (Entry<?, ?> e : originalMap.entrySet()) {
-                clonedMap.put(deepCopyParams(e.getKey(), context), deepCopyParams(e.getValue(), context));
-            }
-            clone = (T) clonedMap;
-        } else if (original instanceof List) {
-            List<?> originalList = (List<?>) original;
-            List<Object> clonedList = new ArrayList<Object>();
-            for (Object o : originalList) {
-                clonedList.add(deepCopyParams(o, context));
-            }
-            clone = (T) clonedList;
-        } else if (original instanceof String || original instanceof Integer || original instanceof Long || original instanceof Short
-                || original instanceof Byte || original instanceof Float || original instanceof Double || original instanceof Character
-                || original instanceof Boolean) {
-            clone = original;
-        } else {
-            throw new SearchParseException(context,
-                    "Can only clone primitives, String, ArrayList, and HashMap. Found: " + original.getClass().getCanonicalName(), null);
-        }
-        return clone;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
new file mode 100644
index 0000000..803123f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.scripted;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Builder for the {@link ScriptedMetric} aggregation.
+ */
+public class ScriptedMetricBuilder extends MetricsAggregationBuilder<ScriptedMetricBuilder> {
+
+    private Script initScript = null;
+    private Script mapScript = null;
+    private Script combineScript = null;
+    private Script reduceScript = null;
+    private Map<String, Object> params = null;
+
+    /**
+     * Sole constructor.
+     */
+    public ScriptedMetricBuilder(String name) {
+        super(name, InternalScriptedMetric.TYPE.name());
+    }
+
+    /**
+     * Set the <tt>init</tt> script.
+     */
+    public ScriptedMetricBuilder initScript(Script initScript) {
+        this.initScript = initScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>map</tt> script.
+     */
+    public ScriptedMetricBuilder mapScript(Script mapScript) {
+        this.mapScript = mapScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>combine</tt> script.
+     */
+    public ScriptedMetricBuilder combineScript(Script combineScript) {
+        this.combineScript = combineScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>reduce</tt> script.
+     */
+    public ScriptedMetricBuilder reduceScript(Script reduceScript) {
+        this.reduceScript = reduceScript;
+        return this;
+    }
+
+    /**
+     * Set parameters that will be available in the <tt>init</tt>, <tt>map</tt>
+     * and <tt>combine</tt> phases.
+     */
+    public ScriptedMetricBuilder params(Map<String, Object> params) {
+        this.params = params;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+
+        if (initScript != null) {
+            builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
+        }
+
+        if (mapScript != null) {
+            builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
+        }
+
+        if (combineScript != null) {
+            builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
+        }
+
+        if (reduceScript != null) {
+            builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
+        }
+        if (params != null) {
+            builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
+            builder.map(params);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
index c2fffde..528078c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
@@ -20,14 +20,14 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashSet;
@@ -35,7 +35,7 @@ import java.util.Map;
 import java.util.Set;
 
 public class ScriptedMetricParser implements Aggregator.Parser {
-
+    
     public static final String INIT_SCRIPT = "init_script";
     public static final String MAP_SCRIPT = "map_script";
     public static final String COMBINE_SCRIPT = "combine_script";
@@ -54,7 +54,7 @@ public class ScriptedMetricParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         Script initScript = null;
         Script mapScript = null;
         Script combineScript = null;
@@ -87,16 +87,17 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, REDUCE_PARAMS_FIELD)) {
                   reduceParams = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token.isValue()) {
                 if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
@@ -106,8 +107,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 initScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (initScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object");
+            throw new SearchParseException(
+                    context,
+                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object",
+                    parser.getTokenLocation());
         }
 
         if (mapScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -116,8 +119,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 mapScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (mapScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object");
+            throw new SearchParseException(
+                    context,
+                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object",
+                    parser.getTokenLocation());
         }
 
         if (combineScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -126,8 +131,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 combineScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (combineScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object");
+            throw new SearchParseException(
+                    context,
+                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object",
+                    parser.getTokenLocation());
         }
 
         if (reduceScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -136,23 +143,11 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 reduceScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), reduceParams);
             }
         }
-
+        
         if (mapScript == null) {
-            throw new ParsingException(parser.getTokenLocation(), "map_script field is required in [" + aggregationName + "].");
+            throw new SearchParseException(context, "map_script field is required in [" + aggregationName + "].", parser.getTokenLocation());
         }
-
-        ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder factory = new ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder(aggregationName);
-        factory.initScript(initScript);
-        factory.mapScript(mapScript);
-        factory.combineScript(combineScript);
-        factory.reduceScript(reduceScript);
-        factory.params(params);
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder(null);
+        return new ScriptedMetricAggregator.Factory(aggregationName, initScript, mapScript, combineScript, reduceScript, params);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
index bdd513e..6e648cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
@@ -19,29 +19,21 @@
 package org.elasticsearch.search.aggregations.metrics.stats;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -163,42 +155,23 @@ public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
         return new InternalStats(name, 0, 0, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class StatsAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, StatsAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public StatsAggregatorBuilder(String name) {
-            super(name, InternalStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalStats.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected StatsAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new StatsAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new StatsAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected StatsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new StatsAggregator.StatsAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new StatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregatorFactory.java
deleted file mode 100644
index 4564cda..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.stats;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class StatsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, StatsAggregatorFactory> {
-
-    public StatsAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new StatsAggregator(name, null, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new StatsAggregator(name, valuesSource, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
new file mode 100644
index 0000000..3b6e549
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Stats} aggregation.
+ */
+public class StatsBuilder extends ValuesSourceMetricsAggregationBuilder<StatsBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public StatsBuilder(String name) {
+        super(name, InternalStats.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
index e8490dc..86c85e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.stats;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class StatsParser extends NumericValuesSourceParser {
+public class StatsParser extends NumericValuesSourceMetricsAggregatorParser<InternalStats> {
 
     public StatsParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalStats.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected StatsAggregator.StatsAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new StatsAggregator.StatsAggregatorBuilder(aggregationName);
+        super(InternalStats.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new StatsAggregator.StatsAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new StatsAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
index fab1cd5..86a6481 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
@@ -19,44 +19,32 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue {
 
-    public static final ParseField SIGMA_FIELD = new ParseField("sigma");
-
     final ValuesSource.Numeric valuesSource;
     final ValueFormatter formatter;
     final double sigma;
@@ -200,57 +188,29 @@ public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue
         Releasables.close(counts, maxes, mins, sumOfSqrs, sums);
     }
 
-    public static class ExtendedStatsAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, ExtendedStatsAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double sigma = 2.0;
+        private final double sigma;
 
-        public ExtendedStatsAggregatorBuilder(String name) {
-            super(name, InternalExtendedStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double sigma) {
+            super(name, InternalExtendedStats.TYPE.name(), valuesSourceConfig);
 
-        public ExtendedStatsAggregatorBuilder sigma(double sigma) {
             this.sigma = sigma;
-            return this;
-        }
-
-        public double sigma() {
-            return sigma;
-        }
-
-        @Override
-        protected ExtendedStatsAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new ExtendedStatsAggregatorFactory(name, type, config, sigma, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected ExtendedStatsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder(name);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SIGMA_FIELD.getPreferredName(), sigma);
-            return builder;
         }
 
         @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new ExtendedStatsAggregator(name, null, config.formatter(), aggregationContext, parent, sigma, pipelineAggregators,
+                    metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            ExtendedStatsAggregatorBuilder other = (ExtendedStatsAggregatorBuilder) obj;
-            return Objects.equals(sigma, other.sigma);
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new ExtendedStatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, sigma,
+                    pipelineAggregators, metaData);
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregatorFactory.java
deleted file mode 100644
index 22f49e3..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregatorFactory.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.stats.extended;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ExtendedStatsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, ExtendedStatsAggregatorFactory> {
-
-    private final double sigma;
-
-    public ExtendedStatsAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double sigma,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.sigma = sigma;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new ExtendedStatsAggregator(name, null, config.formatter(), context, parent, sigma, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new ExtendedStatsAggregator(name, valuesSource, config.formatter(), context, parent, sigma, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
new file mode 100644
index 0000000..28f4d73
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ExtendedStats} aggregation.
+ */
+public class ExtendedStatsBuilder extends ValuesSourceMetricsAggregationBuilder<ExtendedStatsBuilder> {
+
+    private Double sigma;
+
+    /**
+     * Sole constructor.
+     */
+    public ExtendedStatsBuilder(String name) {
+        super(name, InternalExtendedStats.TYPE.name());
+    }
+
+    public ExtendedStatsBuilder sigma(double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        if (sigma != null) {
+            builder.field("sigma", sigma);
+        }
+
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
index fb8caae..b29ce08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
@@ -19,55 +19,65 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ExtendedStatsParser extends NumericValuesSourceParser {
+public class ExtendedStatsParser  implements Aggregator.Parser {
 
-    public ExtendedStatsParser() {
-        super(true, true, false);
-    }
+    static final ParseField SIGMA = new ParseField("sigma");
 
     @Override
     public String type() {
         return InternalExtendedStats.TYPE.name();
     }
 
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config, double sigma) {
+        return new ExtendedStatsAggregator.Factory(aggregationName, config, sigma);
+    }
+
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, ExtendedStatsAggregator.SIGMA_FIELD)) {
-            if (token.isValue()) {
-                otherOptions.put(ExtendedStatsAggregator.SIGMA_FIELD, parser.doubleValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalExtendedStats.TYPE, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        double sigma = 2.0;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, SIGMA)) {
+                    sigma = parser.doubleValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder(aggregationName);
-        Double sigma = (Double) otherOptions.get(ExtendedStatsAggregator.SIGMA_FIELD);
-        if (sigma != null) {
-            factory.sigma(sigma);
+        if (sigma < 0) {
+            throw new SearchParseException(context, "[sigma] must not be negative. Value provided was" + sigma, parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder(null);
+        return createFactory(aggregationName, vsParser.config(), sigma);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
index 7c1548d..8a6b40c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
@@ -19,28 +19,20 @@
 package org.elasticsearch.search.aggregations.metrics.sum;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -113,42 +105,23 @@ public class SumAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalSum(name, 0.0, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class SumAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, SumAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public SumAggregatorBuilder(String name) {
-            super(name, InternalSum.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalSum.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected SumAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new SumAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new SumAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected ValuesSourceAggregatorBuilder<Numeric, SumAggregatorBuilder> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new SumAggregator.SumAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new SumAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregatorFactory.java
deleted file mode 100644
index 7570c76..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.sum;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class SumAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, SumAggregatorFactory> {
-
-    public SumAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new SumAggregator(name, null, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new SumAggregator(name, valuesSource, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
new file mode 100644
index 0000000..53e8780
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.sum;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Sum} aggregation.
+ */
+public class SumBuilder extends ValuesSourceMetricsAggregationBuilder<SumBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public SumBuilder(String name) {
+        super(name, InternalSum.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
index dd34b3a..b43285a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.sum;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class SumParser extends NumericValuesSourceParser {
+public class SumParser extends NumericValuesSourceMetricsAggregatorParser<InternalSum> {
 
     public SumParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalSum.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected SumAggregator.SumAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new SumAggregator.SumAggregatorBuilder(aggregationName);
+        super(InternalSum.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SumAggregator.SumAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new SumAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
index a55f28e..82dea48 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
@@ -30,48 +30,28 @@ import org.apache.lucene.search.TopDocsCollector;
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.search.TopFieldDocs;
 import org.apache.lucene.search.TopScoreDocCollector;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHits;
 import org.elasticsearch.search.internal.SubSearchContext;
-import org.elasticsearch.search.sort.SortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
+
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  */
@@ -201,542 +181,26 @@ public class TopHitsAggregator extends MetricsAggregator {
         Releasables.close(topDocsCollectors);
     }
 
-    public static class TopHitsAggregatorBuilder extends AggregatorBuilder<TopHitsAggregatorBuilder> {
-
-        private int from = 0;
-        private int size = 3;
-        private boolean explain = false;
-        private boolean version = false;
-        private boolean trackScores = false;
-        private List<BytesReference> sorts = null;
-        private HighlightBuilder highlightBuilder;
-        private List<String> fieldNames;
-        private List<String> fieldDataFields;
-        private List<ScriptField> scriptFields;
-        private FetchSourceContext fetchSourceContext;
-
-        public TopHitsAggregatorBuilder(String name) {
-            super(name, InternalTopHits.TYPE);
-        }
-
-        /**
-         * From index to start the search from. Defaults to <tt>0</tt>.
-         */
-        public TopHitsAggregatorBuilder from(int from) {
-            this.from = from;
-            return this;
-        }
-
-        /**
-         * Gets the from index to start the search from.
-         **/
-        public int from() {
-            return from;
-        }
-
-        /**
-         * The number of search hits to return. Defaults to <tt>10</tt>.
-         */
-        public TopHitsAggregatorBuilder size(int size) {
-            this.size = size;
-            return this;
-        }
-
-        /**
-         * Gets the number of search hits to return.
-         */
-        public int size() {
-            return size;
-        }
-
-        /**
-         * Adds a sort against the given field name and the sort ordering.
-         *
-         * @param name
-         *            The name of the field
-         * @param order
-         *            The sort ordering
-         */
-        public TopHitsAggregatorBuilder sort(String name, SortOrder order) {
-            sort(SortBuilders.fieldSort(name).order(order));
-            return this;
-        }
-
-        /**
-         * Add a sort against the given field name.
-         *
-         * @param name
-         *            The name of the field to sort by
-         */
-        public TopHitsAggregatorBuilder sort(String name) {
-            sort(SortBuilders.fieldSort(name));
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public TopHitsAggregatorBuilder sort(SortBuilder sort) {
-            try {
-                if (sorts == null) {
-                    sorts = new ArrayList<>();
-                }
-                // NORELEASE when sort has been refactored and made writeable
-                // add the sortBuilcer to the List directly instead of
-                // serialising to XContent
-                XContentBuilder builder = XContentFactory.jsonBuilder();
-                builder.startObject();
-                sort.toXContent(builder, EMPTY_PARAMS);
-                builder.endObject();
-                sorts.add(builder.bytes());
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public TopHitsAggregatorBuilder sorts(List<BytesReference> sorts) {
-            if (this.sorts == null) {
-                this.sorts = new ArrayList<>();
-            }
-            for (BytesReference sort : sorts) {
-                this.sorts.add(sort);
-            }
-            return this;
-        }
-
-        /**
-         * Gets the bytes representing the sort builders for this request.
-         */
-        public List<BytesReference> sorts() {
-            return sorts;
-        }
-
-        /**
-         * Adds highlight to perform as part of the search.
-         */
-        public TopHitsAggregatorBuilder highlighter(HighlightBuilder highlightBuilder) {
-            this.highlightBuilder = highlightBuilder;
-            return this;
-        }
-
-        /**
-         * Gets the hightlighter builder for this request.
-         */
-        public HighlightBuilder highlighter() {
-            return highlightBuilder;
-        }
-
-        /**
-         * Indicates whether the response should contain the stored _source for
-         * every hit
-         */
-        public TopHitsAggregatorBuilder fetchSource(boolean fetch) {
-            if (this.fetchSourceContext == null) {
-                this.fetchSourceContext = new FetchSourceContext(fetch);
-            } else {
-                this.fetchSourceContext.fetchSource(fetch);
-            }
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param include
-         *            An optional include (optionally wildcarded) pattern to
-         *            filter the returned _source
-         * @param exclude
-         *            An optional exclude (optionally wildcarded) pattern to
-         *            filter the returned _source
-         */
-        public TopHitsAggregatorBuilder fetchSource(@Nullable String include, @Nullable String exclude) {
-            fetchSource(include == null ? Strings.EMPTY_ARRAY : new String[] { include },
-                    exclude == null ? Strings.EMPTY_ARRAY : new String[] { exclude });
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param includes
-         *            An optional list of include (optionally wildcarded)
-         *            pattern to filter the returned _source
-         * @param excludes
-         *            An optional list of exclude (optionally wildcarded)
-         *            pattern to filter the returned _source
-         */
-        public TopHitsAggregatorBuilder fetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
-            fetchSourceContext = new FetchSourceContext(includes, excludes);
-            return this;
-        }
-
-        /**
-         * Indicate how the _source should be fetched.
-         */
-        public TopHitsAggregatorBuilder fetchSource(@Nullable FetchSourceContext fetchSourceContext) {
-            this.fetchSourceContext = fetchSourceContext;
-            return this;
-        }
+    public static class Factory extends AggregatorFactory {
 
-        /**
-         * Gets the {@link FetchSourceContext} which defines how the _source
-         * should be fetched.
-         */
-        public FetchSourceContext fetchSource() {
-            return fetchSourceContext;
-        }
-
-        /**
-         * Adds a field to load and return (note, it must be stored) as part of
-         * the search request. If none are specified, the source of the document
-         * will be return.
-         */
-        public TopHitsAggregatorBuilder field(String name) {
-            if (fieldNames == null) {
-                fieldNames = new ArrayList<>();
-            }
-            fieldNames.add(name);
-            return this;
-        }
-
-        /**
-         * Sets the fields to load and return as part of the search request. If
-         * none are specified, the source of the document will be returned.
-         */
-        public TopHitsAggregatorBuilder fields(List<String> fields) {
-            this.fieldNames = fields;
-            return this;
-        }
-
-        /**
-         * Sets no fields to be loaded, resulting in only id and type to be
-         * returned per field.
-         */
-        public TopHitsAggregatorBuilder noFields() {
-            this.fieldNames = Collections.emptyList();
-            return this;
-        }
+        private final FetchPhase fetchPhase;
+        private final SubSearchContext subSearchContext;
 
-        /**
-         * Gets the fields to load and return as part of the search request.
-         */
-        public List<String> fields() {
-            return fieldNames;
-        }
-
-        /**
-         * Adds a field to load from the field data cache and return as part of
-         * the search request.
-         */
-        public TopHitsAggregatorBuilder fieldDataField(String name) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.add(name);
-            return this;
-        }
-
-        /**
-         * Adds fields to load from the field data cache and return as part of
-         * the search request.
-         */
-        public TopHitsAggregatorBuilder fieldDataFields(List<String> names) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.addAll(names);
-            return this;
-        }
-
-        /**
-         * Gets the field-data fields.
-         */
-        public List<String> fieldDataFields() {
-            return fieldDataFields;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public TopHitsAggregatorBuilder scriptField(String name, Script script) {
-            scriptField(name, script, false);
-            return this;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public TopHitsAggregatorBuilder scriptField(String name, Script script, boolean ignoreFailure) {
-            if (scriptFields == null) {
-                scriptFields = new ArrayList<>();
-            }
-            scriptFields.add(new ScriptField(name, script, ignoreFailure));
-            return this;
-        }
-
-        public TopHitsAggregatorBuilder scriptFields(List<ScriptField> scriptFields) {
-            if (this.scriptFields == null) {
-                this.scriptFields = new ArrayList<>();
-            }
-            this.scriptFields.addAll(scriptFields);
-            return this;
-        }
-
-        /**
-         * Gets the script fields.
-         */
-        public List<ScriptField> scriptFields() {
-            return scriptFields;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with an explanation of the hit (ranking).
-         */
-        public TopHitsAggregatorBuilder explain(boolean explain) {
-            this.explain = explain;
-            return this;
-        }
-
-        /**
-         * Indicates whether each search hit will be returned with an
-         * explanation of the hit (ranking)
-         */
-        public boolean explain() {
-            return explain;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with a version associated with it.
-         */
-        public TopHitsAggregatorBuilder version(boolean version) {
-            this.version = version;
-            return this;
-        }
-
-        /**
-         * Indicates whether the document's version will be included in the
-         * search hits.
-         */
-        public boolean version() {
-            return version;
-        }
-
-        /**
-         * Applies when sorting, and controls if scores will be tracked as well.
-         * Defaults to <tt>false</tt>.
-         */
-        public TopHitsAggregatorBuilder trackScores(boolean trackScores) {
-            this.trackScores = trackScores;
-            return this;
-        }
-
-        /**
-         * Indicates whether scores will be tracked for this request.
-         */
-        public boolean trackScores() {
-            return trackScores;
+        public Factory(String name, FetchPhase fetchPhase, SubSearchContext subSearchContext) {
+            super(name, InternalTopHits.TYPE.name());
+            this.fetchPhase = fetchPhase;
+            this.subSearchContext = subSearchContext;
         }
 
         @Override
-        public TopHitsAggregatorBuilder subAggregations(Builder subFactories) {
-            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
+        public Aggregator createInternal(AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TopHitsAggregator(fetchPhase, subSearchContext, name, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected TopHitsAggregatorFactory doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subfactoriesBuilder)
-                throws IOException {
-            return new TopHitsAggregatorFactory(name, type, from, size, explain, version, trackScores, sorts, highlightBuilder, fieldNames,
-                    fieldDataFields, scriptFields, fetchSourceContext, context, parent, subfactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SearchSourceBuilder.FROM_FIELD.getPreferredName(), from);
-            builder.field(SearchSourceBuilder.SIZE_FIELD.getPreferredName(), size);
-            builder.field(SearchSourceBuilder.VERSION_FIELD.getPreferredName(), version);
-            builder.field(SearchSourceBuilder.EXPLAIN_FIELD.getPreferredName(), explain);
-            if (fetchSourceContext != null) {
-                builder.field(SearchSourceBuilder._SOURCE_FIELD.getPreferredName(), fetchSourceContext);
-            }
-            if (fieldNames != null) {
-                if (fieldNames.size() == 1) {
-                    builder.field(SearchSourceBuilder.FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
-                } else {
-                    builder.startArray(SearchSourceBuilder.FIELDS_FIELD.getPreferredName());
-                    for (String fieldName : fieldNames) {
-                        builder.value(fieldName);
-                    }
-                    builder.endArray();
-                }
-            }
-            if (fieldDataFields != null) {
-                builder.startArray(SearchSourceBuilder.FIELDDATA_FIELDS_FIELD.getPreferredName());
-                for (String fieldDataField : fieldDataFields) {
-                    builder.value(fieldDataField);
-                }
-                builder.endArray();
-            }
-            if (scriptFields != null) {
-                builder.startObject(SearchSourceBuilder.SCRIPT_FIELDS_FIELD.getPreferredName());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.toXContent(builder, params);
-                }
-                builder.endObject();
-            }
-            if (sorts != null) {
-                builder.startArray(SearchSourceBuilder.SORT_FIELD.getPreferredName());
-                for (BytesReference sort : sorts) {
-                    XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                    parser.nextToken();
-                    builder.copyCurrentStructure(parser);
-                }
-                builder.endArray();
-            }
-            if (trackScores) {
-                builder.field(SearchSourceBuilder.TRACK_SCORES_FIELD.getPreferredName(), true);
-            }
-            if (highlightBuilder != null) {
-                this.highlightBuilder.toXContent(builder, params);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected TopHitsAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            TopHitsAggregatorBuilder factory = new TopHitsAggregatorBuilder(name);
-            factory.explain = in.readBoolean();
-            factory.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldDataFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldDataFields.add(in.readString());
-                }
-                factory.fieldDataFields = fieldDataFields;
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldNames = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldNames.add(in.readString());
-                }
-                factory.fieldNames = fieldNames;
-            }
-            factory.from = in.readVInt();
-            if (in.readBoolean()) {
-                factory.highlightBuilder = HighlightBuilder.PROTOTYPE.readFrom(in);
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<ScriptField> scriptFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-                }
-                factory.scriptFields = scriptFields;
-            }
-            factory.size = in.readVInt();
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<BytesReference> sorts = new ArrayList<>();
-                for (int i = 0; i < size; i++) {
-                    sorts.add(in.readBytesReference());
-                }
-                factory.sorts = sorts;
-            }
-            factory.trackScores = in.readBoolean();
-            factory.version = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(explain);
-            FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-            boolean hasFieldDataFields = fieldDataFields != null;
-            out.writeBoolean(hasFieldDataFields);
-            if (hasFieldDataFields) {
-                out.writeVInt(fieldDataFields.size());
-                for (String fieldName : fieldDataFields) {
-                    out.writeString(fieldName);
-                }
-            }
-            boolean hasFieldNames = fieldNames != null;
-            out.writeBoolean(hasFieldNames);
-            if (hasFieldNames) {
-                out.writeVInt(fieldNames.size());
-                for (String fieldName : fieldNames) {
-                    out.writeString(fieldName);
-                }
-            }
-            out.writeVInt(from);
-            boolean hasHighlighter = highlightBuilder != null;
-            out.writeBoolean(hasHighlighter);
-            if (hasHighlighter) {
-                highlightBuilder.writeTo(out);
-            }
-            boolean hasScriptFields = scriptFields != null;
-            out.writeBoolean(hasScriptFields);
-            if (hasScriptFields) {
-                out.writeVInt(scriptFields.size());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.writeTo(out);
-                }
-            }
-            out.writeVInt(size);
-            boolean hasSorts = sorts != null;
-            out.writeBoolean(hasSorts);
-            if (hasSorts) {
-                out.writeVInt(sorts.size());
-                for (BytesReference sort : sorts) {
-                    out.writeBytesReference(sort);
-                }
-            }
-            out.writeBoolean(trackScores);
-            out.writeBoolean(version);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(explain, fetchSourceContext, fieldDataFields, fieldNames, from, highlightBuilder, scriptFields, size, sorts,
-                    trackScores, version);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            TopHitsAggregatorBuilder other = (TopHitsAggregatorBuilder) obj;
-            return Objects.equals(explain, other.explain)
-                    && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                    && Objects.equals(fieldDataFields, other.fieldDataFields)
-                    && Objects.equals(fieldNames, other.fieldNames)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(highlightBuilder, other.highlightBuilder)
-                    && Objects.equals(scriptFields, other.scriptFields)
-                    && Objects.equals(size, other.size)
-                    && Objects.equals(sorts, other.sorts)
-                    && Objects.equals(trackScores, other.trackScores)
-                    && Objects.equals(version, other.version);
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
+            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java
deleted file mode 100644
index c4d2165..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.tophits;
-
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.internal.SubSearchContext;
-import org.elasticsearch.search.sort.SortParseElement;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-public class TopHitsAggregatorFactory extends AggregatorFactory<TopHitsAggregatorFactory> {
-
-    private static final SortParseElement sortParseElement = new SortParseElement();
-    private final int from;
-    private final int size;
-    private final boolean explain;
-    private final boolean version;
-    private final boolean trackScores;
-    private final List<BytesReference> sorts;
-    private final HighlightBuilder highlightBuilder;
-    private final List<String> fieldNames;
-    private final List<String> fieldDataFields;
-    private final List<ScriptField> scriptFields;
-    private final FetchSourceContext fetchSourceContext;
-
-    public TopHitsAggregatorFactory(String name, Type type, int from, int size, boolean explain, boolean version, boolean trackScores,
-            List<BytesReference> sorts, HighlightBuilder highlightBuilder, List<String> fieldNames, List<String> fieldDataFields,
-            List<ScriptField> scriptFields, FetchSourceContext fetchSourceContext, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.from = from;
-        this.size = size;
-        this.explain = explain;
-        this.version = version;
-        this.trackScores = trackScores;
-        this.sorts = sorts;
-        this.highlightBuilder = highlightBuilder;
-        this.fieldNames = fieldNames;
-        this.fieldDataFields = fieldDataFields;
-        this.scriptFields = scriptFields;
-        this.fetchSourceContext = fetchSourceContext;
-    }
-
-    @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        SubSearchContext subSearchContext = new SubSearchContext(context.searchContext());
-        subSearchContext.explain(explain);
-        subSearchContext.version(version);
-        subSearchContext.trackScores(trackScores);
-        subSearchContext.from(from);
-        subSearchContext.size(size);
-        if (sorts != null) {
-            XContentParser completeSortParser = null;
-            try {
-                XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                completeSortBuilder.startObject();
-                completeSortBuilder.startArray("sort");
-                for (BytesReference sort : sorts) {
-                    XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                    parser.nextToken();
-                    completeSortBuilder.copyCurrentStructure(parser);
-                }
-                completeSortBuilder.endArray();
-                completeSortBuilder.endObject();
-                BytesReference completeSortBytes = completeSortBuilder.bytes();
-                completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                sortParseElement.parse(completeSortParser, subSearchContext);
-            } catch (Exception e) {
-                XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                throw new ParsingException(location, "failed to parse sort source in aggregation [" + name + "]", e);
-            }
-        }
-        if (fieldNames != null) {
-            subSearchContext.fieldNames().addAll(fieldNames);
-        }
-        if (fieldDataFields != null) {
-            FieldDataFieldsContext fieldDataFieldsContext = subSearchContext
-                    .getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-            for (String field : fieldDataFields) {
-                fieldDataFieldsContext.add(new FieldDataField(field));
-            }
-            fieldDataFieldsContext.setHitExecutionNeeded(true);
-        }
-        if (scriptFields != null) {
-            for (ScriptField field : scriptFields) {
-                SearchScript searchScript = subSearchContext.scriptService().search(subSearchContext.lookup(), field.script(),
-                        ScriptContext.Standard.SEARCH, Collections.emptyMap());
-                subSearchContext.scriptFields().add(new org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField(
-                        field.fieldName(), searchScript, field.ignoreFailure()));
-            }
-        }
-        if (fetchSourceContext != null) {
-            subSearchContext.fetchSourceContext(fetchSourceContext);
-        }
-        if (highlightBuilder != null) {
-            subSearchContext.highlight(highlightBuilder.build(context.searchContext().getQueryShardContext()));
-        }
-        return new TopHitsAggregator(context.searchContext().fetchPhase(), subSearchContext, name, context, parent,
-                pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
new file mode 100644
index 0000000..1efd4a7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -0,0 +1,204 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.tophits;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.highlight.HighlightBuilder;
+import org.elasticsearch.search.sort.SortBuilder;
+import org.elasticsearch.search.sort.SortOrder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link TopHits} aggregation.
+ */
+public class TopHitsBuilder extends AbstractAggregationBuilder {
+
+    private SearchSourceBuilder sourceBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public TopHitsBuilder(String name) {
+        super(name, InternalTopHits.TYPE.name());
+    }
+
+    /**
+     * The index to start to return hits from. Defaults to <tt>0</tt>.
+     */
+    public TopHitsBuilder setFrom(int from) {
+        sourceBuilder().from(from);
+        return this;
+    }
+
+
+    /**
+     * The number of search hits to return. Defaults to <tt>10</tt>.
+     */
+    public TopHitsBuilder setSize(int size) {
+        sourceBuilder().size(size);
+        return this;
+    }
+
+    /**
+     * Applies when sorting, and controls if scores will be tracked as well. Defaults to
+     * <tt>false</tt>.
+     */
+    public TopHitsBuilder setTrackScores(boolean trackScores) {
+        sourceBuilder().trackScores(trackScores);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with an
+     * explanation of the hit (ranking).
+     */
+    public TopHitsBuilder setExplain(boolean explain) {
+        sourceBuilder().explain(explain);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with its
+     * version.
+     */
+    public TopHitsBuilder setVersion(boolean version) {
+        sourceBuilder().version(version);
+        return this;
+    }
+
+    /**
+     * Adds a field to loaded and returned.
+     */
+    public TopHitsBuilder addField(String field) {
+        sourceBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Sets no fields to be loaded, resulting in only id and type to be returned per field.
+     */
+    public TopHitsBuilder setNoFields() {
+        sourceBuilder().noFields();
+        return this;
+    }
+
+    /**
+     * Indicates whether the response should contain the stored _source for every hit
+     */
+    public TopHitsBuilder setFetchSource(boolean fetch) {
+        sourceBuilder().fetchSource(fetch);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param include An optional include (optionally wildcarded) pattern to filter the returned _source
+     * @param exclude An optional exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String include, @Nullable String exclude) {
+        sourceBuilder().fetchSource(include, exclude);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param includes An optional list of include (optionally wildcarded) pattern to filter the returned _source
+     * @param excludes An optional list of exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
+        sourceBuilder().fetchSource(includes, excludes);
+        return this;
+    }
+
+    /**
+     * Adds a field data based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name The field to get from the field data cache
+     */
+    public TopHitsBuilder addFieldDataField(String name) {
+        sourceBuilder().fieldDataField(name);
+        return this;
+    }
+
+    /**
+     * Adds a script based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name   The name that will represent this value in the return hit
+     * @param script The script to use
+     */
+    public TopHitsBuilder addScriptField(String name, Script script) {
+        sourceBuilder().scriptField(name, script);
+        return this;
+    }
+
+    /**
+     * Adds a sort against the given field name and the sort ordering.
+     *
+     * @param field The name of the field
+     * @param order The sort ordering
+     */
+    public TopHitsBuilder addSort(String field, SortOrder order) {
+        sourceBuilder().sort(field, order);
+        return this;
+    }
+
+    /**
+     * Adds a generic sort builder.
+     *
+     * @see org.elasticsearch.search.sort.SortBuilders
+     */
+    public TopHitsBuilder addSort(SortBuilder sort) {
+        sourceBuilder().sort(sort);
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).field(type);
+        sourceBuilder().toXContent(builder, params);
+        return builder.endObject();
+    }
+
+    private SearchSourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    public HighlightBuilder highlighter() {
+        return sourceBuilder().highlighter();
+    }
+
+    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
+        sourceBuilder().highlighter(highlightBuilder);
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
index f8e0095..50b3482 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
@@ -18,36 +18,30 @@
  */
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.highlight.HighlighterParseElement;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.internal.SubSearchContext;
 import org.elasticsearch.search.sort.SortParseElement;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
 
 /**
  *
  */
 public class TopHitsParser implements Aggregator.Parser {
 
+    private final FetchPhase fetchPhase;
     private final SortParseElement sortParseElement;
     private final FetchSourceParseElement sourceParseElement;
     private final HighlighterParseElement highlighterParseElement;
@@ -56,9 +50,10 @@ public class TopHitsParser implements Aggregator.Parser {
     private final FieldsParseElement fieldsParseElement;
 
     @Inject
-    public TopHitsParser(SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
+    public TopHitsParser(FetchPhase fetchPhase, SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
             HighlighterParseElement highlighterParseElement, FieldDataFieldsParseElement fieldDataFieldsParseElement,
             ScriptFieldsParseElement scriptFieldsParseElement, FieldsParseElement fieldsParseElement) {
+        this.fetchPhase = fetchPhase;
         this.sortParseElement = sortParseElement;
         this.sourceParseElement = sourceParseElement;
         this.highlighterParseElement = highlighterParseElement;
@@ -73,141 +68,74 @@ public class TopHitsParser implements Aggregator.Parser {
     }
 
     @Override
-    public TopHitsAggregator.TopHitsAggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-        TopHitsAggregator.TopHitsAggregatorBuilder factory = new TopHitsAggregator.TopHitsAggregatorBuilder(aggregationName);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SubSearchContext subSearchContext = new SubSearchContext(context);
         XContentParser.Token token;
         String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FROM_FIELD)) {
-                    factory.from(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SIZE_FIELD)) {
-                    factory.size(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.VERSION_FIELD)) {
-                    factory.version(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.EXPLAIN_FIELD)) {
-                    factory.explain(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.TRACK_SCORES_FIELD)) {
-                    factory.trackScores(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    factory.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            Script script = null;
-                            boolean ignoreFailure = false;
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else if (context.parseFieldMatcher().match(currentFieldName,
-                                            SearchSourceBuilder.IGNORE_FAILURE_FIELD)) {
-                                        ignoreFailure = parser.booleanValue();
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(),
-                                            "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation());
-                                }
-                            }
-                            scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure));
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+        try {
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    currentFieldName = parser.currentName();
+                } else if ("sort".equals(currentFieldName)) {
+                    sortParseElement.parse(parser, subSearchContext);
+                } else if ("_source".equals(currentFieldName)) {
+                    sourceParseElement.parse(parser, subSearchContext);
+                } else if ("fields".equals(currentFieldName)) {
+                    fieldsParseElement.parse(parser, subSearchContext);
+                } else if (token.isValue()) {
+                    switch (currentFieldName) {
+                        case "from":
+                            subSearchContext.from(parser.intValue());
+                            break;
+                        case "size":
+                            subSearchContext.size(parser.intValue());
+                            break;
+                        case "track_scores":
+                        case "trackScores":
+                            subSearchContext.trackScores(parser.booleanValue());
+                            break;
+                        case "version":
+                            subSearchContext.version(parser.booleanValue());
+                            break;
+                        case "explain":
+                            subSearchContext.explain(parser.booleanValue());
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.scriptFields(scriptFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.HIGHLIGHT_FIELD)) {
-                    factory.highlighter(HighlightBuilder.PROTOTYPE.fromXContent(context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    factory.sorts(sorts);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+                } else if (token == XContentParser.Token.START_OBJECT) {
+                    switch (currentFieldName) {
+                        case "highlight":
+                            highlighterParseElement.parse(parser, subSearchContext);
+                            break;
+                        case "scriptFields":
+                        case "script_fields":
+                            scriptFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.fieldDataFields(fieldDataFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
+                } else if (token == XContentParser.Token.START_ARRAY) {
+                    switch (currentFieldName) {
+                        case "fielddataFields":
+                        case "fielddata_fields":
+                            fieldDataFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.sorts(sorts);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
                             parser.getTokenLocation());
                 }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
             }
+        } catch (Exception e) {
+            throw ExceptionsHelper.convertToElastic(e);
         }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new TopHitsAggregator.TopHitsAggregatorBuilder(null);
+        return new TopHitsAggregator.Factory(aggregationName, fetchPhase, subSearchContext);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
index 16c38e2..2d1d9db 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
@@ -19,29 +19,20 @@
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -117,43 +108,24 @@ public class ValueCountAggregator extends NumericMetricsAggregator.SingleValue {
         Releasables.close(counts);
     }
 
-    public static class ValueCountAggregatorBuilder
-            extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource, ValueCountAggregatorBuilder> {
+    public static class Factory<VS extends ValuesSource> extends ValuesSourceAggregatorFactory.LeafOnly<VS> {
 
-        public ValueCountAggregatorBuilder(String name, ValueType targetValueType) {
-            super(name, InternalValueCount.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig<VS> config) {
+            super(name, InternalValueCount.TYPE.name(), config);
         }
 
         @Override
-        protected ValueCountAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-                AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder) throws IOException {
-            return new ValueCountAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new ValueCountAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected ValuesSourceAggregatorBuilder<ValuesSource, ValueCountAggregatorBuilder> innerReadFrom(String name,
-                ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in) {
-            return new ValueCountAggregator.ValueCountAggregatorBuilder(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(VS valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new ValueCountAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators,
+                    metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregatorFactory.java
deleted file mode 100644
index 73481e1..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregatorFactory.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.valuecount;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ValueCountAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, ValueCountAggregatorFactory> {
-
-    public ValueCountAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        return new ValueCountAggregator(name, null, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new ValueCountAggregator(name, valuesSource, config.formatter(), context, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
new file mode 100644
index 0000000..1d37914
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.valuecount;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link ValueCount} aggregation.
+ */
+public class ValueCountBuilder extends ValuesSourceMetricsAggregationBuilder<ValueCountBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public ValueCountBuilder(String name) {
+        super(name, InternalValueCount.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
index 043958f..764f6ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
@@ -18,27 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ValueCountParser extends AnyValuesSourceParser {
-
-    public ValueCountParser() {
-        super(true, true);
-    }
+public class ValueCountParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +38,22 @@ public class ValueCountParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorBuilder<ValuesSource, ValueCountAggregator.ValueCountAggregatorBuilder> createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new ValueCountAggregator.ValueCountAggregatorBuilder(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ValueCountAggregator.ValueCountAggregatorBuilder(null, null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalValueCount.TYPE, context)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new ValueCountAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
index 24d2913..881a8e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
@@ -20,17 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.InvalidAggregationPathException;
 import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;
 import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
 import org.elasticsearch.search.aggregations.support.AggregationPath;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -62,7 +62,7 @@ public class BucketHelpers {
          * @param text    GapPolicy in string format (e.g. "ignore")
          * @return        GapPolicy enum
          */
-        public static GapPolicy parse(QueryParseContext context, String text, XContentLocation tokenLocation) {
+        public static GapPolicy parse(SearchContext context, String text, XContentLocation tokenLocation) {
             GapPolicy result = null;
             for (GapPolicy policy : values()) {
                 if (context.parseFieldMatcher().match(text, policy.parseField)) {
@@ -79,7 +79,7 @@ public class BucketHelpers {
                 for (GapPolicy policy : values()) {
                     validNames.add(policy.getName());
                 }
-                throw new ParsingException(tokenLocation, "Invalid gap policy: [" + text + "], accepted values: " + validNames);
+                throw new SearchParseException(context, "Invalid gap policy: [" + text + "], accepted values: " + validNames, tokenLocation);
             }
             return result;
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
index 0dc398b..b2ee037 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
@@ -25,10 +25,10 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Map;
@@ -38,8 +38,8 @@ public abstract class PipelineAggregator implements Streamable {
     /**
      * Parses the pipeline aggregation request and creates the appropriate
      * pipeline aggregator factory for it.
-     *
-     * @see PipelineAggregatorBuilder
+     * 
+     * @see PipelineAggregatorFactory
      */
     public static interface Parser {
 
@@ -56,7 +56,7 @@ public abstract class PipelineAggregator implements Streamable {
         /**
          * Returns the pipeline aggregator factory with which this parser is
          * associated.
-         *
+         * 
          * @param pipelineAggregatorName
          *            The name of the pipeline aggregation
          * @param parser
@@ -67,13 +67,7 @@ public abstract class PipelineAggregator implements Streamable {
          * @throws java.io.IOException
          *             When parsing fails
          */
-        PipelineAggregatorBuilder parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context) throws IOException;
-
-        /**
-         * @return an empty {@link PipelineAggregatorBuilder} instance for this
-         *         parser that can be used for deserialization
-         */
-        PipelineAggregatorBuilder getFactoryPrototype();
+        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException;
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
index 8df233a..56ae321 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
@@ -16,118 +16,46 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
- * A factory that knows how to create an {@link PipelineAggregator} of a
- * specific type.
+ * A base class for all pipeline aggregator builders.
  */
-public abstract class PipelineAggregatorBuilder extends ToXContentToBytes implements NamedWriteable<PipelineAggregatorBuilder>, ToXContent {
+public abstract class PipelineAggregatorBuilder<B extends PipelineAggregatorBuilder<B>> extends AbstractAggregationBuilder {
 
-    protected String name;
-    protected String type;
-    protected String[] bucketsPaths;
-    protected Map<String, Object> metaData;
+    private String[] bucketsPaths;
+    private Map<String, Object> metaData;
 
     /**
-     * Constructs a new pipeline aggregator factory.
-     *
-     * @param name
-     *            The aggregation name
-     * @param type
-     *            The aggregation type
+     * Sole constructor, typically used by sub-classes.
      */
-    public PipelineAggregatorBuilder(String name, String type, String[] bucketsPaths) {
-        this.name = name;
-        this.type = type;
-        this.bucketsPaths = bucketsPaths;
-    }
-
-    public String name() {
-        return name;
-    }
-
-    public String type() {
-        return type;
+    protected PipelineAggregatorBuilder(String name, String type) {
+        super(name, type);
     }
 
     /**
-     * Validates the state of this factory (makes sure the factory is properly
-     * configured)
+     * Sets the paths to the buckets to use for this pipeline aggregator
      */
-    public final void validate(AggregatorFactory<?> parent, AggregatorFactory<?>[] factories,
-            List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
-        doValidate(parent, factories, pipelineAggregatorFactories);
+    public B setBucketsPaths(String... bucketsPaths) {
+        this.bucketsPaths = bucketsPaths;
+        return (B) this;
     }
 
-    protected abstract PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException;
-
     /**
-     * Creates the pipeline aggregator
-     *
-     * @return The created aggregator
+     * Sets the meta data to be included in the pipeline aggregator's response
      */
-    public final PipelineAggregator create() throws IOException {
-        PipelineAggregator aggregator = createInternal(this.metaData);
-        return aggregator;
-    }
-
-    public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] factories,
-            List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
-    }
-
-    public void setMetaData(Map<String, Object> metaData) {
+    public B setMetaData(Map<String, Object> metaData) {
         this.metaData = metaData;
+        return (B)this;
     }
 
-    public String getName() {
-        return name;
-    }
-
-    public String[] getBucketsPaths() {
-        return bucketsPaths;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        out.writeStringArray(bucketsPaths);
-        doWriteTo(out);
-        out.writeMap(metaData);
-    }
-
-    protected abstract void doWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    public String getWriteableName() {
-        return type;
-    }
-
-    @Override
-    public PipelineAggregatorBuilder readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        String[] bucketsPaths = in.readStringArray();
-        PipelineAggregatorBuilder factory = doReadFrom(name, bucketsPaths, in);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    protected abstract PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException;
-
     @Override
     public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(getName());
@@ -137,7 +65,7 @@ public abstract class PipelineAggregatorBuilder extends ToXContentToBytes implem
         }
         builder.startObject(type);
 
-        if (!overrideBucketsPath() && bucketsPaths != null) {
+        if (bucketsPaths != null) {
             builder.startArray(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName());
             for (String path : bucketsPaths) {
                 builder.value(path);
@@ -152,41 +80,5 @@ public abstract class PipelineAggregatorBuilder extends ToXContentToBytes implem
         return builder.endObject();
     }
 
-    /**
-     * @return <code>true</code> if the {@link PipelineAggregatorBuilder}
-     *         overrides the XContent rendering of the bucketPath option.
-     */
-    protected boolean overrideBucketsPath() {
-        return false;
-    }
-
     protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(Arrays.hashCode(bucketsPaths), metaData, name, type, doHashCode());
-    }
-
-    protected abstract int doHashCode();
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        PipelineAggregatorBuilder other = (PipelineAggregatorBuilder) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.deepEquals(bucketsPaths, other.bucketsPaths))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        return doEquals(obj);
-    }
-
-    protected abstract boolean doEquals(Object obj);
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
index 5087835..6fbc6f8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
@@ -19,92 +19,74 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-
-import java.util.Map;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptBuilder;
+import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumBuilder;
+import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeBuilder;
+import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgBuilder;
+import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffBuilder;
 
 public final class PipelineAggregatorBuilders {
 
     private PipelineAggregatorBuilders() {
     }
 
-    public static final DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder derivative(String name, String bucketsPath) {
-        return new DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder(name, bucketsPath);
+    public static final DerivativeBuilder derivative(String name) {
+        return new DerivativeBuilder(name);
     }
 
-    public static final MaxBucketPipelineAggregator.MaxBucketPipelineAggregatorBuilder maxBucket(String name, String bucketsPath) {
-        return new MaxBucketPipelineAggregator.MaxBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final MaxBucketBuilder maxBucket(String name) {
+        return new MaxBucketBuilder(name);
     }
 
-    public static final MinBucketPipelineAggregator.MinBucketPipelineAggregatorBuilder minBucket(String name, String bucketsPath) {
-        return new MinBucketPipelineAggregator.MinBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final MinBucketBuilder minBucket(String name) {
+        return new MinBucketBuilder(name);
     }
 
-    public static final AvgBucketPipelineAggregator.AvgBucketPipelineAggregatorBuilder avgBucket(String name, String bucketsPath) {
-        return new AvgBucketPipelineAggregator.AvgBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final AvgBucketBuilder avgBucket(String name) {
+        return new AvgBucketBuilder(name);
     }
 
-    public static final SumBucketPipelineAggregator.SumBucketPipelineAggregatorBuilder sumBucket(String name, String bucketsPath) {
-        return new SumBucketPipelineAggregator.SumBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final SumBucketBuilder sumBucket(String name) {
+        return new SumBucketBuilder(name);
     }
 
-    public static final StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder statsBucket(String name, String bucketsPath) {
-        return new StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final StatsBucketBuilder statsBucket(String name) {
+        return new StatsBucketBuilder(name);
     }
 
-    public static final ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder extendedStatsBucket(String name,
-            String bucketsPath) {
-        return new ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final ExtendedStatsBucketBuilder extendedStatsBucket(String name) {
+        return new ExtendedStatsBucketBuilder(name);
     }
 
-    public static final PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder percentilesBucket(String name,
-            String bucketsPath) {
-        return new PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder(name, bucketsPath);
+    public static final PercentilesBucketBuilder percentilesBucket(String name) {
+        return new PercentilesBucketBuilder(name);
     }
 
-    public static final MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder movingAvg(String name, String bucketsPath) {
-        return new MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder(name, bucketsPath);
+    public static final MovAvgBuilder movingAvg(String name) {
+        return new MovAvgBuilder(name);
     }
 
-    public static final BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder bucketScript(String name,
-            Map<String, String> bucketsPathsMap, Script script) {
-        return new BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder(name, bucketsPathsMap, script);
+    public static final BucketScriptBuilder bucketScript(String name) {
+        return new BucketScriptBuilder(name);
     }
 
-    public static final BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder bucketScript(String name, Script script,
-            String... bucketsPaths) {
-        return new BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder(name, script, bucketsPaths);
+    public static final BucketSelectorBuilder having(String name) {
+        return new BucketSelectorBuilder(name);
     }
 
-    public static final BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder bucketSelector(String name,
-            Map<String, String> bucketsPathsMap, Script script) {
-        return new BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder(name, bucketsPathsMap, script);
+    public static final CumulativeSumBuilder cumulativeSum(String name) {
+        return new CumulativeSumBuilder(name);
     }
 
-    public static final BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder bucketSelector(String name, Script script,
-            String... bucketsPaths) {
-        return new BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder(name, script, bucketsPaths);
-    }
-
-    public static final CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder cumulativeSum(String name,
-            String bucketsPath) {
-        return new CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder(name, bucketsPath);
-    }
-
-    public static final SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder diff(String name, String bucketsPath) {
-        return new SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder(name, bucketsPath);
+    public static final SerialDiffBuilder diff(String name) {
+        return new SerialDiffBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
new file mode 100644
index 0000000..6fc0185
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.pipeline;
+
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * A factory that knows how to create an {@link PipelineAggregator} of a
+ * specific type.
+ */
+public abstract class PipelineAggregatorFactory {
+
+    protected String name;
+    protected String type;
+    protected String[] bucketsPaths;
+    protected Map<String, Object> metaData;
+
+    /**
+     * Constructs a new pipeline aggregator factory.
+     *
+     * @param name
+     *            The aggregation name
+     * @param type
+     *            The aggregation type
+     */
+    public PipelineAggregatorFactory(String name, String type, String[] bucketsPaths) {
+        this.name = name;
+        this.type = type;
+        this.bucketsPaths = bucketsPaths;
+    }
+
+    public String name() {
+        return name;
+    }
+
+    /**
+     * Validates the state of this factory (makes sure the factory is properly
+     * configured)
+     */
+    public final void validate(AggregatorFactory parent, AggregatorFactory[] factories,
+            List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
+        doValidate(parent, factories, pipelineAggregatorFactories);
+    }
+
+    protected abstract PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException;
+
+    /**
+     * Creates the pipeline aggregator
+     *
+     * @return The created aggregator
+     */
+    public final PipelineAggregator create() throws IOException {
+        PipelineAggregator aggregator = createInternal(this.metaData);
+        return aggregator;
+    }
+
+    public void doValidate(AggregatorFactory parent, AggregatorFactory[] factories,
+            List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
+    }
+
+    public void setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public String[] getBucketsPaths() {
+        return bucketsPaths;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java
new file mode 100644
index 0000000..88a1f42
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketParser;
+import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
+
+import java.io.IOException;
+
+/**
+ * A builder for building requests for a {@link BucketMetricsPipelineAggregator}
+ */
+public abstract class BucketMetricsBuilder<B extends BucketMetricsBuilder<B>> extends PipelineAggregatorBuilder<B> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+
+    public BucketMetricsBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    public B format(String format) {
+        this.format = format;
+        return (B) this;
+    }
+
+    public B gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return (B) this;
+    }
+
+    @Override
+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(MinBucketParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        doInternalXContent(builder, params);
+        return builder;
+    }
+
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
index 5868864..3cf084b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
@@ -20,12 +20,14 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -46,13 +48,12 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public final PipelineAggregatorBuilder parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
         Map<String, Object> leftover = new HashMap<>(5);
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -85,34 +86,34 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        BucketMetricsPipelineAggregatorBuilder factory = null;
+        ValueFormatter formatter = null;
+        if (format != null) {
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
+        }
+
+        PipelineAggregatorFactory factory = null;
         try {
-            factory = buildFactory(pipelineAggregatorName, bucketsPaths[0], leftover);
-            if (format != null) {
-                factory.format(format);
-            }
-            if (gapPolicy != null) {
-                factory.gapPolicy(gapPolicy);
-            }
+            factory = buildFactory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, leftover);
         } catch (ParseException exception) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Could not parse settings for aggregation [" + pipelineAggregatorName + "].", exception);
+            throw new SearchParseException(context, "Could not parse settings for aggregation ["
+                    + pipelineAggregatorName + "].", null, exception);
         }
 
         if (leftover.size() > 0) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].");
+            throw new SearchParseException(context, "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].", null);
         }
         assert(factory != null);
 
         return factory;
     }
 
-    protected abstract BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPaths,
-            Map<String, Object> unparsedParams) throws ParseException;
+    protected abstract PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException;
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregatorBuilder.java
deleted file mode 100644
index 5ba1d1d..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregatorBuilder.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-public abstract class BucketMetricsPipelineAggregatorBuilder<AF extends BucketMetricsPipelineAggregatorBuilder<AF>>
-        extends PipelineAggregatorBuilder {
-
-    private String format = null;
-    private GapPolicy gapPolicy = GapPolicy.SKIP;
-
-    public BucketMetricsPipelineAggregatorBuilder(String name, String type, String[] bucketsPaths) {
-        super(name, type, bucketsPaths);
-    }
-
-    /**
-     * Sets the format to use on the output of this aggregation.
-     */
-    public AF format(String format) {
-        this.format = format;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the format to use on the output of this aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    protected ValueFormatter formatter() {
-        if (format != null) {
-            return ValueFormat.Patternable.Number.format(format).formatter();
-        } else {
-            return ValueFormatter.RAW;
-        }
-    }
-
-    /**
-     * Sets the gap policy to use for this aggregation.
-     */
-    public AF gapPolicy(GapPolicy gapPolicy) {
-        this.gapPolicy = gapPolicy;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the gap policy to use for this aggregation.
-     */
-    public GapPolicy gapPolicy() {
-        return gapPolicy;
-    }
-
-    @Override
-    protected abstract PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException;
-
-    @Override
-    public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-            List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
-        if (bucketsPaths.length != 1) {
-            throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
-                    + " must contain a single entry for aggregation [" + name + "]");
-        }
-    }
-
-    @Override
-    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        if (format != null) {
-            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-        }
-        if (gapPolicy != null) {
-            builder.field(BucketMetricsParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-        }
-        doXContentBody(builder, params);
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-        BucketMetricsPipelineAggregatorBuilder factory = innerReadFrom(name, bucketsPaths, in);
-        factory.format = in.readOptionalString();
-        factory.gapPolicy = GapPolicy.readFrom(in);
-        return factory;
-    }
-
-    protected abstract BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-            throws IOException;
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        innerWriteTo(out);
-        out.writeOptionalString(format);
-        gapPolicy.writeTo(out);
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(format, gapPolicy, innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        BucketMetricsPipelineAggregatorBuilder other = (BucketMetricsPipelineAggregatorBuilder) obj;
-        return Objects.equals(format, other.format)
-                && Objects.equals(gapPolicy, other.gapPolicy)
-                && innerEquals(other);
-    }
-
-    protected abstract boolean innerEquals(BucketMetricsPipelineAggregatorBuilder other);
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java
new file mode 100644
index 0000000..627cded
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class AvgBucketBuilder extends BucketMetricsBuilder<AvgBucketBuilder> {
+
+    public AvgBucketBuilder(String name) {
+        super(name, AvgBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
index 2cc1dde..658284f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,13 +33,8 @@ public class AvgBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) {
-        return new AvgBucketPipelineAggregator.AvgBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new AvgBucketPipelineAggregator.AvgBucketPipelineAggregatorBuilder(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
index b6697bd..3ab134c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
@@ -20,17 +20,14 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -89,55 +86,30 @@ public class AvgBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), avgValue, formatter, pipelineAggregators, metadata);
     }
 
-    public static class AvgBucketPipelineAggregatorBuilder extends BucketMetricsPipelineAggregatorBuilder<AvgBucketPipelineAggregatorBuilder> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public AvgBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private AvgBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new AvgBucketPipelineAggregatorBuilder(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java
new file mode 100644
index 0000000..8b214d3
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class MaxBucketBuilder extends BucketMetricsBuilder<MaxBucketBuilder> {
+
+    public MaxBucketBuilder(String name) {
+        super(name, MaxBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
index cf784fa..4cd584a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -33,14 +34,9 @@ public class MaxBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) {
-        return new MaxBucketPipelineAggregator.MaxBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new MaxBucketPipelineAggregator.MaxBucketPipelineAggregatorBuilder(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
index 231faa3..95a70af 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
@@ -20,16 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -96,57 +93,31 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, maxValue, formatter, Collections.emptyList(), metaData());
     }
 
-    public static class MaxBucketPipelineAggregatorBuilder
-            extends BucketMetricsPipelineAggregatorBuilder<MaxBucketPipelineAggregatorBuilder> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public MaxBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private MaxBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-                throws IOException {
-            return new MaxBucketPipelineAggregatorBuilder(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java
new file mode 100644
index 0000000..327bf4e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+
+public class MinBucketBuilder extends BucketMetricsBuilder<MinBucketBuilder> {
+
+    public MinBucketBuilder(String name) {
+        super(name, MinBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
index a53a22e..db7bc9b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,15 +33,9 @@ public class MinBucketParser extends BucketMetricsParser {
         return MinBucketPipelineAggregator.TYPE.name();
     }
 
-    @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) {
-        return new MinBucketPipelineAggregator.MinBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new MinBucketPipelineAggregator.MinBucketPipelineAggregatorBuilder(null, null);
-    }
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
+    };
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
index 8cf8f64..755b206 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
@@ -20,16 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -97,57 +94,31 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, minValue, formatter, Collections.emptyList(), metaData());
     };
 
-    public static class MinBucketPipelineAggregatorBuilder
-            extends BucketMetricsPipelineAggregatorBuilder<MinBucketPipelineAggregatorBuilder> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public MinBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private MinBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-                throws IOException {
-            return new MinBucketPipelineAggregatorBuilder(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java
new file mode 100644
index 0000000..9293e14
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+import java.io.IOException;
+
+public class PercentilesBucketBuilder extends BucketMetricsBuilder<PercentilesBucketBuilder> {
+
+    Double[] percents;
+
+    public PercentilesBucketBuilder(String name) {
+        super(name, PercentilesBucketPipelineAggregator.TYPE.name());
+    }
+
+    public PercentilesBucketBuilder percents(Double[] percents) {
+        this.percents = percents;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (percents != null) {
+            builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), (Object[])percents);
+        }
+    }
+
+
+}
+
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
index edec97d..7c9da5c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
@@ -20,14 +20,16 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.List;
 import java.util.Map;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 
 public class PercentilesBucketParser extends BucketMetricsParser {
 
@@ -39,10 +41,10 @@ public class PercentilesBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        double[] percents = null;
+        double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
         int counter = 0;
         Object percentParam = unparsedParams.get(PERCENTS.getPreferredName());
 
@@ -65,16 +67,6 @@ public class PercentilesBucketParser extends BucketMetricsParser {
             }
         }
 
-        PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder factory = new 
-                PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-        if (percents != null) {
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder(null, null);
+        return new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, percents);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
index f6ddde3..24e8204 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
@@ -19,33 +19,28 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
+
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 
 public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
     public final static Type TYPE = new Type("percentiles_bucket");
-    public final ParseField PERCENTS_FIELD = new ParseField("percents");
 
     public final static PipelineAggregatorStreams.Stream STREAM = new PipelineAggregatorStreams.Stream() {
         @Override
@@ -124,48 +119,27 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
         out.writeDoubleArray(percents);
     }
 
-    public static class PercentilesBucketPipelineAggregatorBuilder
-            extends BucketMetricsPipelineAggregatorBuilder<PercentilesBucketPipelineAggregatorBuilder> {
-
-        private double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public PercentilesBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double[] percents;
 
-        private PercentilesBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter, double[] percents) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Get the percentages to calculate percentiles for in this aggregation
-         */
-        public double[] percents() {
-            return percents;
-        }
-
-        /**
-         * Set the percentages to calculate percentiles for in this aggregation
-         */
-        public PercentilesBucketPipelineAggregatorBuilder percents(double[] percents) {
-            for (Double p : percents) {
-                if (p == null || p < 0.0 || p > 100.0) {
-                    throw new IllegalArgumentException(PercentilesBucketParser.PERCENTS.getPreferredName()
-                            + " must only contain non-null doubles from 0.0-100.0 inclusive");
-                }
-            }
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.percents = percents;
-            return this;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                               List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
@@ -179,38 +153,6 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            if (percents != null) {
-                builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), percents);
-            }
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-                throws IOException {
-            PercentilesBucketPipelineAggregatorBuilder factory = new PercentilesBucketPipelineAggregatorBuilder(name, bucketsPaths);
-            factory.percents = in.readDoubleArray();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(percents);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Arrays.hashCode(percents);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder obj) {
-            PercentilesBucketPipelineAggregatorBuilder other = (PercentilesBucketPipelineAggregatorBuilder) obj;
-            return Objects.deepEquals(percents, other.percents);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java
new file mode 100644
index 0000000..a8c19db
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class StatsBucketBuilder extends BucketMetricsBuilder<StatsBucketBuilder> {
+
+    public StatsBucketBuilder(String name) {
+        super(name, StatsBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
index 6e5fa51..b250447 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,13 +33,8 @@ public class StatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) {
-        return new StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
index 6ac5f22..66726ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
@@ -20,16 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -95,57 +92,31 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
         return new InternalStatsBucket(name(), count, sum, min, max, formatter, pipelineAggregators, metadata);
     }
 
-    public static class StatsBucketPipelineAggregatorBuilder
-            extends BucketMetricsPipelineAggregatorBuilder<StatsBucketPipelineAggregatorBuilder> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public StatsBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private StatsBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-                throws IOException {
-            return new StatsBucketPipelineAggregatorBuilder(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java
new file mode 100644
index 0000000..25880bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+import java.io.IOException;
+
+public class ExtendedStatsBucketBuilder extends BucketMetricsBuilder<ExtendedStatsBucketBuilder> {
+
+    Double sigma;
+
+    public ExtendedStatsBucketBuilder(String name) {
+        super(name, ExtendedStatsBucketPipelineAggregator.TYPE.name());
+    }
+
+    public ExtendedStatsBucketBuilder sigma(Double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (sigma != null) {
+            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
index b70406c..b4d1f18 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
@@ -20,9 +20,10 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.Map;
@@ -36,10 +37,10 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        Double sigma = null;
+        double sigma = 2.0;
         Object param = unparsedParams.get(SIGMA.getPreferredName());
 
         if (param != null) {
@@ -51,16 +52,6 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
                         + param.getClass().getSimpleName() + "` provided instead", 0);
             }
         }
-        ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder factory = 
-                new ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-        if (sigma != null) {
-            factory.sigma(sigma);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder(null, null);
+        return new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, sigma, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
index 92562d9..6a7f2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
@@ -20,23 +20,19 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
@@ -101,47 +97,27 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
         return new InternalExtendedStatsBucket(name(), count, sum, min, max, sumOfSqrs, sigma, formatter, pipelineAggregators, metadata);
     }
 
-    public static class ExtendedStatsBucketPipelineAggregatorBuilder
-            extends BucketMetricsPipelineAggregatorBuilder<ExtendedStatsBucketPipelineAggregatorBuilder> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private double sigma = 2.0;
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double sigma;
 
-        public ExtendedStatsBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private ExtendedStatsBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, double sigma, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Set the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public ExtendedStatsBucketPipelineAggregatorBuilder sigma(double sigma) {
-            if (sigma < 0.0) {
-                throw new IllegalArgumentException(ExtendedStatsBucketParser.SIGMA.getPreferredName() + " must be a non-negative double");
-            }
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.sigma = sigma;
-            return this;
-        }
-
-        /**
-         * Get the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public double sigma() {
-            return sigma;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy(), formatter(), metaData);
+            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
@@ -153,36 +129,6 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-                throws IOException {
-            ExtendedStatsBucketPipelineAggregatorBuilder factory = new ExtendedStatsBucketPipelineAggregatorBuilder(name, bucketsPaths);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder obj) {
-            ExtendedStatsBucketPipelineAggregatorBuilder other = (ExtendedStatsBucketPipelineAggregatorBuilder) obj;
-            return Objects.equals(sigma, other.sigma);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java
new file mode 100644
index 0000000..5b2201c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class SumBucketBuilder extends BucketMetricsBuilder<SumBucketBuilder> {
+
+    public SumBucketBuilder(String name) {
+        super(name, SumBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
index 9ae717c..3fad95d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,13 +33,8 @@ public class SumBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsPipelineAggregatorBuilder buildFactory(String pipelineAggregatorName, String bucketsPath,
-            Map<String, Object> unparsedParams) {
-        return new SumBucketPipelineAggregator.SumBucketPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new SumBucketPipelineAggregator.SumBucketPipelineAggregatorBuilder(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
index 4072a55..138bd63 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
@@ -20,17 +20,14 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregatorBuilder;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -85,57 +82,31 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), sum, formatter, pipelineAggregators, metadata);
     }
 
-    public static class SumBucketPipelineAggregatorBuilder
-            extends BucketMetricsPipelineAggregatorBuilder<SumBucketPipelineAggregatorBuilder> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public SumBucketPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private SumBucketPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsPipelineAggregatorBuilder innerReadFrom(String name, String[] bucketsPaths, StreamInput in)
-                throws IOException {
-            return new SumBucketPipelineAggregatorBuilder(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsPipelineAggregatorBuilder other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java
new file mode 100644
index 0000000..ee5fa94
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketscript;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class BucketScriptBuilder extends PipelineAggregatorBuilder<BucketScriptBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private Script script;
+    private Map<String, String> bucketsPathsMap;
+
+    public BucketScriptBuilder(String name) {
+        super(name, BucketScriptPipelineAggregator.TYPE.name());
+    }
+
+    public BucketScriptBuilder script(Script script) {
+        this.script = script;
+        return this;
+    }
+
+    public BucketScriptBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public BucketScriptBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator
+     */
+    public BucketScriptBuilder setBucketsPathsMap(Map<String, String> bucketsPathsMap) {
+        this.bucketsPathsMap = bucketsPathsMap;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        if (script != null) {
+            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        }
+        if (format != null) {
+            builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (bucketsPathsMap != null) {
+            builder.field(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
index d18874d..05ff7e9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
@@ -20,18 +20,19 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,13 +49,13 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorBuilder parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -70,8 +71,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -85,8 +86,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -98,38 +99,33 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder factory = 
-                new BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder(reducerName, bucketsPathsMap, script);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder(null, Collections.emptyMap(), null);
+        return new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, formatter, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
index 218a80c..4da355f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
@@ -21,11 +21,9 @@ package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -37,9 +35,8 @@ import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Buck
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -49,8 +46,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -160,126 +155,24 @@ public class BucketScriptPipelineAggregator extends PipelineAggregator {
         bucketsPathsMap = (Map<String, String>) in.readGenericValue();
     }
 
-    public static class BucketScriptPipelineAggregatorBuilder extends PipelineAggregatorBuilder {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private final Script script;
-        private final Map<String, String> bucketsPathsMap;
-        private String format = null;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private Script script;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Map<String, String> bucketsPathsMap;
 
-        public BucketScriptPipelineAggregatorBuilder(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, ValueFormatter formatter, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        public BucketScriptPipelineAggregatorBuilder(String name, Script script, String... bucketsPaths) {
-            this(name, convertToBucketsPathMap(bucketsPaths), script);
-        }
-
-        private static Map<String, String> convertToBucketsPathMap(String[] bucketsPaths) {
-            Map<String, String> bucketsPathsMap = new HashMap<>();
-            for (int i = 0; i < bucketsPaths.length; i++) {
-                bucketsPathsMap.put("_value" + i, bucketsPaths[i]);
-            }
-            return bucketsPathsMap;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public BucketScriptPipelineAggregatorBuilder format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public BucketScriptPipelineAggregatorBuilder gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter(), gapPolicy, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            if (format != null) {
-                builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            BucketScriptPipelineAggregatorBuilder factory = new BucketScriptPipelineAggregatorBuilder(name, bucketsPathsMap, script);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, format, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            BucketScriptPipelineAggregatorBuilder other = (BucketScriptPipelineAggregatorBuilder) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(format, other.format) && Objects.equals(gapPolicy, other.gapPolicy);
+            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter, gapPolicy, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java
new file mode 100644
index 0000000..282ded8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class CumulativeSumBuilder extends PipelineAggregatorBuilder<CumulativeSumBuilder> {
+
+    private String format;
+
+    public CumulativeSumBuilder(String name) {
+        super(name, CumulativeSumPipelineAggregator.TYPE.name());
+    }
+
+    public CumulativeSumBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(CumulativeSumParser.FORMAT.getPreferredName(), format);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
index 2efece5..f3e2ead 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
@@ -20,11 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,8 +44,7 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorBuilder parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
@@ -58,8 +59,8 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
                     bucketsPaths = new String[] { parser.text() };
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -70,31 +71,28 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder factory = 
-                new CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder(null, null);
+        return new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
index e8ec130..49c6f4f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
@@ -21,21 +21,18 @@ package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.AbstractHistogramAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -43,7 +40,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -111,57 +107,31 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
         ValueFormatterStreams.writeOptional(formatter, out);
     }
 
-    public static class CumulativeSumPipelineAggregatorBuilder extends PipelineAggregatorBuilder {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
+        private final ValueFormatter formatter;
 
-        public CumulativeSumPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private CumulativeSumPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public CumulativeSumPipelineAggregatorBuilder format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter(), metaData);
+            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories, List<PipelineAggregatorBuilder> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
-            if (!(parent instanceof AbstractHistogramAggregatorFactory<?>)) {
+            if (!(parent instanceof HistogramAggregator.Factory)) {
                 throw new IllegalStateException("cumulative sum aggregation [" + name
                         + "] must have a histogram or date_histogram as parent");
             } else {
-                AbstractHistogramAggregatorFactory<?> histoParent = (AbstractHistogramAggregatorFactory<?>) parent;
+                HistogramAggregator.Factory histoParent = (HistogramAggregator.Factory) parent;
                 if (histoParent.minDocCount() != 0) {
                     throw new IllegalStateException("parent histogram of cumulative sum aggregation [" + name
                             + "] must have min_doc_count of 0");
@@ -169,35 +139,5 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-            }
-            return builder;
-        }
-
-        @Override
-        protected final PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            CumulativeSumPipelineAggregatorBuilder factory = new CumulativeSumPipelineAggregatorBuilder(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected final void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            CumulativeSumPipelineAggregatorBuilder other = (CumulativeSumPipelineAggregatorBuilder) obj;
-            return Objects.equals(format, other.format);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java
new file mode 100644
index 0000000..50b4578
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.derivative;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class DerivativeBuilder extends PipelineAggregatorBuilder<DerivativeBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private String unit;
+
+    public DerivativeBuilder(String name) {
+        super(name, DerivativePipelineAggregator.TYPE.name());
+    }
+
+    public DerivativeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public DerivativeBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    public DerivativeBuilder unit(String unit) {
+        this.unit = unit;
+        return this;
+    }
+
+    /**
+     * Sets the unit using the provided {@link DateHistogramInterval}. This
+     * method is only useful when calculating the derivative using a
+     * `date_histogram`
+     */
+    public DerivativeBuilder unit(DateHistogramInterval unit) {
+        this.unit = unit.toString();
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (unit != null) {
+            builder.field(DerivativeParser.UNIT.getPreferredName(), unit);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
index 8054f48..f413987 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
@@ -20,12 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -43,14 +48,13 @@ public class DerivativeParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorBuilder parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
         String units = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -65,8 +69,8 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, UNIT)) {
                     units = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -77,37 +81,41 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder factory = 
-                new DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
+
+        Long xAxisUnits = null;
         if (units != null) {
-            factory.unit(units);
+            DateTimeUnit dateTimeUnit = DateHistogramParser.DATE_FIELD_UNITS.get(units);
+            if (dateTimeUnit != null) {
+                xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
+            } else {
+                TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
+                if (timeValue != null) {
+                    xAxisUnits = timeValue.getMillis();
+                }
+            }
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder(null, null);
+        return new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, xAxisUnits);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
index de0148b..855fea8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
@@ -21,24 +21,18 @@ package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.rounding.DateTimeUnit;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.AbstractHistogramAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -47,7 +41,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -159,86 +152,35 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
         }
     }
 
-    public static class DerivativePipelineAggregatorBuilder extends PipelineAggregatorBuilder {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private String units;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Long xAxisUnits;
 
-        public DerivativePipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private DerivativePipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy, Long xAxisUnits) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        public DerivativePipelineAggregatorBuilder format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        public String format() {
-            return format;
-        }
-
-        public DerivativePipelineAggregatorBuilder gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        public DerivativePipelineAggregatorBuilder unit(String units) {
-            this.units = units;
-            return this;
-        }
-
-        public DerivativePipelineAggregatorBuilder unit(DateHistogramInterval units) {
-            this.units = units.toString();
-            return this;
-        }
-
-        public String unit() {
-            return units;
+            this.xAxisUnits = xAxisUnits;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            ValueFormatter formatter;
-            if (format != null) {
-                formatter = ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                formatter = ValueFormatter.RAW;
-            }
-            Long xAxisUnits = null;
-            if (units != null) {
-                DateTimeUnit dateTimeUnit = DateHistogramAggregatorFactory.DATE_FIELD_UNITS.get(units);
-                if (dateTimeUnit != null) {
-                    xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
-                } else {
-                    TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
-                    if (timeValue != null) {
-                        xAxisUnits = timeValue.getMillis();
-                    }
-                }
-            }
             return new DerivativePipelineAggregator(name, bucketsPaths, formatter, gapPolicy, xAxisUnits, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories, List<PipelineAggregatorBuilder> pipelineAggregatoractories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
-            if (!(parent instanceof AbstractHistogramAggregatorFactory<?>)) {
+            if (!(parent instanceof HistogramAggregator.Factory)) {
                 throw new IllegalStateException("derivative aggregation [" + name
                         + "] must have a histogram or date_histogram as parent");
             } else {
-                AbstractHistogramAggregatorFactory<?> histoParent = (AbstractHistogramAggregatorFactory<?>) parent;
+                HistogramAggregator.Factory histoParent = (HistogramAggregator.Factory) parent;
                 if (histoParent.minDocCount() != 0) {
                     throw new IllegalStateException("parent histogram of derivative aggregation [" + name
                             + "] must have min_doc_count of 0");
@@ -246,61 +188,5 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            DerivativePipelineAggregatorBuilder factory = new DerivativePipelineAggregatorBuilder(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            if (in.readBoolean()) {
-                factory.gapPolicy = GapPolicy.readFrom(in);
-            }
-            factory.units = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            boolean hasGapPolicy = gapPolicy != null;
-            out.writeBoolean(hasGapPolicy);
-            if (hasGapPolicy) {
-                gapPolicy.writeTo(out);
-            }
-            out.writeOptionalString(units);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
-            }
-            if (gapPolicy != null) {
-                builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            }
-            if (units != null) {
-                builder.field(DerivativeParser.UNIT.getPreferredName(), units);
-            }
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            DerivativePipelineAggregatorBuilder other = (DerivativePipelineAggregatorBuilder) obj;
-            if (!Objects.equals(format, other.format)) {
-                return false;
-            }
-            if (!Objects.equals(gapPolicy, other.gapPolicy)) {
-                return false;
-            }
-            if (!Objects.equals(units, other.units)) {
-                return false;
-            }
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, units);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java
new file mode 100644
index 0000000..c291c63
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.having;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class BucketSelectorBuilder extends PipelineAggregatorBuilder<BucketSelectorBuilder> {
+
+    private GapPolicy gapPolicy;
+    private Script script;
+    private Map<String, String> bucketsPathsMap;
+
+    public BucketSelectorBuilder(String name) {
+        super(name, BucketSelectorPipelineAggregator.TYPE.name());
+    }
+
+    public BucketSelectorBuilder script(Script script) {
+        this.script = script;
+        return this;
+    }
+
+    public BucketSelectorBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator. The
+     * map given to this method must contain script variable name as keys with
+     * bucket paths values to the metrics to use for each variable.
+     */
+    public BucketSelectorBuilder setBucketsPathsMap(Map<String, String> bucketsPathsMap) {
+        this.bucketsPathsMap = bucketsPathsMap;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        if (script != null) {
+            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        }
+        if (gapPolicy != null) {
+            builder.field(BucketSelectorParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (bucketsPathsMap != null) {
+            builder.field(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
index 55467c9..e2623b5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
@@ -20,18 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,12 +47,12 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorBuilder parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -67,8 +66,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -82,8 +81,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -95,36 +94,26 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder factory = 
-                new BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder(reducerName, bucketsPathsMap, script);
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-
-    }
-
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder(null, Collections.emptyMap(), null);
+        return new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
index d14e7ab..1032d0f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
@@ -22,11 +22,9 @@ package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
@@ -35,9 +33,8 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -45,8 +42,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
@@ -139,99 +134,23 @@ public class BucketSelectorPipelineAggregator extends PipelineAggregator {
         bucketsPathsMap = (Map<String, String>) in.readGenericValue();
     }
 
-    public static class BucketSelectorPipelineAggregatorBuilder extends PipelineAggregatorBuilder {
+    public static class Factory extends PipelineAggregatorFactory {
 
         private Script script;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private GapPolicy gapPolicy;
         private Map<String, String> bucketsPathsMap;
 
-        public BucketSelectorPipelineAggregatorBuilder(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        public BucketSelectorPipelineAggregatorBuilder(String name, Script script, String... bucketsPaths) {
-            this(name, convertToBucketsPathMap(bucketsPaths), script);
-        }
-
-        private static Map<String, String> convertToBucketsPathMap(String[] bucketsPaths) {
-            Map<String, String> bucketsPathsMap = new HashMap<>();
-            for (int i = 0; i < bucketsPaths.length; i++) {
-                bucketsPathsMap.put("_value" + i, bucketsPaths[i]);
-            }
-            return bucketsPathsMap;
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public BucketSelectorPipelineAggregatorBuilder gapPolicy(GapPolicy gapPolicy) {
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
             return new BucketSelectorPipelineAggregator(name, bucketsPathsMap, script, gapPolicy, metaData);
         }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            BucketSelectorPipelineAggregatorBuilder factory = new BucketSelectorPipelineAggregatorBuilder(name, bucketsPathsMap, script);
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            BucketSelectorPipelineAggregatorBuilder other = (BucketSelectorPipelineAggregatorBuilder) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(gapPolicy, other.gapPolicy);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
new file mode 100644
index 0000000..b2dc718
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
@@ -0,0 +1,148 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.movavg;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A builder to create MovingAvg pipeline aggregations
+ */
+public class MovAvgBuilder extends PipelineAggregatorBuilder<MovAvgBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private MovAvgModelBuilder modelBuilder;
+    private Integer window;
+    private Integer predict;
+    private Boolean minimize;
+    private Map<String, Object> settings;
+
+    public MovAvgBuilder(String name) {
+        super(name, MovAvgPipelineAggregator.TYPE.name());
+    }
+
+    public MovAvgBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    /**
+     * Defines what should be done when a gap in the series is discovered
+     *
+     * @param gapPolicy A GapPolicy enum defining the selected policy
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets a MovAvgModelBuilder for the Moving Average.  The model builder is used to
+     * define what type of moving average you want to use on the series
+     *
+     * @param modelBuilder A MovAvgModelBuilder which has been prepopulated with settings
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder modelBuilder(MovAvgModelBuilder modelBuilder) {
+        this.modelBuilder = modelBuilder;
+        return this;
+    }
+
+    /**
+     * Sets the window size for the moving average.  This window will "slide" across the
+     * series, and the values inside that window will be used to calculate the moving avg value
+     *
+     * @param window Size of window
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder window(int window) {
+        this.window = window;
+        return this;
+    }
+
+    /**
+     * Sets the number of predictions that should be returned.  Each prediction will be spaced at
+     * the intervals specified in the histogram.  E.g "predict: 2" will return two new buckets at the
+     * end of the histogram with the predicted values.
+     *
+     * @param numPredictions Number of predictions to make
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder predict(int numPredictions) {
+        this.predict = numPredictions;
+        return this;
+    }
+
+    /**
+     * Determines if the model should be fit to the data using a cost
+     * minimizing algorithm.
+     *
+     * @param minimize If the model should be fit to the underlying data
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder minimize(boolean minimize) {
+        this.minimize = minimize;
+        return this;
+    }
+
+    /**
+     * The hash of settings that should be provided to the model when it is
+     * instantiated
+     */
+    public MovAvgBuilder settings(Map<String, Object> settings) {
+        this.settings = settings;
+        return this;
+    }
+
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (modelBuilder != null) {
+            modelBuilder.toXContent(builder, params);
+        }
+        if (window != null) {
+            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
+        }
+        if (predict != null) {
+            builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
+        }
+        if (minimize != null) {
+            builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
+        }
+        if (settings != null) {
+            builder.field(MovAvgParser.SETTINGS.getPreferredName(), settings);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
index 21ab1f3..5856735 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
@@ -20,15 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelParserMapper;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -57,18 +59,17 @@ public class MovAvgParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorBuilder parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
 
-        GapPolicy gapPolicy = null;
-        Integer window = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int window = 5;
         Map<String, Object> settings = null;
-        String model = null;
-        Integer predict = null;
+        String model = "simple";
+        int predict = 0;
         Boolean minimize = null;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -78,18 +79,20 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, WINDOW)) {
                     window = parser.intValue();
                     if (window <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive, "
-                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else if (context.parseFieldMatcher().match(currentFieldName, PREDICT)) {
                     predict = parser.intValue();
                     if (predict <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive integer."
-                                + "  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, FORMAT)) {
@@ -101,8 +104,8 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, MODEL)) {
                     model = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -113,72 +116,66 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, SETTINGS)) {
                     settings = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
                 if (context.parseFieldMatcher().match(currentFieldName, MINIMIZE)) {
                     minimize = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder factory = 
-                new MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        if (window != null) {
-            factory.window(window);
-        }
-        if (predict != null) {
-            factory.predict(predict);
+
+        MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
+        if (modelParser == null) {
+            throw new SearchParseException(context, "Unknown model [" + model + "] specified.  Valid options are:"
+                    + movAvgModelParserMapper.getAllNames().toString(), parser.getTokenLocation());
         }
-        if (model != null) {
-            MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
-            if (modelParser == null) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown model [" + model + "] specified.  Valid options are:" + movAvgModelParserMapper.getAllNames().toString());
-            }
 
-            MovAvgModel movAvgModel;
-            try {
-                movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
-            } catch (ParseException exception) {
-                throw new ParsingException(parser.getTokenLocation(), "Could not parse settings for model [" + model + "].", exception);
-            }
-            factory.model(movAvgModel);
+        MovAvgModel movAvgModel;
+        try {
+            movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
+        } catch (ParseException exception) {
+            throw new SearchParseException(context, "Could not parse settings for model [" + model + "].", null, exception);
         }
-        if (minimize != null) {
-            factory.minimize(minimize);
+
+        // If the user doesn't set a preference for cost minimization, ask what the model prefers
+        if (minimize == null) {
+            minimize = movAvgModel.minimizeByDefault();
+        } else if (minimize && !movAvgModel.canBeMinimized()) {
+            // If the user asks to minimize, but this model doesn't support it, throw exception
+            throw new SearchParseException(context, "The [" + model + "] model cannot be minimized.", null);
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder(null, null);
+
+        return new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, window, predict,
+                movAvgModel, minimize);
     }
 
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
index b134c77..4f7034b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
@@ -22,25 +22,21 @@ package org.elasticsearch.search.aggregations.pipeline.movavg;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.AbstractHistogramAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelStreams;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -50,7 +46,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.ListIterator;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -279,190 +274,43 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
 
     }
 
-    public static class MovAvgPipelineAggregatorBuilder extends PipelineAggregatorBuilder {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int window = 5;
-        private MovAvgModel model = new SimpleModel();
-        private int predict = 0;
-        private Boolean minimize;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int window;
+        private MovAvgModel model;
+        private int predict;
+        private boolean minimize;
 
-        public MovAvgPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private MovAvgPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy,
+                       int window, int predict, MovAvgModel model, boolean minimize) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public MovAvgPipelineAggregatorBuilder format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public MovAvgPipelineAggregatorBuilder gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         *
-         * @param window
-         *            Size of window
-         */
-        public MovAvgPipelineAggregatorBuilder window(int window) {
-            if (window <= 0) {
-                throw new IllegalArgumentException("[window] must be a positive integer: [" + name + "]");
-            }
             this.window = window;
-            return this;
-        }
-
-        /**
-         * Gets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         */
-        public int window() {
-            return window;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public MovAvgPipelineAggregatorBuilder modelBuilder(MovAvgModelBuilder model) {
-            this.model = model.build();
-            return this;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public MovAvgPipelineAggregatorBuilder model(MovAvgModel model) {
             this.model = model;
-            return this;
-        }
-
-        /**
-         * Gets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         */
-        public MovAvgModel model() {
-            return model;
-        }
-
-        /**
-         * Sets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         *
-         * @param predict
-         *            Number of predictions to make
-         */
-        public MovAvgPipelineAggregatorBuilder predict(int predict) {
-            if (predict <= 0) {
-                throw new IllegalArgumentException("predict must be greater than 0. Found [" + predict + "] in [" + name + "]");
-            }
             this.predict = predict;
-            return this;
-        }
-
-        /**
-         * Gets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         */
-        public int predict() {
-            return predict;
-        }
-
-        /**
-         * Sets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         *
-         * @param minimize
-         *            If the model should be fit to the underlying data
-         */
-        public MovAvgPipelineAggregatorBuilder minimize(boolean minimize) {
             this.minimize = minimize;
-            return this;
-        }
-
-        /**
-         * Gets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         */
-        public Boolean minimize() {
-            return minimize;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            // If the user doesn't set a preference for cost minimization, ask
-            // what the model prefers
-            boolean minimize = this.minimize == null ? model.minimizeByDefault() : this.minimize;
-            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, window, predict, model, minimize, metaData);
+            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, window, predict, model, minimize, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorBuilder> pipelineAggregatoractories) {
-            if (minimize != null && minimize && !model.canBeMinimized()) {
-                // If the user asks to minimize, but this model doesn't support
-                // it, throw exception
-                throw new IllegalStateException("The [" + model + "] model cannot be minimized for aggregation [" + name + "]");
-            }
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                List<PipelineAggregatorFactory> pipelineAggregatoractories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
-            if (!(parent instanceof AbstractHistogramAggregatorFactory<?>)) {
+            if (!(parent instanceof HistogramAggregator.Factory)) {
                 throw new IllegalStateException("moving average aggregation [" + name
                         + "] must have a histogram or date_histogram as parent");
             } else {
-                AbstractHistogramAggregatorFactory<?> histoParent = (AbstractHistogramAggregatorFactory<?>) parent;
+                HistogramAggregator.Factory histoParent = (HistogramAggregator.Factory) parent;
                 if (histoParent.minDocCount() != 0) {
                     throw new IllegalStateException("parent histogram of moving average aggregation [" + name
                             + "] must have min_doc_count of 0");
@@ -470,60 +318,5 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            model.toXContent(builder, params);
-            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
-            if (predict > 0) {
-                builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
-            }
-            if (minimize != null) {
-                builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
-            }
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            MovAvgPipelineAggregatorBuilder factory = new MovAvgPipelineAggregatorBuilder(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.window = in.readVInt();
-            factory.model = MovAvgModelStreams.read(in);
-            factory.predict = in.readVInt();
-            factory.minimize = in.readOptionalBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(window);
-            model.writeTo(out);
-            out.writeVInt(predict);
-            out.writeOptionalBoolean(minimize);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, window, model, predict, minimize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            MovAvgPipelineAggregatorBuilder other = (MovAvgPipelineAggregatorBuilder) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(window, other.window)
-                    && Objects.equals(model, other.model)
-                    && Objects.equals(predict, other.predict)
-                    && Objects.equals(minimize, other.minimize);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
index edbfa66..84de794 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
@@ -32,16 +32,13 @@ import java.text.ParseException;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a exponentially weighted moving average
  */
 public class EwmaModel extends MovAvgModel {
 
-    private static final EwmaModel PROTOTYPE = new EwmaModel();
     protected static final ParseField NAME_FIELD = new ParseField("ewma");
-    public static final double DEFAULT_ALPHA = 0.3;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -51,10 +48,6 @@ public class EwmaModel extends MovAvgModel {
      */
     private final double alpha;
 
-    public EwmaModel() {
-        this(DEFAULT_ALPHA);
-    }
-
     public EwmaModel(double alpha) {
         this.alpha = alpha;
     }
@@ -104,7 +97,7 @@ public class EwmaModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new EwmaModel(in.readDouble());
         }
 
         @Override
@@ -114,42 +107,11 @@ public class EwmaModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new EwmaModel(in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        EwmaModel other = (EwmaModel) obj;
-        return Objects.equals(alpha, other.alpha);
-    }
-
     public static class SingleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -161,7 +123,7 @@ public class EwmaModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
             checkUnrecognizedParams(settings);
             return new EwmaModel(alpha);
         }
@@ -170,7 +132,7 @@ public class EwmaModel extends MovAvgModel {
 
     public static class EWMAModelBuilder implements MovAvgModelBuilder {
 
-        private double alpha = DEFAULT_ALPHA;
+        private Double alpha;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -190,16 +152,13 @@ public class EwmaModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new EwmaModel(alpha);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
index 06ce050..fe0321b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
@@ -31,17 +31,13 @@ import java.io.IOException;
 import java.text.ParseException;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a doubly exponential weighted moving average
  */
 public class HoltLinearModel extends MovAvgModel {
 
-    private static final HoltLinearModel PROTOTYPE = new HoltLinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("holt");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -59,10 +55,6 @@ public class HoltLinearModel extends MovAvgModel {
      */
     private final double beta;
 
-    public HoltLinearModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA);
-    }
-
     public HoltLinearModel(double alpha, double beta) {
         this.alpha = alpha;
         this.beta = beta;
@@ -165,7 +157,7 @@ public class HoltLinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new HoltLinearModel(in.readDouble(), in.readDouble());
         }
 
         @Override
@@ -175,45 +167,12 @@ public class HoltLinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltLinearModel(in.readDouble(), in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
         out.writeDouble(beta);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltLinearModel other = (HoltLinearModel) obj;
-        return Objects.equals(alpha, other.alpha)
-                && Objects.equals(beta, other.beta);
-    }
-
     public static class DoubleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -225,8 +184,8 @@ public class HoltLinearModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
             checkUnrecognizedParams(settings);
             return new HoltLinearModel(alpha, beta);
         }
@@ -235,8 +194,8 @@ public class HoltLinearModel extends MovAvgModel {
     public static class HoltLinearModelBuilder implements MovAvgModelBuilder {
 
 
-        private double alpha = DEFAULT_ALPHA;
-        private double beta = DEFAULT_BETA;
+        private Double alpha;
+        private Double beta;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -268,17 +227,18 @@ public class HoltLinearModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
-            builder.field("beta", beta);
+
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
+
+            if (beta != null) {
+                builder.field("beta", beta);
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new HoltLinearModel(alpha, beta);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
index be8d2fb..afdd67e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
@@ -37,7 +37,6 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a triple exponential weighted moving average
@@ -45,13 +44,6 @@ import java.util.Objects;
 public class HoltWintersModel extends MovAvgModel {
 
     protected static final ParseField NAME_FIELD = new ParseField("holt_winters");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
-    public static final double DEFAULT_GAMMA = 0.3;
-    public static final int DEFAULT_PERIOD = 1;
-    public static final SeasonalityType DEFAULT_SEASONALITY_TYPE = SeasonalityType.ADDITIVE;
-    public static final boolean DEFAULT_PAD = false;
-    private static final HoltWintersModel PROTOTYPE = new HoltWintersModel();
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -167,9 +159,6 @@ public class HoltWintersModel extends MovAvgModel {
         }
     }
 
-    public HoltWintersModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA, DEFAULT_GAMMA, DEFAULT_PERIOD, DEFAULT_SEASONALITY_TYPE, DEFAULT_PAD);
-    }
 
     public HoltWintersModel(double alpha, double beta, double gamma, int period, SeasonalityType seasonalityType, boolean pad) {
         this.alpha = alpha;
@@ -180,7 +169,7 @@ public class HoltWintersModel extends MovAvgModel {
         this.pad = pad;
 
         // Only pad if we are multiplicative and padding is enabled
-        // The padding amount is not currently user-configurable...i dont see a reason to expose it?
+        // The padding amount is not currently user-configurable...i don't see a reason to expose it?
         this.padding = seasonalityType.equals(SeasonalityType.MULTIPLICATIVE) && pad ? 0.0000000001 : 0;
     }
 
@@ -284,8 +273,8 @@ public class HoltWintersModel extends MovAvgModel {
             s += vs[i];
             b += (vs[i + period] - vs[i]) / period;
         }
-        s /= period;
-        b /= period;
+        s /= (double) period;
+        b /= (double) period;
         last_s = s;
 
         // Calculate first seasonal
@@ -335,7 +324,14 @@ public class HoltWintersModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            double alpha = in.readDouble();
+            double beta = in.readDouble();
+            double gamma = in.readDouble();
+            int period = in.readVInt();
+            SeasonalityType type = SeasonalityType.readFrom(in);
+            boolean pad = in.readBoolean();
+
+            return new HoltWintersModel(alpha, beta, gamma, period, type, pad);
         }
 
         @Override
@@ -345,26 +341,6 @@ public class HoltWintersModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.field("gamma", gamma);
-        builder.field("period", period);
-        builder.field("pad", pad);
-        builder.field("type", seasonalityType.getName());
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltWintersModel(in.readDouble(), in.readDouble(), in.readDouble(), in.readVInt(), SeasonalityType.readFrom(in),
-                in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
@@ -375,28 +351,6 @@ public class HoltWintersModel extends MovAvgModel {
         out.writeBoolean(pad);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta, gamma, period, seasonalityType, pad);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltWintersModel other = (HoltWintersModel) obj;
-        return Objects.equals(alpha, other.alpha)
-                && Objects.equals(beta, other.beta)
-                && Objects.equals(gamma, other.gamma)
-                && Objects.equals(period, other.period)
-                && Objects.equals(seasonalityType, other.seasonalityType)
-                && Objects.equals(pad, other.pad);
-    }
-
     public static class HoltWintersModelParser extends AbstractModelParser {
 
         @Override
@@ -408,10 +362,10 @@ public class HoltWintersModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
-            double gamma = parseDoubleParam(settings, "gamma", DEFAULT_GAMMA);
-            int period = parseIntegerParam(settings, "period", DEFAULT_PERIOD);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
+            double gamma = parseDoubleParam(settings, "gamma", 0.3);
+            int period = parseIntegerParam(settings, "period", 1);
 
             if (windowSize < 2 * period) {
                 throw new ParseException("Field [window] must be at least twice as large as the period when " +
@@ -419,7 +373,7 @@ public class HoltWintersModel extends MovAvgModel {
                         + (2 * period), 0);
             }
 
-            SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
+            SeasonalityType seasonalityType = SeasonalityType.ADDITIVE;
 
             if (settings != null) {
                 Object value = settings.get("type");
@@ -443,12 +397,12 @@ public class HoltWintersModel extends MovAvgModel {
 
     public static class HoltWintersModelBuilder implements MovAvgModelBuilder {
 
-        private double alpha = DEFAULT_ALPHA;
-        private double beta = DEFAULT_BETA;
-        private double gamma = DEFAULT_GAMMA;
-        private int period = DEFAULT_PERIOD;
-        private SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
-        private Boolean pad = null;
+        private Double alpha;
+        private Double beta;
+        private Double gamma;
+        private Integer period;
+        private SeasonalityType seasonalityType;
+        private Boolean pad;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -500,24 +454,34 @@ public class HoltWintersModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
-            builder.field("beta", beta);
-            builder.field("gamma", gamma);
-            builder.field("period", period);
+
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
+
+            if (beta != null) {
+                builder.field("beta", beta);
+            }
+
+            if (gamma != null) {
+                builder.field("gamma", gamma);
+            }
+
+            if (period != null) {
+                builder.field("period", period);
+            }
+
             if (pad != null) {
                 builder.field("pad", pad);
             }
-            builder.field("type", seasonalityType.getName());
+
+            if (seasonalityType != null) {
+                builder.field("type", seasonalityType.getName());
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            boolean pad = this.pad == null ? (seasonalityType == SeasonalityType.MULTIPLICATIVE) : this.pad;
-            return new HoltWintersModel(alpha, beta, gamma, period, seasonalityType, pad);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
index bc3de8b..264a425 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
@@ -40,7 +40,6 @@ import java.util.Map;
  */
 public class LinearModel extends MovAvgModel {
 
-    private static final LinearModel PROTOTYPE = new LinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("linear");
 
 
@@ -86,7 +85,7 @@ public class LinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new LinearModel();
         }
 
         @Override
@@ -96,17 +95,6 @@ public class LinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new LinearModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -132,26 +120,5 @@ public class LinearModel extends MovAvgModel {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new LinearModel();
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
index 92f4615..4bfac9d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
@@ -22,8 +22,6 @@ package org.elasticsearch.search.aggregations.pipeline.movavg.models;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -31,7 +29,7 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
 
-public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent {
+public abstract class MovAvgModel {
 
     /**
      * Should this model be fit to the data via a cost minimizing algorithm by default?
@@ -118,21 +116,13 @@ public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent
      *
      * @param out   Output stream
      */
-    @Override
     public abstract void writeTo(StreamOutput out) throws IOException;
 
     /**
      * Clone the model, returning an exact copy
      */
-    @Override
     public abstract MovAvgModel clone();
 
-    @Override
-    public abstract int hashCode();
-
-    @Override
-    public abstract boolean equals(Object obj);
-
     /**
      * Abstract class which also provides some concrete parsing functionality.
      */
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
index 759c493..e491d12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
@@ -26,6 +26,4 @@ import org.elasticsearch.common.xcontent.ToXContent;
  * average models are used by the MovAvg aggregation
  */
 public interface MovAvgModelBuilder extends ToXContent {
-
-    public MovAvgModel build();
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
index 61f8c66..e0c7781 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
@@ -38,7 +38,6 @@ import java.util.Map;
  */
 public class SimpleModel extends MovAvgModel {
 
-    private static final SimpleModel PROTOTYPE = new SimpleModel();
     protected static final ParseField NAME_FIELD = new ParseField("simple");
 
 
@@ -79,7 +78,7 @@ public class SimpleModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new SimpleModel();
         }
 
         @Override
@@ -89,17 +88,6 @@ public class SimpleModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new SimpleModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -125,26 +113,5 @@ public class SimpleModel extends MovAvgModel {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new SimpleModel();
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java
new file mode 100644
index 0000000..052f3f0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.serialdiff;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class SerialDiffBuilder extends PipelineAggregatorBuilder<SerialDiffBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private Integer lag;
+
+    public SerialDiffBuilder(String name) {
+        super(name, SerialDiffPipelineAggregator.TYPE.name());
+    }
+
+    public SerialDiffBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public SerialDiffBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    public SerialDiffBuilder lag(Integer lag) {
+        this.lag = lag;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (lag != null) {
+            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
index f53daab..109cbcc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
@@ -20,17 +20,20 @@
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 public class SerialDiffParser implements PipelineAggregator.Parser {
 
     public static final ParseField FORMAT = new ParseField("format");
@@ -43,13 +46,13 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorBuilder parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
-        Integer lag = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int lag = 1;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -62,21 +65,20 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, GAP_POLICY)) {
                     gapPolicy = GapPolicy.parse(context, parser.text(), parser.getTokenLocation());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
                 if (context.parseFieldMatcher().match(currentFieldName, LAG)) {
                     lag = parser.intValue(true);
                     if (lag <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Lag must be a positive, non-zero integer.  Value supplied was" +
+                        throw new SearchParseException(context, "Lag must be a positive, non-zero integer.  Value supplied was" +
                                 lag + " in [" + reducerName + "]: ["
-                                        + currentFieldName + "].");
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
                 }  else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -87,37 +89,28 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].",
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
                         parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for derivative aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder factory = 
-                new SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder(reducerName, bucketsPaths[0]);
-        if (lag != null) {
-            factory.lag(lag);
-        }
+        ValueFormatter formatter;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        }  else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorBuilder getFactoryPrototype() {
-        return new SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder(null, null);
+        return new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths, formatter, gapPolicy, lag);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
index 811461b..5df97d3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
@@ -23,18 +23,15 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -42,10 +39,10 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
 public class SerialDiffPipelineAggregator extends PipelineAggregator {
@@ -145,117 +142,22 @@ public class SerialDiffPipelineAggregator extends PipelineAggregator {
         out.writeVInt(lag);
     }
 
-    public static class SerialDiffPipelineAggregatorBuilder extends PipelineAggregatorBuilder {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int lag = 1;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int lag;
 
-        public SerialDiffPipelineAggregatorBuilder(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private SerialDiffPipelineAggregatorBuilder(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, @Nullable ValueFormatter formatter, GapPolicy gapPolicy, int lag) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the lag to use when calculating the serial difference.
-         */
-        public SerialDiffPipelineAggregatorBuilder lag(int lag) {
-            if (lag <= 0) {
-                throw new IllegalArgumentException("[lag] must be a positive integer: [" + name + "]");
-            }
-            this.lag = lag;
-            return this;
-        }
-
-        /**
-         * Gets the lag to use when calculating the serial difference.
-         */
-        public int lag() {
-            return lag;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public SerialDiffPipelineAggregatorBuilder format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public SerialDiffPipelineAggregatorBuilder gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.lag = lag;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, lag, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorBuilder doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            SerialDiffPipelineAggregatorBuilder factory = new SerialDiffPipelineAggregatorBuilder(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.lag = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(lag);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, lag);
-        }
-        @Override
-        protected boolean doEquals(Object obj) {
-            SerialDiffPipelineAggregatorBuilder other = (SerialDiffPipelineAggregatorBuilder) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(lag, other.lag);
+            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, lag, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
deleted file mode 100644
index e7575a4..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.joda.time.DateTimeZone;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- *
- */
-public abstract class AbstractValuesSourceParser<VS extends ValuesSource>
-        implements Aggregator.Parser {
-    static final ParseField TIME_ZONE = new ParseField("time_zone");
-
-    public abstract static class AnyValuesSourceParser extends AbstractValuesSourceParser<ValuesSource> {
-
-        protected AnyValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.ANY, null);
-        }
-    }
-
-    public abstract static class NumericValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Numeric> {
-
-        protected NumericValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-            super(scriptable, formattable, timezoneAware, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-    }
-
-    public abstract static class BytesValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Bytes> {
-
-        protected BytesValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.BYTES, ValueType.STRING);
-        }
-    }
-
-    public abstract static class GeoPointValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.GeoPoint> {
-
-        protected GeoPointValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
-    }
-
-    private boolean scriptable = true;
-    private boolean formattable = false;
-    private boolean timezoneAware = false;
-    private ValuesSourceType valuesSourceType = null;
-    private ValueType targetValueType = null;
-
-    private AbstractValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware, ValuesSourceType valuesSourceType,
-            ValueType targetValueType) {
-        this.timezoneAware = timezoneAware;
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-        this.scriptable = scriptable;
-        this.formattable = formattable;
-    }
-
-    @Override
-    public final ValuesSourceAggregatorBuilder<VS, ?> parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-
-        String field = null;
-        Script script = null;
-        ValueType valueType = null;
-        String format = null;
-        Object missing = null;
-        DateTimeZone timezone = null;
-        Map<ParseField, Object> otherOptions = new HashMap<>();
-
-        XContentParser.Token token;
-        String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if ("missing".equals(currentFieldName) && token.isValue()) {
-                missing = parser.objectText();
-            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    timezone = DateTimeZone.forID(parser.text());
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    timezone = DateTimeZone.forOffsetHours(parser.intValue());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("field".equals(currentFieldName)) {
-                    field = parser.text();
-                } else if (formattable && "format".equals(currentFieldName)) {
-                    format = parser.text();
-                } else if (scriptable) {
-                    if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
-                        valueType = ValueType.resolveForScript(parser.text());
-                        if (targetValueType != null && valueType.isNotA(targetValueType)) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    type() + " aggregation [" + aggregationName + "] was configured with an incompatible value type ["
-                                            + valueType + "]. [" + type() + "] aggregation can only work on value of type ["
-                                            + targetValueType + "]");
-                        }
-                    } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                    }
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (scriptable && token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
-                    script = Script.parse(parser, context.parseFieldMatcher());
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-            }
-        }
-
-        ValuesSourceAggregatorBuilder<VS, ?> factory = createFactory(aggregationName, this.valuesSourceType, this.targetValueType,
-                otherOptions);
-        factory.field(field);
-        factory.script(script);
-        factory.valueType(valueType);
-        factory.format(format);
-        factory.missing(missing);
-        factory.timeZone(timezone);
-        return factory;
-    }
-
-    /**
-     * Creates a {@link ValuesSourceAggregatorBuilder} from the information
-     * gathered by the subclass. Options parsed in
-     * {@link AbstractValuesSourceParser} itself will be added to the factory
-     * after it has been returned by this method.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param valuesSourceType
-     *            the type of the {@link ValuesSource}
-     * @param targetValueType
-     *            the target type of the final value output by the aggregation
-     * @param otherOptions
-     *            a {@link Map} containing the extra options parsed by the
-     *            {@link #token(String, String, org.elasticsearch.common.xcontent.XContentParser.Token,
-     *             XContentParser, ParseFieldMatcher, Map)}
-     *            method
-     * @return the created factory
-     */
-    protected abstract ValuesSourceAggregatorBuilder<VS, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions);
-
-    /**
-     * Allows subclasses of {@link AbstractValuesSourceParser} to parse extra
-     * parameters and store them in a {@link Map} which will later be passed to
-     * {@link #createFactory(String, ValuesSourceType, ValueType, Map)}.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param currentFieldName
-     *            the name of the current field being parsed
-     * @param token
-     *            the current token for the parser
-     * @param parser
-     *            the parser
-     * @param parseFieldMatcher
-     *            the {@link ParseFieldMatcher} to use to match field names
-     * @param otherOptions
-     *            a {@link Map} of options to be populated by successive calls
-     *            to this method which will then be passed to the
-     *            {@link #createFactory(String, ValuesSourceType, ValueType, Map)}
-     *            method
-     * @return <code>true</code> if the current token was correctly parsed,
-     *         <code>false</code> otherwise
-     * @throws IOException
-     *             if an error occurs whilst parsing
-     */
-    protected abstract boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException;
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
index 14e8481..ee91782 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
@@ -70,11 +70,13 @@ public class AggregationContext {
             if (config.missing == null) {
                 // otherwise we will have values because of the missing value
                 vs = null;
-            } else if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            } else if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Numeric.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+            } else if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.GeoPoint.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.ANY || config.valueSourceType == ValuesSourceType.BYTES) {
+            } else if (ValuesSource.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.WithOrdinals.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Bytes.EMPTY;
             } else {
                 throw new SearchParseException(searchContext, "Can't deal with unmapped ValuesSource type " + config.valueSourceType, null);
@@ -130,20 +132,19 @@ public class AggregationContext {
      */
     private <VS extends ValuesSource> VS originalValuesSource(ValuesSourceConfig<VS> config) throws IOException {
         if (config.fieldContext == null) {
-            if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) numericScript(config);
             }
-            if (config.valueSourceType == ValuesSourceType.BYTES) {
+            if (ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) bytesScript(config);
             }
-            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.name()
-                    + "] is not supported by scripts");
+            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.getSimpleName() + "] is not supported by scripts");
         }
 
-        if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+        if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) numericField(config);
         }
-        if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+        if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) geoPointField(config);
         }
         // falling back to bytes values
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
index fd2f363..3dfab20 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
@@ -19,39 +19,41 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
 public class GeoPointParser {
 
+    private final String aggName;
     private final InternalAggregation.Type aggType;
+    private final SearchContext context;
     private final ParseField field;
 
-    public GeoPointParser(InternalAggregation.Type aggType, ParseField field) {
+    GeoPoint point;
+
+    public GeoPointParser(String aggName, InternalAggregation.Type aggType, SearchContext context, ParseField field) {
+        this.aggName = aggName;
         this.aggType = aggType;
+        this.context = context;
         this.field = field;
     }
 
-    public boolean token(String aggName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (!parseFieldMatcher.match(currentFieldName, field)) {
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if (!context.parseFieldMatcher().match(currentFieldName, field)) {
             return false;
         }
         if (token == XContentParser.Token.VALUE_STRING) {
-            GeoPoint point = new GeoPoint();
+            point = new GeoPoint();
             point.resetFromString(parser.text());
-            otherOptions.put(field, point);
             return true;
         }
         if (token == XContentParser.Token.START_ARRAY) {
@@ -63,12 +65,12 @@ public class GeoPointParser {
                 } else if (Double.isNaN(lat)) {
                     lat = parser.doubleValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "malformed [" + currentFieldName + "] geo point array in ["
-                            + aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]");
+                    throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point array in [" +
+                            aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]", 
+                            parser.getTokenLocation());
                 }
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         if (token == XContentParser.Token.START_OBJECT) {
@@ -86,15 +88,17 @@ public class GeoPointParser {
                 }
             }
             if (Double.isNaN(lat) || Double.isNaN(lon)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " + "missing in ["
-                                + aggName + "] " + aggType + " aggregation");
+                throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " +
+                        "missing in [" + aggName + "] " + aggType + " aggregation", parser.getTokenLocation());
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         return false;
     }
 
+    public GeoPoint geoPoint() {
+        return point;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
index 688e447..0ae99b4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
@@ -19,35 +19,26 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 
-import java.io.IOException;
-
 /**
  *
  */
-public enum ValueType implements Writeable<ValueType> {
+public enum ValueType {
 
     @Deprecated
-    ANY((byte) 0, "any", "any", ValuesSourceType.ANY, IndexFieldData.class, ValueFormat.RAW), 
-    STRING((byte) 1, "string", "string", ValuesSourceType.BYTES,
-            IndexFieldData.class,
+    ANY("any", ValuesSource.class, IndexFieldData.class, ValueFormat.RAW), STRING("string", ValuesSource.Bytes.class, IndexFieldData.class,
             ValueFormat.RAW),
-    LONG((byte) 2, "byte|short|integer|long", "long",
-                    ValuesSourceType.NUMERIC,
-            IndexNumericFieldData.class, ValueFormat.RAW) {
+    LONG("byte|short|integer|long", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DOUBLE((byte) 3, "float|double", "double", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    DOUBLE("float|double", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
@@ -58,31 +49,31 @@ public enum ValueType implements Writeable<ValueType> {
             return true;
         }
     },
-    NUMBER((byte) 4, "number", "number", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMBER("number", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DATE((byte) 5, "date", "date", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
+    DATE("date", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    IP((byte) 6, "ip", "ip", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.IPv4) {
+    IP("ip", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.IPv4) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    NUMERIC((byte) 7, "numeric", "numeric", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMERIC("numeric", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    GEOPOINT((byte) 8, "geo_point", "geo_point", ValuesSourceType.GEOPOINT, IndexGeoPointFieldData.class, ValueFormat.RAW) {
+    GEOPOINT("geo_point", ValuesSource.GeoPoint.class, IndexGeoPointFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isGeoPoint() {
             return true;
@@ -90,17 +81,12 @@ public enum ValueType implements Writeable<ValueType> {
     };
 
     final String description;
-    final ValuesSourceType valuesSourceType;
+    final Class<? extends ValuesSource> valuesSourceType;
     final Class<? extends IndexFieldData> fieldDataType;
     final ValueFormat defaultFormat;
-    private final byte id;
-    private String preferredName;
 
-    private ValueType(byte id, String description, String preferredName, ValuesSourceType valuesSourceType, Class<? extends IndexFieldData> fieldDataType,
-            ValueFormat defaultFormat) {
-        this.id = id;
+    private ValueType(String description, Class<? extends ValuesSource> valuesSourceType, Class<? extends IndexFieldData> fieldDataType, ValueFormat defaultFormat) {
         this.description = description;
-        this.preferredName = preferredName;
         this.valuesSourceType = valuesSourceType;
         this.fieldDataType = fieldDataType;
         this.defaultFormat = defaultFormat;
@@ -110,11 +96,7 @@ public enum ValueType implements Writeable<ValueType> {
         return description;
     }
 
-    public String getPreferredName() {
-        return preferredName;
-    }
-    
-    public ValuesSourceType getValuesSourceType() {
+    public Class<? extends ValuesSource> getValuesSourceType() {
         return valuesSourceType;
     }
 
@@ -123,7 +105,7 @@ public enum ValueType implements Writeable<ValueType> {
     }
 
     public boolean isA(ValueType valueType) {
-        return valueType.valuesSourceType == valuesSourceType &&
+        return valueType.valuesSourceType.isAssignableFrom(valuesSourceType) &&
                 valueType.fieldDataType.isAssignableFrom(fieldDataType);
     }
 
@@ -167,20 +149,4 @@ public enum ValueType implements Writeable<ValueType> {
     public String toString() {
         return description;
     }
-
-    @Override
-    public ValueType readFrom(StreamInput in) throws IOException {
-        byte id = in.readByte();
-        for (ValueType valueType : values()) {
-            if (id == valueType.id) {
-                return valueType;
-            }
-        }
-        throw new IOException("No valueType found for id [" + id + "]");
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeByte(id);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java
deleted file mode 100644
index 999b35f..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java
+++ /dev/null
@@ -1,407 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.fielddata.IndexNumericFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.NumberFieldMapper;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.aggregations.AggregationInitializationException;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.internal.SearchContext;
-import org.joda.time.DateTimeZone;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Objects;
-
-/**
- *
- */
-public abstract class ValuesSourceAggregatorBuilder<VS extends ValuesSource, AB extends ValuesSourceAggregatorBuilder<VS, AB>>
-        extends AggregatorBuilder<AB> {
-
-    public static abstract class LeafOnly<VS extends ValuesSource, AB extends ValuesSourceAggregatorBuilder<VS, AB>>
-            extends ValuesSourceAggregatorBuilder<VS, AB> {
-
-        protected LeafOnly(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-            super(name, type, valuesSourceType, targetValueType);
-        }
-
-        @Override
-        public AB subAggregations(Builder subFactories) {
-            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
-        }
-    }
-
-    private final ValuesSourceType valuesSourceType;
-    private final ValueType targetValueType;
-    private String field = null;
-    private Script script = null;
-    private ValueType valueType = null;
-    private String format = null;
-    private Object missing = null;
-    private DateTimeZone timeZone;
-    protected ValuesSourceConfig<VS> config;
-
-    protected ValuesSourceAggregatorBuilder(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-        super(name, type);
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-    }
-
-    /**
-     * Sets the field to use for this aggregation.
-     */
-    public AB field(String field) {
-        this.field = field;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the field to use for this aggregation.
-     */
-    public String field() {
-        return field;
-    }
-
-    /**
-     * Sets the script to use for this aggregation.
-     */
-    public AB script(Script script) {
-        this.script = script;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the script to use for this aggregation.
-     */
-    public Script script() {
-        return script;
-    }
-
-    /**
-     * Sets the {@link ValueType} for the value produced by this aggregation
-     */
-    public AB valueType(ValueType valueType) {
-        this.valueType = valueType;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the {@link ValueType} for the value produced by this aggregation
-     */
-    public ValueType valueType() {
-        return valueType;
-    }
-
-    /**
-     * Sets the format to use for the output of the aggregation.
-     */
-    public AB format(String format) {
-        this.format = format;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the format to use for the output of the aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    /**
-     * Sets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public AB missing(Object missing) {
-        this.missing = missing;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public Object missing() {
-        return missing;
-    }
-
-    /**
-     * Sets the time zone to use for this aggregation
-     */
-    public AB timeZone(DateTimeZone timeZone) {
-        this.timeZone = timeZone;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the time zone to use for this aggregation
-     */
-    public DateTimeZone timeZone() {
-        return timeZone;
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorFactory<VS, ?> doBuild(AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder) throws IOException {
-        ValuesSourceConfig<VS> config = resolveConfig(context);
-        ValuesSourceAggregatorFactory<VS, ?> factory = innerBuild(context, config, parent, subFactoriesBuilder);
-        return factory;
-    }
-
-    protected ValuesSourceConfig<VS> resolveConfig(AggregationContext context) {
-        ValuesSourceConfig<VS> config = config(context);
-        return config;
-    }
-
-    protected abstract ValuesSourceAggregatorFactory<VS, ?> innerBuild(AggregationContext context, ValuesSourceConfig<VS> config,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder) throws IOException;
-
-    public ValuesSourceConfig<VS> config(AggregationContext context) {
-
-        ValueType valueType = this.valueType != null ? this.valueType : targetValueType;
-
-        if (field == null) {
-            if (script == null) {
-                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSourceType.ANY);
-                config.format = resolveFormat(null, valueType);
-                return config;
-            }
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            if (valuesSourceType == null || valuesSourceType == ValuesSourceType.ANY) {
-                // the specific value source type is undefined, but for scripts,
-                // we need to have a specific value source
-                // type to know how to handle the script values, so we fallback
-                // on Bytes
-                valuesSourceType = ValuesSourceType.BYTES;
-            }
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
-            config.missing = missing;
-            config.timeZone = timeZone;
-            config.format = resolveFormat(format, valueType);
-            config.script = createScript(script, context.searchContext());
-            config.scriptValueType = valueType;
-            return config;
-        }
-
-        MappedFieldType fieldType = context.searchContext().smartNameFieldType(field);
-        if (fieldType == null) {
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
-            config.missing = missing;
-            config.timeZone = timeZone;
-            config.format = resolveFormat(format, valueType);
-            config.unmapped = true;
-            if (valueType != null) {
-                // todo do we really need this for unmapped?
-                config.scriptValueType = valueType;
-            }
-            return config;
-        }
-
-        IndexFieldData<?> indexFieldData = context.searchContext().fieldData().getForField(fieldType);
-
-        ValuesSourceConfig config;
-        if (valuesSourceType == ValuesSourceType.ANY) {
-            if (indexFieldData instanceof IndexNumericFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.NUMERIC);
-            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.GEOPOINT);
-            } else {
-                config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            }
-        } else {
-            config = new ValuesSourceConfig(valuesSourceType);
-        }
-
-        config.fieldContext = new FieldContext(field, indexFieldData, fieldType);
-        config.missing = missing;
-        config.timeZone = timeZone;
-        config.script = createScript(script, context.searchContext());
-        config.format = resolveFormat(format, this.timeZone, fieldType);
-        return config;
-    }
-
-    private SearchScript createScript(Script script, SearchContext context) {
-        return script == null ? null
-                : context.scriptService().search(context.lookup(), script, ScriptContext.Standard.AGGS, Collections.emptyMap());
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
-        if (valueType == null) {
-            return ValueFormat.RAW; // we can't figure it out
-        }
-        ValueFormat valueFormat = valueType.defaultFormat;
-        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
-            return ((ValueFormat.Patternable) valueFormat).create(format);
-        }
-        return valueFormat;
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone, MappedFieldType fieldType) {
-        if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper(
-                    (DateFieldMapper.DateFieldType) fieldType, timezone);
-        }
-        if (fieldType instanceof IpFieldMapper.IpFieldType) {
-            return ValueFormat.IPv4;
-        }
-        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
-            return ValueFormat.BOOLEAN;
-        }
-        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
-        }
-        return ValueFormat.RAW;
-    }
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        valuesSourceType.writeTo(out);
-        boolean hasTargetValueType = targetValueType != null;
-        out.writeBoolean(hasTargetValueType);
-        if (hasTargetValueType) {
-            targetValueType.writeTo(out);
-        }
-        innerWriteTo(out);
-        out.writeOptionalString(field);
-        boolean hasScript = script != null;
-        out.writeBoolean(hasScript);
-        if (hasScript) {
-            script.writeTo(out);
-        }
-        boolean hasValueType = valueType != null;
-        out.writeBoolean(hasValueType);
-        if (hasValueType) {
-            valueType.writeTo(out);
-        }
-        out.writeOptionalString(format);
-        out.writeGenericValue(missing);
-        boolean hasTimeZone = timeZone != null;
-        out.writeBoolean(hasTimeZone);
-        if (hasTimeZone) {
-            out.writeString(timeZone.getID());
-        }
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final AB doReadFrom(String name, StreamInput in) throws IOException {
-        ValuesSourceType valuesSourceType = ValuesSourceType.ANY.readFrom(in);
-        ValueType targetValueType = null;
-        if (in.readBoolean()) {
-            targetValueType = ValueType.STRING.readFrom(in);
-        }
-        ValuesSourceAggregatorBuilder<VS, AB> factory = innerReadFrom(name, valuesSourceType, targetValueType, in);
-        factory.field = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.script = Script.readScript(in);
-        }
-        if (in.readBoolean()) {
-            factory.valueType = ValueType.STRING.readFrom(in);
-        }
-        factory.format = in.readOptionalString();
-        factory.missing = in.readGenericValue();
-        if (in.readBoolean()) {
-            factory.timeZone = DateTimeZone.forID(in.readString());
-        }
-        return (AB) factory;
-    }
-
-    protected abstract ValuesSourceAggregatorBuilder<VS, AB> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException;
-
-    @Override
-    public final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (field != null) {
-            builder.field("field", field);
-        }
-        if (script != null) {
-            builder.field("script", script);
-        }
-        if (missing != null) {
-            builder.field("missing", missing);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        if (valueType != null) {
-            builder.field("value_type", valueType.getPreferredName());
-        }
-        doXContentBody(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(field, format, missing, script, targetValueType, timeZone, valueType, valuesSourceType,
-                innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        ValuesSourceAggregatorBuilder<?, ?> other = (ValuesSourceAggregatorBuilder<?, ?>) obj;
-        if (!Objects.equals(field, other.field))
-            return false;
-        if (!Objects.equals(format, other.format))
-            return false;
-        if (!Objects.equals(missing, other.missing))
-            return false;
-        if (!Objects.equals(script, other.script))
-            return false;
-        if (!Objects.equals(targetValueType, other.targetValueType))
-            return false;
-        if (!Objects.equals(timeZone, other.timeZone))
-            return false;
-        if (!Objects.equals(valueType, other.valueType))
-            return false;
-        if (!Objects.equals(valuesSourceType, other.valuesSourceType))
-            return false;
-        return innerEquals(obj);
-    }
-
-    protected abstract boolean innerEquals(Object obj);
-}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
index c273430..d0eaec2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
@@ -16,50 +16,88 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations.support;
 
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
+import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.joda.time.DateTimeZone;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF extends ValuesSourceAggregatorFactory<VS, AF>>
-        extends AggregatorFactory<AF> {
+/**
+ *
+ */
+public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource> extends AggregatorFactory {
+
+    public static abstract class LeafOnly<VS extends ValuesSource> extends ValuesSourceAggregatorFactory<VS> {
+
+        protected LeafOnly(String name, String type, ValuesSourceConfig<VS> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
+        }
+
+        @Override
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
+            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
+        }
+    }
 
     protected ValuesSourceConfig<VS> config;
 
-    public ValuesSourceAggregatorFactory(String name, Type type, ValuesSourceConfig<VS> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactoriesBuilder, metaData);
+    protected ValuesSourceAggregatorFactory(String name, String type, ValuesSourceConfig<VS> config) {
+        super(name, type);
         this.config = config;
     }
 
-    public DateTimeZone timeZone() {
-        return config.timeZone;
-        }
-
     @Override
-    public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket,
+    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
         VS vs = context.valuesSource(config, context.searchContext());
         if (vs == null) {
-            return createUnmapped(parent, pipelineAggregators, metaData);
+            return createUnmapped(context, parent, pipelineAggregators, metaData);
+        }
+        return doCreateInternal(vs, context, parent, collectsFromSingleBucket, pipelineAggregators, metaData);
+    }
+
+    @Override
+    public void doValidate() {
+        if (config == null || !config.valid()) {
+            resolveValuesSourceConfigFromAncestors(name, parent, config.valueSourceType());
         }
-        return doCreateInternal(vs, parent, collectsFromSingleBucket, pipelineAggregators, metaData);
     }
 
-    protected abstract Aggregator createUnmapped(Aggregator parent,
+    protected abstract Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException;
 
-    protected abstract Aggregator doCreateInternal(VS valuesSource, Aggregator parent,
+    protected abstract Aggregator doCreateInternal(VS valuesSource, AggregationContext aggregationContext, Aggregator parent,
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException;
 
-}
\ No newline at end of file
+    private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, Class<VS> requiredValuesSourceType) {
+        ValuesSourceConfig config;
+        while (parent != null) {
+            if (parent instanceof ValuesSourceAggregatorFactory) {
+                config = ((ValuesSourceAggregatorFactory) parent).config;
+                if (config != null && config.valid()) {
+                    if (requiredValuesSourceType == null || requiredValuesSourceType.isAssignableFrom(config.valueSourceType)) {
+                        ValueFormat format = config.format;
+                        this.config = config;
+                        // if the user explicitly defined a format pattern, we'll do our best to keep it even when we inherit the
+                        // value source form one of the ancestor aggregations
+                        if (this.config.formatPattern != null && format != null && format instanceof ValueFormat.Patternable) {
+                            this.config.format = ((ValueFormat.Patternable) format).create(this.config.formatPattern);
+                        }
+                        return;
+                    }
+                }
+            }
+            parent = parent.parent();
+        }
+        throw new AggregationExecutionException("could not find the appropriate value context to perform aggregation [" + aggName + "]");
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
index 79e9efd..a831f2f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
@@ -22,14 +22,13 @@ import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
-import org.joda.time.DateTimeZone;
 
 /**
  *
  */
 public class ValuesSourceConfig<VS extends ValuesSource> {
 
-    final ValuesSourceType valueSourceType;
+    final Class<VS> valueSourceType;
     FieldContext fieldContext;
     SearchScript script;
     ValueType scriptValueType;
@@ -37,13 +36,12 @@ public class ValuesSourceConfig<VS extends ValuesSource> {
     String formatPattern;
     ValueFormat format = ValueFormat.RAW;
     Object missing;
-    DateTimeZone timeZone;
 
-    public ValuesSourceConfig(ValuesSourceType valueSourceType) {
+    public ValuesSourceConfig(Class<VS> valueSourceType) {
         this.valueSourceType = valueSourceType;
     }
 
-    public ValuesSourceType valueSourceType() {
+    public Class<VS> valueSourceType() {
         return valueSourceType;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
new file mode 100644
index 0000000..fced5fd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
@@ -0,0 +1,297 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.support;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.mapper.core.NumberFieldMapper;
+import org.elasticsearch.index.mapper.ip.IpFieldMapper;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptParameterParser;
+import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.internal.SearchContext;
+import org.joda.time.DateTimeZone;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ *
+ */
+public class ValuesSourceParser<VS extends ValuesSource> {
+
+    static final ParseField TIME_ZONE = new ParseField("time_zone");
+
+    public static Builder any(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.class);
+    }
+
+    public static Builder<ValuesSource.Numeric> numeric(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.Numeric.class).targetValueType(ValueType.NUMERIC);
+    }
+
+    public static Builder<ValuesSource.Bytes> bytes(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.Bytes.class).targetValueType(ValueType.STRING);
+    }
+
+    public static Builder<ValuesSource.GeoPoint> geoPoint(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.GeoPoint.class).targetValueType(ValueType.GEOPOINT).scriptable(false);
+    }
+
+    public static class Input {
+        private String field = null;
+        private Script script = null;
+        @Deprecated
+        private Map<String, Object> params = null; // TODO Remove in 3.0
+        private ValueType valueType = null;
+        private String format = null;
+        private Object missing = null;
+        private DateTimeZone timezone = DateTimeZone.UTC;
+
+        public DateTimeZone timezone() {
+            return this.timezone;
+        }
+    }
+
+    private final String aggName;
+    private final InternalAggregation.Type aggType;
+    private final SearchContext context;
+    private final Class<VS> valuesSourceType;
+
+    private boolean scriptable = true;
+    private boolean formattable = false;
+    private boolean timezoneAware = false;
+    private ValueType targetValueType = null;
+    private ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
+
+    private Input input = new Input();
+
+    private ValuesSourceParser(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
+        this.aggName = aggName;
+        this.aggType = aggType;
+        this.context = context;
+        this.valuesSourceType = valuesSourceType;
+    }
+
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if ("missing".equals(currentFieldName) && token.isValue()) {
+            input.missing = parser.objectText();
+            return true;
+        }
+        if (token == XContentParser.Token.VALUE_STRING) {
+            if ("field".equals(currentFieldName)) {
+                input.field = parser.text();
+            } else if (formattable && "format".equals(currentFieldName)) {
+                input.format = parser.text();
+            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
+                input.timezone = DateTimeZone.forID(parser.text());
+            } else if (scriptable) {
+                if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
+                    input.valueType = ValueType.resolveForScript(parser.text());
+                    if (targetValueType != null && input.valueType.isNotA(targetValueType)) {
+                        throw new SearchParseException(context, aggType.name() + " aggregation [" + aggName +
+                                "] was configured with an incompatible value type [" + input.valueType + "]. [" + aggType +
+                                "] aggregation can only work on value of type [" + targetValueType + "]",
+                                parser.getTokenLocation());
+                    }
+                } else if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
+                    return false;
+                }
+                return true;
+            } else {
+                return false;
+            }
+            return true;
+        }
+        if (token == XContentParser.Token.VALUE_NUMBER) {
+            if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
+                input.timezone = DateTimeZone.forOffsetHours(parser.intValue());
+            } else {
+                return false;
+            }
+            return true;
+        }
+        if (scriptable && token == XContentParser.Token.START_OBJECT) {
+            if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
+                input.script = Script.parse(parser, context.parseFieldMatcher());
+                return true;
+            } else if ("params".equals(currentFieldName)) {
+                input.params = parser.map();
+                return true;
+            }
+            return false;
+        }
+
+        return false;
+    }
+
+    public ValuesSourceConfig<VS> config() {
+
+        if (input.script == null) { // Didn't find anything using the new API so try using the old one instead
+            ScriptParameterValue scriptValue = scriptParameterParser.getDefaultScriptParameterValue();
+            if (scriptValue != null) {
+                if (input.params == null) {
+                    input.params = new HashMap<>();
+                }
+                input.script = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), input.params);
+            }
+        }
+
+        ValueType valueType = input.valueType != null ? input.valueType : targetValueType;
+
+        if (input.field == null) {
+            if (input.script == null) {
+                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSource.class);
+                config.format = resolveFormat(null, valueType);
+                return config;
+            }
+            Class valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            if (valuesSourceType == null || valuesSourceType == ValuesSource.class) {
+                // the specific value source type is undefined, but for scripts, we need to have a specific value source
+                // type to know how to handle the script values, so we fallback on Bytes
+                valuesSourceType = ValuesSource.Bytes.class;
+            }
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.script = createScript();
+            config.scriptValueType = valueType;
+            return config;
+        }
+
+        MappedFieldType fieldType = context.smartNameFieldType(input.field);
+        if (fieldType == null) {
+            Class<VS> valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.unmapped = true;
+            if (valueType != null) {
+                // todo do we really need this for unmapped?
+                config.scriptValueType = valueType;
+            }
+            return config;
+        }
+
+        IndexFieldData<?> indexFieldData = context.fieldData().getForField(fieldType);
+
+        ValuesSourceConfig config;
+        if (valuesSourceType == ValuesSource.class) {
+            if (indexFieldData instanceof IndexNumericFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.Numeric.class);
+            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.GeoPoint.class);
+            } else {
+                config = new ValuesSourceConfig<>(ValuesSource.Bytes.class);
+            }
+        } else {
+            config = new ValuesSourceConfig(valuesSourceType);
+        }
+
+        config.fieldContext = new FieldContext(input.field, indexFieldData, fieldType);
+        config.missing = input.missing;
+        config.script = createScript();
+        config.format = resolveFormat(input.format, input.timezone, fieldType);
+        return config;
+    }
+
+    private SearchScript createScript() {
+        return input.script == null ? null : context.scriptService().search(context.lookup(), input.script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
+        if (valueType == null) {
+            return ValueFormat.RAW; // we can't figure it out
+        }
+        ValueFormat valueFormat = valueType.defaultFormat;
+        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
+            return ((ValueFormat.Patternable) valueFormat).create(format);
+        }
+        return valueFormat;
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone,  MappedFieldType fieldType) {
+        if (fieldType instanceof  DateFieldMapper.DateFieldType) {
+            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper((DateFieldMapper.DateFieldType) fieldType, timezone);
+        }
+        if (fieldType instanceof IpFieldMapper.IpFieldType) {
+            return ValueFormat.IPv4;
+        }
+        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
+            return ValueFormat.BOOLEAN;
+        }
+        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
+            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
+        }
+        return ValueFormat.RAW;
+    }
+
+    public Input input() {
+        return this.input;
+    }
+
+    public static class Builder<VS extends ValuesSource> {
+
+        private final ValuesSourceParser<VS> parser;
+
+        private Builder(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
+            parser = new ValuesSourceParser<>(aggName, aggType, context, valuesSourceType);
+        }
+
+        public Builder<VS> scriptable(boolean scriptable) {
+            parser.scriptable = scriptable;
+            return this;
+        }
+
+        public Builder<VS> formattable(boolean formattable) {
+            parser.formattable = formattable;
+            return this;
+        }
+
+        public Builder<VS> timezoneAware(boolean timezoneAware) {
+            parser.timezoneAware = timezoneAware;
+            return this;
+        }
+
+        public Builder<VS> targetValueType(ValueType valueType) {
+            parser.targetValueType = valueType;
+            return this;
+        }
+
+        public ValuesSourceParser<VS> build() {
+            return parser;
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
deleted file mode 100644
index 46b5698..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-/*
- * The ordinal values for this class are tested in ValuesSourceTypeTests to
- * ensure that the ordinal for each value does not change and break bwc
- */
-public enum ValuesSourceType implements Writeable<ValuesSourceType> {
-
-    ANY,
-    NUMERIC,
-    BYTES,
-    GEOPOINT;
-
-    @Override
-    public ValuesSourceType readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown ValuesSourceType ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 1e195b3..6a9d95d 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -42,15 +42,13 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.searchafter.SearchAfterBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.rescore.RescoreBuilder;
+import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
@@ -105,9 +103,8 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return PROTOTYPE.readFrom(in);
     }
 
-    public static SearchSourceBuilder parseSearchSource(XContentParser parser, QueryParseContext context, AggregatorParsers aggParsers)
-            throws IOException {
-        return PROTOTYPE.fromXContent(parser, context, aggParsers);
+    public static SearchSourceBuilder parseSearchSource(XContentParser parser, QueryParseContext context) throws IOException {
+        return PROTOTYPE.fromXContent(parser, context);
     }
 
     /**
@@ -152,7 +149,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     private List<ScriptField> scriptFields;
     private FetchSourceContext fetchSourceContext;
 
-    private AggregatorFactories.Builder aggregations;
+    private List<BytesReference> aggregations;
 
     private HighlightBuilder highlightBuilder;
 
@@ -413,29 +410,26 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     /**
      * Add an aggregation to perform as part of the search.
      */
-    public SearchSourceBuilder aggregation(AggregatorBuilder<?> aggregation) {
-            if (aggregations == null) {
-            aggregations = AggregatorFactories.builder();
-            }
-        aggregations.addAggregator(aggregation);
-            return this;
-    }
-
-    /**
-     * Add an aggregation to perform as part of the search.
-     */
-    public SearchSourceBuilder aggregation(PipelineAggregatorBuilder aggregation) {
+    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
+        try {
             if (aggregations == null) {
-            aggregations = AggregatorFactories.builder();
+                aggregations = new ArrayList<>();
             }
-        aggregations.addPipelineAggregator(aggregation);
+            XContentBuilder builder = XContentFactory.jsonBuilder();
+            builder.startObject();
+            aggregation.toXContent(builder, EMPTY_PARAMS);
+            builder.endObject();
+            aggregations.add(builder.bytes());
             return this;
+        } catch (IOException e) {
+            throw new RuntimeException(e);
         }
+    }
 
     /**
      * Gets the bytes representing the aggregation builders for this request.
      */
-    public AggregatorFactories.Builder aggregations() {
+    public List<BytesReference> aggregations() {
         return aggregations;
     }
 
@@ -448,7 +442,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the hightlighter builder for this request.
+     * Gets the highlighter builder for this request.
      */
     public HighlightBuilder highlighter() {
         return highlightBuilder;
@@ -685,7 +679,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets the boost a specific index will receive when the query is executeed
+     * Sets the boost a specific index will receive when the query is executed
      * against it.
      *
      * @param index
@@ -703,7 +697,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     /**
      * Gets the boost a specific indices will receive when the query is
-     * executeed against them.
+     * executed against them.
      */
     public ObjectFloatHashMap<String> indexBoost() {
         return indexBoost;
@@ -733,8 +727,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return ext;
     }
 
-    public SearchSourceBuilder fromXContent(XContentParser parser, QueryParseContext context, AggregatorParsers aggParsers)
-            throws IOException {
+    public SearchSourceBuilder fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
         SearchSourceBuilder builder = new SearchSourceBuilder();
         XContentParser.Token token = parser.currentToken();
         String currentFieldName = null;
@@ -836,7 +829,23 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
                     }
                     builder.indexBoost = indexBoost;
                 } else if (context.parseFieldMatcher().match(currentFieldName, AGGREGATIONS_FIELD)) {
-                    builder.aggregations = aggParsers.parseAggregators(parser, context);
+                    List<BytesReference> aggregations = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        currentFieldName = parser.currentName();
+                        token = parser.nextToken();
+                        if (token == XContentParser.Token.START_OBJECT) {
+                            XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
+                            xContentBuilder.startObject();
+                            xContentBuilder.field(currentFieldName);
+                            xContentBuilder.copyCurrentStructure(parser);
+                            xContentBuilder.endObject();
+                            aggregations.add(xContentBuilder.bytes());
+                        } else {
+                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
+                                    parser.getTokenLocation());
+                        }
+                    }
+                    builder.aggregations = aggregations;
                 } else if (context.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_FIELD)) {
                     builder.highlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
                 } else if (context.parseFieldMatcher().match(currentFieldName, INNER_HITS_FIELD)) {
@@ -1033,8 +1042,16 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (aggregations != null) {
-            builder.field(AGGREGATIONS_FIELD.getPreferredName(), aggregations);
+            builder.field(AGGREGATIONS_FIELD.getPreferredName());
+            builder.startObject();
+            for (BytesReference aggregation : aggregations) {
+                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(aggregation);
+                parser.nextToken();
+                parser.nextToken();
+                builder.copyCurrentStructure(parser);
             }
+            builder.endObject();
+        }
 
         if (highlightBuilder != null) {
             this.highlightBuilder.toXContent(builder, params);
@@ -1086,7 +1103,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
             this(fieldName, script, false);
         }
 
-        public ScriptField(String fieldName, Script script, boolean ignoreFailure) {
+        private ScriptField(String fieldName, Script script, boolean ignoreFailure) {
             this.fieldName = fieldName;
             this.script = script;
             this.ignoreFailure = ignoreFailure;
@@ -1149,8 +1166,13 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     public SearchSourceBuilder readFrom(StreamInput in) throws IOException {
         SearchSourceBuilder builder = new SearchSourceBuilder();
         if (in.readBoolean()) {
-            builder.aggregations = AggregatorFactories.Builder.PROTOTYPE.readFrom(in);
+            int size = in.readVInt();
+            List<BytesReference> aggregations = new ArrayList<>(size);
+            for (int i = 0; i < size; i++) {
+                aggregations.add(in.readBytesReference());
             }
+            builder.aggregations = aggregations;
+        }
         builder.explain = in.readOptionalBoolean();
         builder.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
         boolean hasFieldDataFields = in.readBoolean();
@@ -1255,7 +1277,10 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         boolean hasAggregations = aggregations != null;
         out.writeBoolean(hasAggregations);
         if (hasAggregations) {
-            aggregations.writeTo(out);
+            out.writeVInt(aggregations.size());
+            for (BytesReference aggregation : aggregations) {
+                out.writeBytesReference(aggregation);
+            }
         }
         out.writeOptionalBoolean(explain);
         FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/FragmentBuilderHelper.java b/core/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/FragmentBuilderHelper.java
index 363a3b9..b3175e6 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/FragmentBuilderHelper.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/FragmentBuilderHelper.java
@@ -41,7 +41,7 @@ import java.util.Comparator;
 import java.util.List;
 
 /**
- * Simple helper class for {@link FastVectorHighlighter} {@link FragmentsBuilder} implemenations.
+ * Simple helper class for {@link FastVectorHighlighter} {@link FragmentsBuilder} implementations.
  */
 public final class FragmentBuilderHelper {
 
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index 6c01a27..32aa45f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -59,7 +59,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -151,16 +150,16 @@ public class DefaultSearchContext extends SearchContext {
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
     private final QueryShardContext queryShardContext;
-    private FetchPhase fetchPhase;
 
-    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget, Engine.Searcher engineSearcher,
-            IndexService indexService, IndexShard indexShard, ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
-            BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher, TimeValue timeout,
-            FetchPhase fetchPhase) {
+    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget,
+                                Engine.Searcher engineSearcher, IndexService indexService, IndexShard indexShard,
+                                ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher,
+                                TimeValue timeout
+    ) {
         super(parseFieldMatcher);
         this.id = id;
         this.request = request;
-        this.fetchPhase = fetchPhase;
         this.searchType = request.searchType();
         this.shardTarget = shardTarget;
         this.engineSearcher = engineSearcher;
@@ -734,11 +733,6 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public FetchSearchResult fetchResult() {
         return fetchResult;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
index 83ea2b1..c47d32b 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
@@ -42,7 +42,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -490,11 +489,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return in.fetchPhase();
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return in.smartNameFieldType(name);
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index 2b35e18..7bef567 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -46,7 +46,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -305,8 +304,6 @@ public abstract class SearchContext implements Releasable {
 
     public abstract QuerySearchResult queryResult();
 
-    public abstract FetchPhase fetchPhase();
-
     public abstract FetchSearchResult fetchResult();
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java b/core/src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java
index 3e7d66a..82604b1 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java
@@ -27,7 +27,7 @@ import java.io.IOException;
 import java.util.Iterator;
 
 /*
- * Can iterate over the positions of a term an arbotrary number of times. 
+ * Can iterate over the positions of a term an arbitrary number of times.
  * */
 public class CachedPositionIterator extends PositionIterator {
 
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java b/core/src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java
index 09c78d2..fbca4f4 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java
@@ -236,7 +236,7 @@ public class IndexFieldTerm implements Iterable<TermPosition> {
     /*
      * A user might decide inside a script to call get with _POSITIONS and then
      * a second time with _PAYLOADS. If the positions were recorded but the
-     * payloads were not, the user will not have access to them. Therfore, throw
+     * payloads were not, the user will not have access to them. Therefore, throw
      * exception here explaining how to call get().
      */
     public void validateFlags(int flags2) {
diff --git a/core/src/main/java/org/elasticsearch/search/profile/InternalProfileCollector.java b/core/src/main/java/org/elasticsearch/search/profile/InternalProfileCollector.java
index 33d11a1..48d165e 100644
--- a/core/src/main/java/org/elasticsearch/search/profile/InternalProfileCollector.java
+++ b/core/src/main/java/org/elasticsearch/search/profile/InternalProfileCollector.java
@@ -34,7 +34,7 @@ import java.util.List;
  * - doSetNextReader()
  * - needsScores()
  *
- * InternalProfiler facilitates the linking of the the Collector graph
+ * InternalProfiler facilitates the linking of the Collector graph
  */
 public class InternalProfileCollector implements Collector {
 
diff --git a/core/src/main/java/org/elasticsearch/search/profile/ProfileBreakdown.java b/core/src/main/java/org/elasticsearch/search/profile/ProfileBreakdown.java
index 55ad77b..a992f81 100644
--- a/core/src/main/java/org/elasticsearch/search/profile/ProfileBreakdown.java
+++ b/core/src/main/java/org/elasticsearch/search/profile/ProfileBreakdown.java
@@ -51,7 +51,7 @@ public final class ProfileBreakdown {
      */
     private final long[] timings;
 
-    /** Scrach to store the current timing type. */
+    /** Scratch to store the current timing type. */
     private TimingType currentTimingType;
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/profile/Profilers.java b/core/src/main/java/org/elasticsearch/search/profile/Profilers.java
index 0fb7d9a..69a4544 100644
--- a/core/src/main/java/org/elasticsearch/search/profile/Profilers.java
+++ b/core/src/main/java/org/elasticsearch/search/profile/Profilers.java
@@ -31,7 +31,7 @@ public final class Profilers {
     private final ContextIndexSearcher searcher;
     private final List<Profiler> profilers;
 
-    /** Sole constructor. This {@link Profilers} instance will initiall wrap one {@link Profiler}. */
+    /** Sole constructor. This {@link Profilers} instance will initially wrap one {@link Profiler}. */
     public Profilers(ContextIndexSearcher searcher) {
         this.searcher = searcher;
         this.profilers = new ArrayList<>();
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java b/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java
index e3465a4..b475ca9 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java
@@ -45,7 +45,7 @@ public interface Rescorer {
      * Modifies the result of the previously executed search ({@link TopDocs})
      * in place based on the given {@link RescoreSearchContext}.
      *
-     * @param topDocs        the result of the previously exectued search
+     * @param topDocs        the result of the previously executed search
      * @param context        the current {@link SearchContext}. This will never be <code>null</code>.
      * @param rescoreContext the {@link RescoreSearchContext}. This will never be <code>null</code>
      * @throws IOException if an {@link IOException} occurs during rescoring
@@ -66,7 +66,7 @@ public interface Rescorer {
             Explanation sourceExplanation) throws IOException;
 
     /**
-     * Parses the {@link RescoreSearchContext} for this impelementation
+     * Parses the {@link RescoreSearchContext} for this implementation
      *
      * @param parser  the parser to read the context from
      * @param context the current shard context
@@ -76,7 +76,7 @@ public interface Rescorer {
     public RescoreSearchContext parse(XContentParser parser, QueryShardContext context) throws IOException;
 
     /**
-     * Extracts all terms needed to exectue this {@link Rescorer}. This method
+     * Extracts all terms needed to execute this {@link Rescorer}. This method
      * is executed in a distributed frequency collection roundtrip for
      * {@link SearchType#DFS_QUERY_AND_FETCH} and
      * {@link SearchType#DFS_QUERY_THEN_FETCH}
@@ -84,8 +84,8 @@ public interface Rescorer {
     public void extractTerms(SearchContext context, RescoreSearchContext rescoreContext, Set<Term> termsSet);
 
     /*
-     * TODO: At this point we only have one implemenation which modifies the
-     * TopDocs given. Future implemenations might return actual resutls that
+     * TODO: At this point we only have one implementation which modifies the
+     * TopDocs given. Future implementations might return actual results that
      * contain information about the rescore context. For example a pair wise
      * reranker might return the feature vector for the top N window in order to
      * merge results on the callers side. For now we don't have a return type at
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java
index 8517216..09b4012 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java
@@ -85,10 +85,10 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
      * Sets the maximum percentage of the terms that at most considered to be
      * misspellings in order to form a correction. This method accepts a float
      * value in the range [0..1) as a fraction of the actual query terms a
-     * number <tt>&gt;=1</tt> as an absolut number of query terms.
+     * number <tt>&gt;=1</tt> as an absolute number of query terms.
      *
      * The default is set to <tt>1.0</tt> which corresponds to that only
-     * corrections with at most 1 missspelled term are returned.
+     * corrections with at most 1 misspelled term are returned.
      */
     public PhraseSuggestionBuilder maxErrors(Float maxErrors) {
         this.maxErrors = maxErrors;
@@ -288,7 +288,7 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
     }
 
     /**
-     * A "stupid-backoff" smoothing model simialr to <a
+     * A "stupid-backoff" smoothing model similar to <a
      * href="http://en.wikipedia.org/wiki/Katz's_back-off_model"> Katz's
      * Backoff</a>. This model is used as the default if no model is configured.
      * <p>
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 8b174ec..685fded 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -704,7 +704,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     // this code is a take on guava's HostAndPort, like a HostAndPortRange
 
-    // pattern for validating ipv6 bracked addresses.
+    // pattern for validating ipv6 bracket addresses.
     // not perfect, but PortsRange should take care of any port range validation, not a regex
     private static final Pattern BRACKET_PATTERN = Pattern.compile("^\\[(.*:.*)\\](?::([\\d\\-]*))?$");
 
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java
index 17be167..d35704a 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java
@@ -48,7 +48,9 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.ReentrantLock;
 import java.util.function.Function;
 
@@ -252,11 +254,14 @@ public class TasksIT extends ESIntegTestCase {
          */
         ReentrantLock taskFinishLock = new ReentrantLock();
         taskFinishLock.lock();
+        CountDownLatch taskRegistered = new CountDownLatch(1);
         for (ClusterService clusterService : internalCluster().getInstances(ClusterService.class)) {
             ((MockTaskManager)clusterService.getTaskManager()).addListener(new MockTaskManagerListener() {
                 @Override
                 public void onTaskRegistered(Task task) {
-                    // Intentional noop
+                    if (task.getAction().startsWith(IndexAction.NAME)) {
+                        taskRegistered.countDown();
+                    }
                 }
 
                 @Override
@@ -275,6 +280,7 @@ public class TasksIT extends ESIntegTestCase {
             });
         }
         ListenableActionFuture<?> indexFuture = client().prepareIndex("test", "test").setSource("test", "test").execute();
+        taskRegistered.await(10, TimeUnit.SECONDS); // waiting for at least one task to be registered
         ListTasksResponse tasks = client().admin().cluster().prepareListTasks().setActions("indices:data/write/index*").setDetailed(true)
                 .get();
         taskFinishLock.unlock();
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java b/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
index 8d3c812..828c36e 100644
--- a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
+++ b/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
@@ -24,14 +24,19 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.ingest.RandomDocumentPicks;
 import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.XContentTestUtils;
 
 import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import static org.elasticsearch.test.XContentTestUtils.convertToMap;
+import static org.elasticsearch.test.XContentTestUtils.differenceBetweenMapsIgnoringArrayOrder;
 import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
 import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.nullValue;
 
 public class WriteableIngestDocumentTests extends ESTestCase {
 
@@ -111,4 +116,30 @@ public class WriteableIngestDocumentTests extends ESTestCase {
         WriteableIngestDocument otherWriteableIngestDocument = new WriteableIngestDocument(streamInput);
         assertThat(otherWriteableIngestDocument, equalTo(writeableIngestDocument));
     }
+
+    @SuppressWarnings("unchecked")
+    public void testToXContent() throws IOException {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        WriteableIngestDocument writeableIngestDocument = new WriteableIngestDocument(new IngestDocument(ingestDocument));
+
+        Map<String, Object> toXContentMap = convertToMap(writeableIngestDocument);
+        Map<String, Object> toXContentDoc = (Map<String, Object>) toXContentMap.get("doc");
+        Map<String, Object> toXContentSource = (Map<String, Object>) toXContentDoc.get("_source");
+        Map<String, String> toXContentIngestMetadata = (Map<String, String>) toXContentDoc.get("_ingest");
+
+        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
+        for (Map.Entry<IngestDocument.MetaData, String> metadata : metadataMap.entrySet()) {
+            String fieldName = metadata.getKey().getFieldName();
+            if (metadata.getValue() == null) {
+               assertThat(toXContentDoc.containsKey(fieldName), is(false));
+            } else {
+                assertThat(toXContentDoc.get(fieldName), equalTo(metadata.getValue()));
+            }
+        }
+
+        String sourceDiff = differenceBetweenMapsIgnoringArrayOrder(toXContentSource, ingestDocument.getSourceAndMetadata());
+        assertThat(sourceDiff, is(nullValue()));
+
+        assertThat(toXContentIngestMetadata, equalTo(ingestDocument.getIngestMetadata()));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
index ed829d8..7301d37 100644
--- a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
@@ -45,7 +45,7 @@ public class MultiSearchRequestTests extends ESTestCase {
         IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, singletonMap("match_all", new MatchAllQueryParser()));
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch1.json");
         MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null,
-                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY, null);
+                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY);
         assertThat(request.requests().size(), equalTo(8));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).indicesOptions(), equalTo(IndicesOptions.fromOptions(true, true, true, true, IndicesOptions.strictExpandOpenAndForbidClosed())));
@@ -72,7 +72,7 @@ public class MultiSearchRequestTests extends ESTestCase {
         IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, singletonMap("match_all", new MatchAllQueryParser()));
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch2.json");
         MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null,
-                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY, null);
+                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY);
         assertThat(request.requests().size(), equalTo(5));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).types().length, equalTo(0));
@@ -91,7 +91,7 @@ public class MultiSearchRequestTests extends ESTestCase {
         IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, singletonMap("match_all", new MatchAllQueryParser()));
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch3.json");
         MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null,
-                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY, null);
+                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY);
         assertThat(request.requests().size(), equalTo(4));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -111,7 +111,7 @@ public class MultiSearchRequestTests extends ESTestCase {
         IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, singletonMap("match_all", new MatchAllQueryParser()));
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch4.json");
         MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null,
-                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY, null);
+                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -133,7 +133,7 @@ public class MultiSearchRequestTests extends ESTestCase {
         IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, singletonMap("match_all", new MatchAllQueryParser()));
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch5.json");
         MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), true, null, null,
-                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY, null);
+                null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry, ParseFieldMatcher.EMPTY);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java b/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
index f4540e6..6b38d35 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
@@ -101,7 +101,9 @@ public class ClusterStateCreationUtils {
                 primaryNode = newNode(0).id();
                 unassignedNodes.remove(primaryNode);
             } else {
-                primaryNode = selectAndRemove(unassignedNodes);
+                Set<String> unassignedNodesExecludingPrimary = new HashSet<>(unassignedNodes);
+                unassignedNodesExecludingPrimary.remove(newNode(0).id());
+                primaryNode = selectAndRemove(unassignedNodesExecludingPrimary);
             }
             if (primaryState == ShardRoutingState.RELOCATING) {
                 relocatingNode = selectAndRemove(unassignedNodes);
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
index d2b0c1a..7cf0a4f 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
@@ -84,9 +84,11 @@ import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.Consumer;
 
 import static org.elasticsearch.action.support.replication.ClusterStateCreationUtils.state;
 import static org.elasticsearch.action.support.replication.ClusterStateCreationUtils.stateWithActivePrimary;
+import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.not;
 import static org.hamcrest.Matchers.arrayWithSize;
 import static org.hamcrest.Matchers.either;
@@ -631,9 +633,11 @@ public class TransportReplicationActionTests extends ESTestCase {
         indexShardRouting.set(primaryShard);
 
         assertIndexShardCounter(2);
-        // TODO: set a default timeout
-        TransportReplicationAction<Request, Request, Response>.ReplicationPhase replicationPhase = action.new ReplicationPhase(task,
-                request, new Response(), request.shardId(), createTransportChannel(listener), reference);
+        AtomicReference<Throwable> error = new AtomicReference<>();
+
+        TransportChannel channel = createTransportChannel(listener, error::set);
+        TransportReplicationAction<Request, Request, Response>.ReplicationPhase replicationPhase =
+                action.new ReplicationPhase(task, request, new Response(), request.shardId(), channel, reference);
 
         assertThat(replicationPhase.totalShards(), equalTo(totalShards));
         assertThat(replicationPhase.pending(), equalTo(assignedReplicas));
@@ -704,7 +708,8 @@ public class TransportReplicationActionTests extends ESTestCase {
                     // the shard the request was sent to and the shard to be failed should be the same
                     assertEquals(shardRoutingEntry.getShardRouting(), routing);
                     failures.add(shardFailedRequest);
-                    if (randomBoolean()) {
+                    int ternary = randomIntBetween(0, 2);
+                    if (ternary == 0) {
                         // simulate master left and test that the shard failure is retried
                         int numberOfRetries = randomIntBetween(1, 4);
                         CapturingTransport.CapturedRequest currentRequest = shardFailedRequest;
@@ -718,8 +723,19 @@ public class TransportReplicationActionTests extends ESTestCase {
                         }
                         // now simulate that the last retry succeeded
                         transport.handleResponse(currentRequest.requestId, TransportResponse.Empty.INSTANCE);
-                    } else {
+                    } else if (ternary == 1) {
+                        // simulate the primary has been demoted
+                        transport.handleRemoteError(shardFailedRequest.requestId, new ShardStateAction.NoLongerPrimaryShardException(shardRoutingEntry.getShardRouting().shardId(), "shard-failed-test"));
+                        // the primary should fail itself
+                        assertShardIsFailed();
+                        // we should see a retry on primary exception
+                        assertNotNull(error.get());
+                        assertThat(error.get(), instanceOf(TransportReplicationAction.RetryOnPrimaryException.class));
+                        return;
+                    } else if (ternary == 2) {
                         transport.handleResponse(shardFailedRequest.requestId, TransportResponse.Empty.INSTANCE);
+                    } else {
+                        assert false;
                     }
                 }
             } else {
@@ -882,14 +898,85 @@ public class TransportReplicationActionTests extends ESTestCase {
         assertPhase(task, "failed");
     }
 
+    public void testReroutePhaseRetriedAfterDemotedPrimary() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, "_na_", 0);
+        boolean localPrimary = true;
+        clusterService.setState(state(index, localPrimary,
+                ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        Action action = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected void resolveRequest(MetaData metaData, String concreteIndex, Request request) {
+                request.setShardId(shardId);
+            }
+        };
+        Request request = new Request();
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(null, request, listener);
+        reroutePhase.run();
+
+        // reroute phase should send primary action
+        CapturingTransport.CapturedRequest[] primaryRequests = transport.getCapturedRequestsAndClear();
+        assertThat(primaryRequests.length, equalTo(1));
+        assertThat(primaryRequests[0].action, equalTo("testAction" + (localPrimary ? "[p]" : "")));
+        AtomicReference<Throwable> error = new AtomicReference<>();
+        TransportChannel channel = createTransportChannel(listener, error::set);
+
+        // simulate primary action
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(maybeTask(), request, channel);
+        primaryPhase.run();
+
+        // primary action should send replica request
+        CapturingTransport.CapturedRequest[] replicaRequests = transport.getCapturedRequestsAndClear();
+        assertThat(replicaRequests.length, equalTo(1));
+        assertThat(replicaRequests[0].action, equalTo("testAction[r]"));
+        indexShardRouting.set(clusterService.state().getRoutingTable().shardRoutingTable(shardId).primaryShard());
+
+        // simulate replica failure
+        transport.handleRemoteError(replicaRequests[0].requestId, new Exception("exception"));
+
+        // the primary should request replica failure
+        CapturingTransport.CapturedRequest[] replicaFailures = transport.getCapturedRequestsAndClear();
+        assertThat(replicaFailures.length, equalTo(1));
+        assertThat(replicaFailures[0].action, equalTo(ShardStateAction.SHARD_FAILED_ACTION_NAME));
+
+        // simulate demoted primary
+        transport.handleRemoteError(replicaFailures[0].requestId, new ShardStateAction.NoLongerPrimaryShardException(shardId, "demoted"));
+        assertTrue(isShardFailed.get());
+        assertTrue(listener.isDone());
+        assertNotNull(error.get());
+        assertThat(error.get(), instanceOf(TransportReplicationAction.RetryOnPrimaryException.class));
+        assertThat(error.get().getMessage(), containsString("was demoted while failing replica shard"));
+
+        // reroute phase sees the retry
+        transport.handleRemoteError(primaryRequests[0].requestId, error.get());
+
+        // publish a new cluster state
+        boolean localPrimaryOnRetry = randomBoolean();
+        clusterService.setState(state(index, localPrimaryOnRetry,
+            ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        CapturingTransport.CapturedRequest[] primaryRetry = transport.getCapturedRequestsAndClear();
+
+        // the request should be retried
+        assertThat(primaryRetry.length, equalTo(1));
+        assertThat(primaryRetry[0].action, equalTo("testAction" + (localPrimaryOnRetry ? "[p]" : "")));
+    }
+
     private void assertIndexShardCounter(int expected) {
         assertThat(count.get(), equalTo(expected));
     }
 
+    private void assertShardIsFailed() {
+        assertTrue(isShardFailed.get());
+    }
+
     private final AtomicInteger count = new AtomicInteger(0);
 
     private final AtomicBoolean isRelocated = new AtomicBoolean(false);
 
+    private final AtomicBoolean isShardFailed = new AtomicBoolean();
+
     private final AtomicReference<ShardRouting> indexShardRouting = new AtomicReference<>();
 
     /**
@@ -904,6 +991,11 @@ public class TransportReplicationActionTests extends ESTestCase {
             }
 
             @Override
+            public void failShard(String reason, @Nullable Throwable e) {
+                isShardFailed.set(true);
+            }
+
+            @Override
             public ShardRouting routingEntry() {
                 ShardRouting shardRouting = indexShardRouting.get();
                 assert shardRouting != null;
@@ -1099,6 +1191,10 @@ public class TransportReplicationActionTests extends ESTestCase {
      * Transport channel that is needed for replica operation testing.
      */
     public TransportChannel createTransportChannel(final PlainActionFuture<Response> listener) {
+        return createTransportChannel(listener, error -> {});
+    }
+
+    public TransportChannel createTransportChannel(final PlainActionFuture<Response> listener, Consumer<Throwable> consumer) {
         return new TransportChannel() {
 
             @Override
@@ -1123,6 +1219,7 @@ public class TransportReplicationActionTests extends ESTestCase {
 
             @Override
             public void sendResponse(Throwable error) throws IOException {
+                consumer.accept(error);
                 listener.onFailure(error);
             }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/SimpleClusterStateIT.java b/core/src/test/java/org/elasticsearch/cluster/SimpleClusterStateIT.java
index e838989..11d176b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/SimpleClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/SimpleClusterStateIT.java
@@ -42,7 +42,7 @@ import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.hamcrest.Matchers.is;
 
 /**
- * Checking simple filtering capabilites of the cluster state
+ * Checking simple filtering capabilities of the cluster state
  *
  */
 public class SimpleClusterStateIT extends ESIntegTestCase {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java
index 00acf1e..53a0faf 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java
@@ -42,7 +42,7 @@ public class AllocationIdTests extends ESTestCase {
         assertThat(shard.allocationId(), nullValue());
 
         logger.info("-- initialize the shard");
-        shard.initialize("node1", -1);
+        shard.initialize("node1", null, -1);
         AllocationId allocationId = shard.allocationId();
         assertThat(allocationId, notNullValue());
         assertThat(allocationId.getId(), notNullValue());
@@ -59,7 +59,7 @@ public class AllocationIdTests extends ESTestCase {
     public void testSuccessfulRelocation() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned(new Index("test","_na_"), 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1", -1);
+        shard.initialize("node1", null, -1);
         shard.moveToStarted();
 
         AllocationId allocationId = shard.allocationId();
@@ -82,7 +82,7 @@ public class AllocationIdTests extends ESTestCase {
     public void testCancelRelocation() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned(new Index("test","_na_"), 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1", -1);
+        shard.initialize("node1", null, -1);
         shard.moveToStarted();
 
         AllocationId allocationId = shard.allocationId();
@@ -102,7 +102,7 @@ public class AllocationIdTests extends ESTestCase {
     public void testMoveToUnassigned() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned(new Index("test","_na_"), 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1", -1);
+        shard.initialize("node1", null, -1);
         shard.moveToStarted();
 
         logger.info("-- move to unassigned");
@@ -113,7 +113,7 @@ public class AllocationIdTests extends ESTestCase {
     public void testReinitializing() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned(new Index("test","_na_"), 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1", -1);
+        shard.initialize("node1", null, -1);
         shard.moveToStarted();
         AllocationId allocationId = shard.allocationId();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java b/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java
index 72ecc17..2b581a9 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java
@@ -42,7 +42,7 @@ public final class RandomShardRoutingMutator {
                 break;
             case 1:
                 if (shardRouting.unassigned()) {
-                    shardRouting.initialize(randomFrom(nodes), -1);
+                    shardRouting.initialize(randomFrom(nodes), null, -1);
                 }
                 break;
             case 2:
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java
index 5d3466b..7299cbd 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java
@@ -25,7 +25,7 @@ package org.elasticsearch.cluster.routing;
 public class ShardRoutingHelper {
 
     public static void relocate(ShardRouting routing, String nodeId) {
-        relocate(routing, nodeId, -1);
+        relocate(routing, nodeId, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
     }
 
     public static void relocate(ShardRouting routing, String nodeId, long expectedByteSize) {
@@ -37,11 +37,11 @@ public class ShardRoutingHelper {
     }
 
     public static void initialize(ShardRouting routing, String nodeId) {
-        initialize(routing, nodeId, -1);
+        initialize(routing, nodeId, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
     }
 
     public static void initialize(ShardRouting routing, String nodeId, long expectedSize) {
-        routing.initialize(nodeId, expectedSize);
+        routing.initialize(nodeId, null, expectedSize);
     }
 
     public static void reinit(ShardRouting routing) {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java
index dd38b0c..4e70761 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java
@@ -254,7 +254,7 @@ public class ShardRoutingTests extends ESTestCase {
             }
 
             try {
-                routing.initialize("boom", -1);
+                routing.initialize("boom", null, -1);
                 fail("must be frozen");
             } catch (IllegalStateException ex) {
                 // expected
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
index 95f69a7..ba73181 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
@@ -187,7 +187,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ShardRouting shard = TestShardRouting.newShardRouting("test", 1, null, null, null, true, ShardRoutingState.UNASSIGNED, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
         ShardRouting mutable = new ShardRouting(shard);
         assertThat(mutable.unassignedInfo(), notNullValue());
-        mutable.initialize("test_node", -1);
+        mutable.initialize("test_node", null, -1);
         assertThat(mutable.state(), equalTo(ShardRoutingState.INITIALIZING));
         assertThat(mutable.unassignedInfo(), notNullValue());
         mutable.moveToStarted();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
index 08cbdc0..68706d9 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
@@ -365,37 +365,37 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
                     switch (sr.id()) {
                         case 0:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node1", -1);
+                                allocation.routingNodes().initialize(sr, "node1", null, -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node0", -1);
+                                allocation.routingNodes().initialize(sr, "node0", null, -1);
                             }
                             break;
                         case 1:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node1", -1);
+                                allocation.routingNodes().initialize(sr, "node1", null, -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node2", -1);
+                                allocation.routingNodes().initialize(sr, "node2", null, -1);
                             }
                             break;
                         case 2:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node3", -1);
+                                allocation.routingNodes().initialize(sr, "node3", null, -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node2", -1);
+                                allocation.routingNodes().initialize(sr, "node2", null, -1);
                             }
                             break;
                         case 3:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node3", -1);
+                                allocation.routingNodes().initialize(sr, "node3", null, -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node1", -1);
+                                allocation.routingNodes().initialize(sr, "node1", null, -1);
                             }
                             break;
                         case 4:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node2", -1);
+                                allocation.routingNodes().initialize(sr, "node2", null, -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node0", -1);
+                                allocation.routingNodes().initialize(sr, "node0", null, -1);
                             }
                             break;
                     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java
index 74ff900..1ba0c06 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java
@@ -51,7 +51,7 @@ import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 
 /**
- * A base testscase that allows to run tests based on the output of the CAT API
+ * A base testcase that allows to run tests based on the output of the CAT API
  * The input is a line based cat/shards output like:
  *   kibana-int           0 p STARTED       2  24.8kb 10.202.245.2 r5-9-35
  *
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
index beb7c13..d37aee1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
@@ -755,7 +755,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
                 .put(IndexMetaData.builder("test1").settings(settings(Version.CURRENT).put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "_id", "node1,node2")).numberOfShards(2).numberOfReplicas(0))
                 .build();
 
-        // we use a second index here (test1) that never gets assigned otherwise allocateUnassinged is never called if we don't have unassigned shards.
+        // we use a second index here (test1) that never gets assigned otherwise allocateUnassigned is never called if we don't have unassigned shards.
         RoutingTable routingTable = RoutingTable.builder()
                 .addAsNew(metaData.index("test"))
                 .addAsNew(metaData.index("test1"))
diff --git a/core/src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java b/core/src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java
index d0e03ce..06fadff 100644
--- a/core/src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java
@@ -163,7 +163,7 @@ public class ShapeBuilderTests extends ESTestCase {
             )
             .build();
 
-        // LineString that needs to be wrappped
+        // LineString that needs to be wrapped
         ShapeBuilders.newMultiLinestring()
             .linestring(new LineStringBuilder(new CoordinatesBuilder()
                 .coordinate(150.0, 60.0)
diff --git a/core/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java b/core/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java
index 5cb30a1..c4775b6 100644
--- a/core/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java
+++ b/core/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java
@@ -332,7 +332,7 @@ public class SimpleJodaTests extends ESTestCase {
         Joda.EpochTimePrinter epochTimePrinter = new Joda.EpochTimePrinter(false);
         epochTimePrinter.printTo(buffer, now, Locale.ROOT);
         assertThat(buffer.length(), is(10));
-        // only check the last digit, as seconds go from 0-99 in the unix timestamp and dont stop at 60
+        // only check the last digit, as seconds go from 0-99 in the unix timestamp and don't stop at 60
         assertThat(buffer.toString(), endsWith(String.valueOf(now.getSecondOfMinute() % 10)));
 
         buffer = new StringBuffer();
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index 739e07d..1bc861b 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -754,7 +754,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     }
 
     /**
-     * A 4 node cluster with m_m_n set to 3 and each node has one unicast enpoint. One node partitions from the master node.
+     * A 4 node cluster with m_m_n set to 3 and each node has one unicast endpoint. One node partitions from the master node.
      * The temporal unicast responses is empty. When partition is solved the one ping response contains a master node.
      * The rejoining node should take this master node and connect.
      */
@@ -971,7 +971,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     }
 
     /**
-     * Adds an asymetric break between a master and one of the nodes and makes
+     * Adds an asymmetric break between a master and one of the nodes and makes
      * sure that the node is removed form the cluster, that the node start pinging and that
      * the cluster reforms when healed.
      */
diff --git a/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java b/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java
index e2830b1..d1cd8d9 100644
--- a/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java
@@ -161,7 +161,8 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
      */
     public void testFoundAllocationAndAllocating() {
         final RoutingAllocation allocation;
-        if (randomBoolean()) {
+        boolean useAllocationIds = randomBoolean();
+        if (useAllocationIds) {
             allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), false, randomFrom(Version.V_2_0_0, Version.CURRENT), "allocId1");
             testAllocator.addData(node1, 1, "allocId1", randomBoolean());
         } else {
@@ -173,6 +174,10 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
         assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));
         assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
         assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo(node1.id()));
+        if (useAllocationIds) {
+            // check that allocation id is reused
+            assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).allocationId().getId(), equalTo("allocId1"));
+        }
     }
 
     /**
diff --git a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
index 0b0691b..55c0c85 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
@@ -88,6 +88,8 @@ public class IndexModuleTests extends ESTestCase {
     private Environment environment;
     private NodeEnvironment nodeEnvironment;
     private NodeServicesProvider nodeServicesProvider;
+    private IndicesQueryCache indicesQueryCache;
+
     private IndexService.ShardStoreDeleter deleter = new IndexService.ShardStoreDeleter() {
         @Override
         public void deleteShardStore(String reason, ShardLock lock, IndexSettings indexSettings) throws IOException {
@@ -103,7 +105,6 @@ public class IndexModuleTests extends ESTestCase {
     static NodeServicesProvider newNodeServiceProvider(Settings settings, Environment environment, Client client, ScriptEngineService... scriptEngineServices) throws IOException {
         // TODO this can be used in other place too - lets first refactor the IndicesQueriesRegistry
         ThreadPool threadPool = new ThreadPool("test");
-        IndicesQueryCache indicesQueryCache = new IndicesQueryCache(settings);
         CircuitBreakerService circuitBreakerService = new NoneCircuitBreakerService();
         PageCacheRecycler recycler = new PageCacheRecycler(settings, threadPool);
         BigArrays bigArrays = new BigArrays(recycler, circuitBreakerService);
@@ -114,13 +115,14 @@ public class IndexModuleTests extends ESTestCase {
         ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
         ScriptService scriptService = new ScriptService(settings, environment, scriptEngines, new ResourceWatcherService(settings, threadPool), scriptEngineRegistry, scriptContextRegistry, scriptSettings);
         IndicesQueriesRegistry indicesQueriesRegistry = new IndicesQueriesRegistry(settings, emptyMap());
-        return new NodeServicesProvider(threadPool, indicesQueryCache, bigArrays, client, scriptService, indicesQueriesRegistry, circuitBreakerService);
+        return new NodeServicesProvider(threadPool, bigArrays, client, scriptService, indicesQueriesRegistry, circuitBreakerService);
     }
 
     @Override
     public void setUp() throws Exception {
         super.setUp();
         settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
+        indicesQueryCache = new IndicesQueryCache(settings);
         indexSettings = IndexSettingsModule.newIndexSettings("foo", settings);
         index = indexSettings.getIndex();
         environment = new Environment(settings);
@@ -133,17 +135,15 @@ public class IndexModuleTests extends ESTestCase {
     public void tearDown() throws Exception {
         super.tearDown();
         nodeEnvironment.close();
-        nodeServicesProvider.getThreadPool().shutdown();
-        if (nodeServicesProvider.getThreadPool().awaitTermination(10, TimeUnit.SECONDS) == false) {
-            nodeServicesProvider.getThreadPool().shutdownNow();
-        }
+        indicesQueryCache.close();
+        ThreadPool.terminate(nodeServicesProvider.getThreadPool(), 10, TimeUnit.SECONDS);
     }
 
     public void testWrapperIsBound() throws IOException {
         IndexModule module = new IndexModule(indexSettings, null, new AnalysisRegistry(null, environment));
         module.setSearcherWrapper((s) -> new Wrapper());
         module.engineFactory.set(new MockEngineFactory(AssertingDirectoryReader.class));
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry, new IndicesFieldDataCache(settings, listener));
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry, new IndicesFieldDataCache(settings, listener));
         assertTrue(indexService.getSearcherWrapper() instanceof Wrapper);
         assertSame(indexService.getEngineFactory(), module.engineFactory.get());
         indexService.close("simon says", false);
@@ -158,10 +158,9 @@ public class IndexModuleTests extends ESTestCase {
             .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), "foo_store")
             .build();
         IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(index, settings);
-        final Index index = indexSettings.getIndex();
         IndexModule module = new IndexModule(indexSettings, null, new AnalysisRegistry(null, environment));
         module.addIndexStore("foo_store", FooStore::new);
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry, new IndicesFieldDataCache(settings, listener));
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry, new IndicesFieldDataCache(settings, listener));
         assertTrue(indexService.getIndexStore() instanceof FooStore);
         try {
             module.addIndexStore("foo_store", FooStore::new);
@@ -184,7 +183,7 @@ public class IndexModuleTests extends ESTestCase {
         IndexModule module = new IndexModule(indexSettings, null, new AnalysisRegistry(null, environment));
         Consumer<Settings> listener = (s) -> {};
         module.addIndexEventListener(eventListener);
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
             new IndicesFieldDataCache(settings, this.listener));
         IndexSettings x = indexService.getIndexSettings();
         assertEquals(x.getSettings().getAsMap(), indexSettings.getSettings().getAsMap());
@@ -209,7 +208,7 @@ public class IndexModuleTests extends ESTestCase {
 
         }
 
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
             new IndicesFieldDataCache(settings, listener));
         assertSame(booleanSetting, indexService.getIndexSettings().getScopedSettings().get(booleanSetting.getKey()));
 
@@ -236,7 +235,7 @@ public class IndexModuleTests extends ESTestCase {
             }
         });
 
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
             new IndicesFieldDataCache(settings, listener));
         SimilarityService similarityService = indexService.similarityService();
         assertNotNull(similarityService.getSimilarity("my_similarity"));
@@ -254,7 +253,7 @@ public class IndexModuleTests extends ESTestCase {
                 .build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings("foo", indexSettings), null, new AnalysisRegistry(null, environment));
         try {
-            module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+            module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
                 new IndicesFieldDataCache(settings, listener));
         } catch (IllegalArgumentException ex) {
             assertEquals("Unknown Similarity type [test_similarity] for [my_similarity]", ex.getMessage());
@@ -269,7 +268,7 @@ public class IndexModuleTests extends ESTestCase {
                 .build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings("foo", indexSettings), null, new AnalysisRegistry(null, environment));
         try {
-            module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+            module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
                 new IndicesFieldDataCache(settings, listener));
         } catch (IllegalArgumentException ex) {
             assertEquals("Similarity [my_similarity] must have an associated type", ex.getMessage());
@@ -317,7 +316,7 @@ public class IndexModuleTests extends ESTestCase {
             assertEquals(e.getMessage(), "Can't register the same [query_cache] more than once for [custom]");
         }
 
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
             new IndicesFieldDataCache(settings, listener));
         assertTrue(indexService.cache().query() instanceof CustomQueryCache);
         indexService.close("simon says", false);
@@ -328,7 +327,7 @@ public class IndexModuleTests extends ESTestCase {
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings("foo", indexSettings), null, new AnalysisRegistry(null, environment));
-        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry,
+        IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, indicesQueryCache, mapperRegistry,
             new IndicesFieldDataCache(settings, listener));
         assertTrue(indexService.cache().query() instanceof IndexQueryCache);
         indexService.close("simon says", false);
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java
index 821eaeb..b9bcaf2 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java
@@ -56,7 +56,7 @@ public class MultiFieldCopyToMapperTests extends ESTestCase {
             assertThat(e.getMessage(), equalTo("copy_to in multi fields is not allowed. Found the copy_to in field [c] which is within a multi field."));
         }
 
-        // now test that with an older version the pasring just works
+        // now test that with an older version the parsing just works
         indexVersion = randomFrom(versionsWithAndWithoutExpectedExceptions.v2());
         mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexVersion).build());
         DocumentMapper documentMapper = mapperService.parse("type", new CompressedXContent(mapping.string()), true);
@@ -86,8 +86,8 @@ public class MultiFieldCopyToMapperTests extends ESTestCase {
         return mapping;
     }
 
-    // returs a tuple where
-    // v1 is a list of versions for which we expect an excpetion when a copy_to in multi fields is found and
+    // returns a tuple where
+    // v1 is a list of versions for which we expect an exception when a copy_to in multi fields is found and
     // v2 is older versions where we throw no exception and we just log a warning
     private static Tuple<List<Version>, List<Version>> versionsWithAndWithoutExpectedExceptions() {
         List<Version> versionsWithException = new ArrayList<>();
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java b/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java
index b9d157f..06374a9 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java
@@ -212,7 +212,7 @@ public class MultiFieldTests extends ESSingleNodeTestCase {
         }
     }
 
-    // The fielddata settings need to be the same after deserializing/re-serialsing, else unneccesary mapping sync's can be triggered
+    // The fielddata settings need to be the same after deserializing/re-serialsing, else unnecessary mapping sync's can be triggered
     public void testMultiFieldsFieldDataSettingsInConsistentOrder() throws Exception {
         final String MY_MULTI_FIELD = "multi_field";
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
index e26c07d..f16e004 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
@@ -143,7 +143,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
                 tester.invalidateCoordinate(builder, true);
                 fail("expected exception for broken " + tester.getClass().getName() + " coordinate");
             } catch (IllegalArgumentException e) {
-                // exptected
+                // expected
             }
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index e5f0ade..b5a3855 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -564,7 +564,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         IndicesService indicesService = getInstanceFromNode(IndicesService.class);
         IndexService test = indicesService.indexService("test");
         IndexShard shard = test.getShardOrNull(0);
-        ShardStats stats = new ShardStats(shard.routingEntry(), shard.shardPath(), new CommonStats(shard, new CommonStatsFlags()), shard.commitStats());
+        ShardStats stats = new ShardStats(shard.routingEntry(), shard.shardPath(), new CommonStats(indicesService.getIndicesQueryCache(), shard, new CommonStatsFlags()), shard.commitStats());
         assertEquals(shard.shardPath().getRootDataPath().toString(), stats.getDataPath());
         assertEquals(shard.shardPath().getRootStatePath().toString(), stats.getStatePath());
         assertEquals(shard.shardPath().isCustomDataPath(), stats.isCustomDataPath());
diff --git a/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java
index cd4ca35..8175eb7 100644
--- a/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java
@@ -111,7 +111,7 @@ public class PreBuiltAnalyzerIntegrationIT extends ESIntegTestCase {
         // check that all above configured analyzers have been loaded
         assertThatAnalyzersHaveBeenLoaded(loadedAnalyzers);
 
-        // check that all of the prebuiltanalyzers are still open
+        // check that all of the prebuilt analyzers are still open
         assertLuceneAnalyzersAreNotClosed(loadedAnalyzers);
     }
 
diff --git a/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java b/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
index bc646b3..f09e033 100644
--- a/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInter
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.joda.time.DateTimeZone;
 
 import java.util.List;
 
@@ -51,19 +50,15 @@ public class IndicesRequestCacheIT extends ESIntegTestCase {
         // which used to not work well with the query cache because of the handles stream output
         // see #9500
         final SearchResponse r1 = client().prepareSearch("index").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH)
-                .addAggregation(dateHistogram("histo").field("f").timeZone(DateTimeZone.forID("+01:00")).minDocCount(0)
-                        .dateHistogramInterval(DateHistogramInterval.MONTH))
-                .get();
+            .addAggregation(dateHistogram("histo").field("f").timeZone("+01:00").minDocCount(0).interval(DateHistogramInterval.MONTH)).get();
         assertSearchResponse(r1);
 
         // The cached is actually used
         assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMemorySizeInBytes(), greaterThan(0L));
 
         for (int i = 0; i < 10; ++i) {
-            final SearchResponse r2 = client().prepareSearch("index").setSize(0)
-                    .setSearchType(SearchType.QUERY_THEN_FETCH).addAggregation(dateHistogram("histo").field("f")
-                            .timeZone(DateTimeZone.forID("+01:00")).minDocCount(0).dateHistogramInterval(DateHistogramInterval.MONTH))
-                    .get();
+            final SearchResponse r2 = client().prepareSearch("index").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH)
+                    .addAggregation(dateHistogram("histo").field("f").timeZone("+01:00").minDocCount(0).interval(DateHistogramInterval.MONTH)).get();
             assertSearchResponse(r2);
             Histogram h1 = r1.getAggregations().get("histo");
             Histogram h2 = r2.getAggregations().get("histo");
diff --git a/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java b/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java
index 60a56c1..1a858c3 100644
--- a/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java
@@ -65,7 +65,7 @@ public class FlushIT extends ESIntegTestCase {
                     @Override
                     public void onResponse(FlushResponse flushResponse) {
                         try {
-                            // dont' use assertAllSuccesssful it uses a randomized context that belongs to a different thread
+                            // don't use assertAllSuccessful it uses a randomized context that belongs to a different thread
                             assertThat("Unexpected ShardFailures: " + Arrays.toString(flushResponse.getShardFailures()), flushResponse.getFailedShards(), equalTo(0));
                             latch.countDown();
                         } catch (Throwable ex) {
diff --git a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
index 129c499..948c005 100644
--- a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
@@ -192,7 +192,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
         assertThat(Files.exists(indexDirectory(node_2, "test")), equalTo(false));
 
         // add a transport delegate that will prevent the shard active request to succeed the first time after relocation has finished.
-        // node_1 will then wait for the next cluster state change before it tries a next attempt to delet the shard.
+        // node_1 will then wait for the next cluster state change before it tries a next attempt to delete the shard.
         MockTransportService transportServiceNode_1 = (MockTransportService) internalCluster().getInstance(TransportService.class, node_1);
         TransportService transportServiceNode_2 = internalCluster().getInstance(TransportService.class, node_2);
         final CountDownLatch shardActiveRequestSent = new CountDownLatch(1);
diff --git a/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java b/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java
index 1819325..8e6016f 100644
--- a/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java
+++ b/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java
@@ -63,7 +63,7 @@ public class ProcessProbeTests extends ESTestCase {
         // CPU percent can be negative if the system recent cpu usage is not available
         assertThat(cpu.getPercent(), anyOf(lessThan((short) 0), allOf(greaterThanOrEqualTo((short) 0), lessThanOrEqualTo((short) 100))));
 
-        // CPU time can return -1 if the the platform does not support this operation, let's see which platforms fail
+        // CPU time can return -1 if the platform does not support this operation, let's see which platforms fail
         assertThat(cpu.total, greaterThan(0L));
 
         ProcessStats.Mem mem = stats.getMem();
diff --git a/core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java
index 5c04666..42750a7 100644
--- a/core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java
@@ -55,7 +55,7 @@ import static org.hamcrest.Matchers.nullValue;
  */
 public class ConcurrentPercolatorIT extends ESIntegTestCase {
     public void testSimpleConcurrentPercolator() throws Exception {
-        // We need to index a document / define mapping, otherwise field1 doesn't get reconized as number field.
+        // We need to index a document / define mapping, otherwise field1 doesn't get recognized as number field.
         // If we don't do this, then 'test2' percolate query gets parsed as a TermQuery and not a RangeQuery.
         // The percolate api doesn't parse the doc if no queries have registered, so it can't lazily create a mapping
         assertAcked(prepareCreate("index").addMapping("type", "field1", "type=long", "field2", "type=string")); // random # shards better has a mapping!
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
index 1c5ecb3..91fb8b5 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
@@ -24,7 +24,6 @@ import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.Index;
@@ -88,10 +87,8 @@ public class PercolateDocumentParserTests extends ESTestCase {
         queryShardContext = new QueryShardContext(indexSettings, null, null, null, mapperService, null, null, indicesQueriesRegistry);
 
         HighlightPhase highlightPhase = new HighlightPhase(Settings.EMPTY, new Highlighters());
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet(),
-                new NamedWriteableRegistry());
-        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry),
-                new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry));
+        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet());
+        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers), new AggregationBinaryParseElement(aggregatorParsers));
         parser = new PercolateDocumentParser(highlightPhase, new SortParseElement(), aggregationPhase);
 
         request = Mockito.mock(PercolateShardRequest.class);
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
index 7bac10d..b8ed2cc 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
@@ -165,7 +165,7 @@ public class PercolatorAggregationsIT extends ESIntegTestCase {
                 percolateRequestBuilder.setOnlyCount(countOnly);
             }
 
-            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_a", "a>_count"));
+            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_a").setBucketsPaths("a>_count"));
 
             PercolateResponse response = percolateRequestBuilder.execute().actionGet();
             assertMatchCount(response, expectedCount[i % numUniqueQueries]);
@@ -245,7 +245,7 @@ public class PercolatorAggregationsIT extends ESIntegTestCase {
                 percolateRequestBuilder.setOnlyCount(countOnly);
             }
 
-            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_terms", "terms>_count"));
+            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_terms").setBucketsPaths("terms>_count"));
 
             PercolateResponse response = percolateRequestBuilder.execute().actionGet();
             assertMatchCount(response, numQueries);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
index 29eb13d..178c7ff 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
@@ -19,12 +19,9 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -37,7 +34,7 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
         IndexService index = createIndex("idx");
         client().prepareIndex("idx", "type", "1").setSource("f", 5).execute().get();
         client().admin().indices().prepareRefresh("idx").get();
-
+        
         // simple field aggregation, no scores needed
         String fieldAgg = "{ \"my_terms\": {\"terms\": {\"field\": \"f\"}}}";
         assertFalse(needsScores(index, fieldAgg));
@@ -61,16 +58,12 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
 
     private boolean needsScores(IndexService index, String agg) throws IOException {
         AggregatorParsers parser = getInstanceFromNode(AggregatorParsers.class);
-        IndicesQueriesRegistry queriesRegistry = getInstanceFromNode(IndicesQueriesRegistry.class);
         XContentParser aggParser = JsonXContent.jsonXContent.createParser(agg);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(aggParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
         aggParser.nextToken();
         SearchContext searchContext = createSearchContext(index);
-        AggregationContext aggContext = new AggregationContext(searchContext);
-        final AggregatorFactories factories = parser.parseAggregators(aggParser, parseContext).build(aggContext, null);
-        final Aggregator[] aggregators = factories.createTopLevelAggregators();
+        final AggregatorFactories factories = parser.parseAggregators(aggParser, searchContext);
+        AggregationContext aggregationContext = new AggregationContext(searchContext);
+        final Aggregator[] aggregators = factories.createTopLevelAggregators(aggregationContext);
         assertEquals(1, aggregators.length);
         return aggregators[0].needsScores();
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
new file mode 100644
index 0000000..1b8d0bb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.core.IsNull.notNullValue;
+
+@ESIntegTestCase.SuiteScopeTestCase
+@AwaitsFix(bugUrl = "needs fixing after the search request refactor. Do we need agg binary?")
+// NO RELEASE
+public class AggregationsBinaryIT extends ESIntegTestCase {
+
+    private static final String STRING_FIELD_NAME = "s_value";
+    private static final String INT_FIELD_NAME = "i_value";
+
+    @Override
+    public void setupSuiteScopeCluster() throws Exception {
+        createIndex("idx");
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (int i = 0; i < 5; i++) {
+            builders.add(client().prepareIndex("idx", "type").setSource(
+                    jsonBuilder().startObject().field(STRING_FIELD_NAME, "val" + i).field(INT_FIELD_NAME, i).endObject()));
+        }
+        indexRandom(true, builders);
+        ensureSearchable();
+    }
+
+    public void testAggregationsBinary() throws Exception {
+        TermsBuilder termsBuilder = AggregationBuilders.terms("terms").field(STRING_FIELD_NAME);
+        TermsBuilder subTerm = AggregationBuilders.terms("subterms").field(INT_FIELD_NAME);
+
+        // Create an XContentBuilder from sub aggregation
+        XContentBuilder subTermContentBuilder = JsonXContent.contentBuilder().startObject();
+        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
+        subTermContentBuilder.endObject();
+
+        // Add sub aggregation as a XContentBuilder (binary_aggregation)
+        termsBuilder.subAggregation(subTermContentBuilder);
+
+        SearchResponse response = client().prepareSearch("idx").setTypes("type").addAggregation(termsBuilder).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(bucket.getKeyAsString(), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Aggregations subAggs = bucket.getAggregations();
+            assertThat(subAggs, notNullValue());
+            assertThat(subAggs.asList().size(), equalTo(1));
+            Terms subTerms = subAggs.get("subterms");
+            assertThat(subTerms, notNullValue());
+            List<Bucket> subTermsBuckets = subTerms.getBuckets();
+            assertThat(subTermsBuckets, notNullValue());
+            assertThat(subTermsBuckets.size(), equalTo(1));
+            assertThat(((Number) subTermsBuckets.get(0).getKey()).intValue(), equalTo(i));
+            assertThat(subTermsBuckets.get(0).getDocCount(), equalTo(1L));
+        }
+    }
+
+    public void testAggregationsBinarySameContentType() throws Exception {
+        TermsBuilder termsBuilder = AggregationBuilders.terms("terms").field(STRING_FIELD_NAME);
+        TermsBuilder subTerm = AggregationBuilders.terms("subterms").field(INT_FIELD_NAME);
+
+        // Create an XContentBuilder from sub aggregation
+
+        XContentBuilder subTermContentBuilder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+        subTermContentBuilder.startObject();
+        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
+        subTermContentBuilder.endObject();
+
+        // Add sub aggregation as a XContentBuilder (binary_aggregation)
+        termsBuilder.subAggregation(subTermContentBuilder);
+
+        SearchResponse response = client().prepareSearch("idx").setTypes("type").addAggregation(termsBuilder).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(bucket.getKeyAsString(), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Aggregations subAggs = bucket.getAggregations();
+            assertThat(subAggs, notNullValue());
+            assertThat(subAggs.asList().size(), equalTo(1));
+            Terms subTerms = subAggs.get("subterms");
+            assertThat(subTerms, notNullValue());
+            List<Bucket> subTermsBuckets = subTerms.getBuckets();
+            assertThat(subTermsBuckets, notNullValue());
+            assertThat(subTermsBuckets.size(), equalTo(1));
+            assertThat(((Number) subTermsBuckets.get(0).getKey()).intValue(), equalTo(i));
+            assertThat(subTermsBuckets.get(0).getDocCount(), equalTo(1L));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java
deleted file mode 100644
index bc63939..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregatorParsingTests.java
+++ /dev/null
@@ -1,389 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.MockScriptEngine;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.InternalSettingsPlugin;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import static org.hamcrest.Matchers.containsString;
-
-public class AggregatorParsingTests extends ESTestCase {
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    protected static AggregatorParsers aggParsers;
-    protected static IndicesQueriesRegistry queriesRegistry;
-    protected static ParseFieldMatcher parseFieldMatcher;
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        // we have to prefer CURRENT since with the range of versions we support
-        // it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT
-                : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder().put("node.name", AbstractQueryTestCase.class.toString())
-                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-
-        namedWriteableRegistry = new NamedWriteableRegistry();
-        index = new Index(randomAsciiOfLengthBetween(1, 10), "_na_");
-        Settings indexSettings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder()
-                .put(new IndexMetaData.Builder(index.getName()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        SettingsModule settingsModule = new SettingsModule(settings);
-        settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                        // no file watching, so we don't need a
-                        // ResourceWatcherService
-                        .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections
-                        .singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null,
-                            scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch (IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
-        injector = new ModulesBuilder().add(new EnvironmentModule(new Environment(settings)), settingsModule,
-                new ThreadPoolModule(new ThreadPool(settings)), scriptModule, new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                }, new IndexSettingsModule(index, settings),
-
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        // create some random type with some default field, those types will
-        // stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    public void testTwoTypes() throws Exception {
-        String source = JsonXContent.contentBuilder()
-            .startObject()
-              .startObject("in_stock")
-                  .startObject("filter")
-                      .startObject("range")
-                          .startObject("stock")
-                              .field("gt", 0)
-                          .endObject()
-                      .endObject()
-                  .endObject()
-                  .startObject("terms")
-                      .field("field", "stock")
-                  .endObject()
-              .endObject()
-            .endObject().string();
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (ParsingException e) {
-            assertThat(e.toString(), containsString("Found two aggregation type definitions in [in_stock]: [filter] and [terms]"));
-        }
-    }
-
-    public void testTwoAggs() throws Exception {
-        String source = JsonXContent.contentBuilder()
-          .startObject()
-              .startObject("by_date")
-                  .startObject("date_histogram")
-                      .field("field", "timestamp")
-                      .field("interval", "month")
-                  .endObject()
-                  .startObject("aggs")
-                      .startObject("tag_count")
-                          .startObject("cardinality")
-                              .field("field", "tag")
-                          .endObject()
-                      .endObject()
-                  .endObject()
-                  .startObject("aggs") // 2nd "aggs": illegal
-                      .startObject("tag_count2")
-                          .startObject("cardinality")
-                              .field("field", "tag")
-                          .endObject()
-                      .endObject()
-                  .endObject()
-          .endObject().string();
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (ParsingException e) {
-            assertThat(e.toString(), containsString("Found two sub aggregation definitions under [by_date]"));
-        }
-    }
-
-    public void testInvalidAggregationName() throws Exception {
-        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
-        String name;
-        Random rand = getRandom();
-        int len = randomIntBetween(1, 5);
-        char[] word = new char[len];
-        while (true) {
-            for (int i = 0; i < word.length; i++) {
-                word[i] = (char) rand.nextInt(127);
-            }
-            name = String.valueOf(word);
-            if (!matcher.reset(name).matches()) {
-                break;
-            }
-        }
-
-        String source = JsonXContent.contentBuilder()
-          .startObject()
-              .startObject(name)
-                  .startObject("filter")
-                      .startObject("range")
-                          .startObject("stock")
-                              .field("gt", 0)
-                          .endObject()
-                      .endObject()
-                  .endObject()
-          .endObject().string();
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (ParsingException e) {
-            assertThat(e.toString(), containsString("Invalid aggregation name [" + name + "]"));
-        }
-    }
-
-    public void testSameAggregationName() throws Exception {
-        final String name = randomAsciiOfLengthBetween(1, 10);
-        String source = JsonXContent.contentBuilder()
-          .startObject()
-              .startObject(name)
-                  .startObject("terms")
-                      .field("field", "a")
-                  .endObject()
-              .endObject()
-              .startObject(name)
-                  .startObject("terms")
-                      .field("field", "b")
-                  .endObject()
-              .endObject()
-          .endObject().string();
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (IllegalArgumentException e) {
-            assertThat(e.toString(), containsString("Two sibling aggregations cannot have the same name: [" + name + "]"));
-        }
-    }
-
-    public void testMissingName() throws Exception {
-        String source = JsonXContent.contentBuilder()
-          .startObject()
-              .startObject("by_date")
-                  .startObject("date_histogram")
-                      .field("field", "timestamp")
-                      .field("interval", "month")
-                  .endObject()
-                  .startObject("aggs")
-                      // the aggregation name is missing
-                      //.startObject("tag_count")
-                          .startObject("cardinality")
-                              .field("field", "tag")
-                          .endObject()
-                      //.endObject()
-                  .endObject()
-          .endObject().string();
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (ParsingException e) {
-            // All Good
-        }
-    }
-
-    public void testMissingType() throws Exception {
-        String source = JsonXContent.contentBuilder()
-          .startObject()
-              .startObject("by_date")
-                  .startObject("date_histogram")
-                      .field("field", "timestamp")
-                      .field("interval", "month")
-                  .endObject()
-                  .startObject("aggs")
-                      .startObject("tag_count")
-                          // the aggregation type is missing
-                          //.startObject("cardinality")
-                              .field("field", "tag")
-                          //.endObject()
-                      .endObject()
-                  .endObject()
-          .endObject().string();
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (ParsingException e) {
-            // All Good
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
deleted file mode 100644
index eeffbb7..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
+++ /dev/null
@@ -1,332 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.MockScriptEngine;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.InternalSettingsPlugin;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BaseAggregationTestCase<AB extends AggregatorBuilder<AB>> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    protected static AggregatorParsers aggParsers;
-    protected static IndicesQueriesRegistry queriesRegistry;
-    protected static ParseFieldMatcher parseFieldMatcher;
-
-    protected abstract AB createTestAggregatorBuilder();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT
-                : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("node.name", AbstractQueryTestCase.class.toString())
-                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
-                .build();
-
-        namedWriteableRegistry =  new NamedWriteableRegistry();
-        index = new Index(randomAsciiOfLengthBetween(1, 10), "_na_");
-        Settings indexSettings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder()
-                .put(new IndexMetaData.Builder(index.getName()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        SettingsModule settingsModule = new SettingsModule(settings);
-        settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                        // no file watching, so we don't need a
-                        // ResourceWatcherService
-                        .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections
-                        .singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null,
-                            scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch (IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                settingsModule,
-                new ThreadPoolModule(new ThreadPool(settings)),
-                scriptModule,
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-    public void testFromXContent() throws IOException {
-        AB testAgg = createTestAggregatorBuilder();
-        AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder().addAggregator(testAgg);
-        String contentString = factoriesBuilder.toString();
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name, parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        AggregatorBuilder newAgg = aggParsers.parser(testAgg.getType()).parse(testAgg.name, parser, parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AB testAgg = createTestAggregatorBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorBuilder prototype = (AggregatorBuilder) namedWriteableRegistry.getPrototype(AggregatorBuilder.class,
-                    testAgg.getWriteableName());
-                AggregatorBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AB firstAgg = createTestAggregatorBuilder();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AB secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AB thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AB copyAggregation(AB agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorBuilder prototype = (AggregatorBuilder) namedWriteableRegistry.getPrototype(AggregatorBuilder.class,
-                    agg.getWriteableName());
-                @SuppressWarnings("unchecked")
-                AB secondAgg = (AB) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
deleted file mode 100644
index f180ab5..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
+++ /dev/null
@@ -1,333 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.MockScriptEngine;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.InternalSettingsPlugin;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BasePipelineAggregationTestCase<AF extends PipelineAggregatorBuilder> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static ParseFieldMatcher parseFieldMatcher;
-    private static IndicesQueriesRegistry queriesRegistry;
-
-    protected abstract AF createTestAggregatorFactory();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-     // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT
-                : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("node.name", AbstractQueryTestCase.class.toString())
-                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
-                .build();
-
-        namedWriteableRegistry =  new NamedWriteableRegistry();
-        index = new Index(randomAsciiOfLengthBetween(1, 10), "_na_");
-        Settings indexSettings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder()
-                .put(new IndexMetaData.Builder(index.getName()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        SettingsModule settingsModule = new SettingsModule(settings);
-        settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                        // no file watching, so we don't need a
-                        // ResourceWatcherService
-                        .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections
-                        .singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null,
-                            scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch (IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                settingsModule,
-                new ThreadPoolModule(new ThreadPool(settings)),
-                scriptModule,
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-
-    public void testFromXContent() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder().skipResolveOrder().addPipelineAggregator(testAgg);
-        String contentString = factoriesBuilder.toString();
-        System.out.println(contentString);
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        PipelineAggregatorBuilder newAgg = aggParsers.pipelineAggregator(testAgg.getWriteableName()).parse(testAgg.name(), parser,
-                parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorBuilder prototype = aggParsers.pipelineAggregator(testAgg.getWriteableName()).getFactoryPrototype();
-                PipelineAggregatorBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AF firstAgg = createTestAggregatorFactory();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AF secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AF thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AF copyAggregation(AF agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorBuilder prototype = aggParsers.pipelineAggregator(agg.getWriteableName()).getFactoryPrototype();
-                @SuppressWarnings("unchecked")
-                AF secondAgg = (AF) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
index 1821d09..63008bc 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
@@ -147,10 +147,7 @@ public class MissingValueIT extends ESIntegTestCase {
     }
 
     public void testDateHistogram() {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(
-                        dateHistogram("my_histogram").field("date").dateHistogramInterval(DateHistogramInterval.YEAR).missing("2014-05-07"))
-                .get();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("my_histogram").field("date").interval(DateHistogramInterval.YEAR).missing("2014-05-07")).get();
         assertSearchResponse(response);
         Histogram histogram = response.getAggregations().get("my_histogram");
         assertEquals(2, histogram.getBuckets().size());
@@ -159,10 +156,7 @@ public class MissingValueIT extends ESIntegTestCase {
         assertEquals("2015-01-01T00:00:00.000Z", histogram.getBuckets().get(1).getKeyAsString());
         assertEquals(1, histogram.getBuckets().get(1).getDocCount());
 
-        response = client().prepareSearch("idx")
-                .addAggregation(
-                        dateHistogram("my_histogram").field("date").dateHistogramInterval(DateHistogramInterval.YEAR).missing("2015-05-07"))
-                .get();
+        response = client().prepareSearch("idx").addAggregation(dateHistogram("my_histogram").field("date").interval(DateHistogramInterval.YEAR).missing("2015-05-07")).get();
         assertSearchResponse(response);
         histogram = response.getAggregations().get("my_histogram");
         assertEquals(1, histogram.getBuckets().size());
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
new file mode 100644
index 0000000..1ac06e6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
@@ -0,0 +1,168 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+// NORELEASE move these tests to unit tests when aggs refactoring is done
+//    @Test(expected=SearchPhaseExecutionException.class)
+//    public void testTwoTypes() throws Exception {
+//        createIndex("idx");
+//        ensureGreen();
+//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+//            .startObject()
+//                .startObject("in_stock")
+//                    .startObject("filter")
+//                        .startObject("range")
+//                            .startObject("stock")
+//                                .field("gt", 0)
+//                            .endObject()
+//                        .endObject()
+//                    .endObject()
+//                    .startObject("terms")
+//                        .field("field", "stock")
+//                    .endObject()
+//                .endObject()
+//            .endObject()).execute().actionGet();
+//    }
+//
+//    @Test(expected=SearchPhaseExecutionException.class)
+//    public void testTwoAggs() throws Exception {
+//        createIndex("idx");
+//        ensureGreen();
+//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+//            .startObject()
+//                .startObject("by_date")
+//                    .startObject("date_histogram")
+//                        .field("field", "timestamp")
+//                        .field("interval", "month")
+//                    .endObject()
+//                    .startObject("aggs")
+//                        .startObject("tag_count")
+//                            .startObject("cardinality")
+//                                .field("field", "tag")
+//                            .endObject()
+//                        .endObject()
+//                    .endObject()
+//                    .startObject("aggs") // 2nd "aggs": illegal
+//                        .startObject("tag_count2")
+//                            .startObject("cardinality")
+//                                .field("field", "tag")
+//                            .endObject()
+//                        .endObject()
+//                    .endObject()
+//            .endObject()).execute().actionGet();
+//    }
+//
+//    @Test(expected=SearchPhaseExecutionException.class)
+//    public void testInvalidAggregationName() throws Exception {
+//
+//        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
+//        String name;
+//        SecureRandom rand = new SecureRandom();
+//        int len = randomIntBetween(1, 5);
+//        char[] word = new char[len];
+//        while(true) {
+//            for (int i = 0; i < word.length; i++) {
+//                word[i] = (char) rand.nextInt(127);
+//            }
+//            name = String.valueOf(word);
+//            if (!matcher.reset(name).matches()) {
+//                break;
+//            }
+//        }
+//
+//        createIndex("idx");
+//        ensureGreen();
+//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+//            .startObject()
+//                .startObject(name)
+//                    .startObject("filter")
+//                        .startObject("range")
+//                            .startObject("stock")
+//                                .field("gt", 0)
+//                            .endObject()
+//                        .endObject()
+//                    .endObject()
+//            .endObject()).execute().actionGet();
+//    }
+//
+//    @Test(expected=SearchPhaseExecutionException.class)
+//    public void testSameAggregationName() throws Exception {
+//        createIndex("idx");
+//        ensureGreen();
+//        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
+//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+//            .startObject()
+//                .startObject(name)
+//                    .startObject("terms")
+//                        .field("field", "a")
+//                    .endObject()
+//                .endObject()
+//                .startObject(name)
+//                    .startObject("terms")
+//                        .field("field", "b")
+//                    .endObject()
+//                .endObject()
+//            .endObject()).execute().actionGet();
+//    }
+//
+//    @Test(expected=SearchPhaseExecutionException.class)
+//    public void testMissingName() throws Exception {
+//        createIndex("idx");
+//        ensureGreen();
+//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+//            .startObject()
+//                .startObject("by_date")
+//                    .startObject("date_histogram")
+//                        .field("field", "timestamp")
+//                        .field("interval", "month")
+//                    .endObject()
+//                    .startObject("aggs")
+//                        // the aggregation name is missing
+//                        //.startObject("tag_count")
+//                            .startObject("cardinality")
+//                                .field("field", "tag")
+//                            .endObject()
+//                        //.endObject()
+//                    .endObject()
+//            .endObject()).execute().actionGet();
+//    }
+//
+//    @Test(expected=SearchPhaseExecutionException.class)
+//    public void testMissingType() throws Exception {
+//        createIndex("idx");
+//        ensureGreen();
+//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+//            .startObject()
+//                .startObject("by_date")
+//                    .startObject("date_histogram")
+//                        .field("field", "timestamp")
+//                        .field("interval", "month")
+//                    .endObject()
+//                    .startObject("aggs")
+//                        .startObject("tag_count")
+//                            // the aggregation type is missing
+//                            //.startObject("cardinality")
+//                                .field("field", "tag")
+//                            //.endObject()
+//                        .endObject()
+//                    .endObject()
+//            .endObject()).execute().actionGet();
+//    }
+
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
deleted file mode 100644
index 131144d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SubAggCollectionModeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(SubAggCollectionMode.DEPTH_FIRST.ordinal(), equalTo(0));
-        assertThat(SubAggCollectionMode.BREADTH_FIRST.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.DEPTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.BREADTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.DEPTH_FIRST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.BREADTH_FIRST));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown SubAggCollectionMode ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
index ea8fffd..2dcb79b 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
@@ -134,8 +134,7 @@ public class ChildrenIT extends ESIntegTestCase {
                 .setQuery(matchQuery("randomized", true))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-children("to_comment", "comment")
-                                .subAggregation(
+                                children("to_comment").childType("comment").subAggregation(
                                         terms("commenters").field("commenter").size(0).subAggregation(
                                                 topHits("top_comments")
                                         ))
@@ -176,7 +175,7 @@ children("to_comment", "comment")
                 .setQuery(matchQuery("randomized", false))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-                        children("to_comment", "comment").subAggregation(topHits("top_comments").sort("_uid", SortOrder.ASC))
+                                children("to_comment").childType("comment").subAggregation(topHits("top_comments").addSort("_uid", SortOrder.ASC))
                         )
                 ).get();
         assertSearchResponse(searchResponse);
@@ -251,7 +250,7 @@ children("to_comment", "comment")
 
         for (int i = 0; i < 10; i++) {
             SearchResponse searchResponse = client().prepareSearch(indexName)
-                    .addAggregation(children("children", "child").subAggregation(sum("counts").field("count")))
+                    .addAggregation(children("children").childType("child").subAggregation(sum("counts").field("count")))
                     .get();
 
             assertNoFailures(searchResponse);
@@ -280,7 +279,7 @@ children("to_comment", "comment")
     public void testNonExistingChildType() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .addAggregation(
-children("non-existing", "xyz")
+                        children("non-existing").childType("xyz")
                 ).get();
         assertSearchResponse(searchResponse);
 
@@ -320,7 +319,8 @@ children("non-existing", "xyz")
 
         SearchResponse response = client().prepareSearch(indexName).setTypes(masterType)
                 .setQuery(hasChildQuery(childType, termQuery("color", "orange")))
-.addAggregation(children("my-refinements", childType)
+                .addAggregation(children("my-refinements")
+                                .childType(childType)
                                 .subAggregation(terms("my-colors").field("color"))
                                 .subAggregation(terms("my-sizes").field("size"))
                 ).get();
@@ -371,7 +371,8 @@ children("non-existing", "xyz")
         SearchResponse response = client().prepareSearch(indexName)
                 .setQuery(matchQuery("name", "europe"))
                 .addAggregation(
-                children(parentType, parentType).subAggregation(children(childType, childType).subAggregation(
+                        children(parentType).childType(parentType).subAggregation(
+                                children(childType).childType(childType).subAggregation(
                                         terms("name").field("name")
                                 )
                         )
@@ -419,7 +420,7 @@ children("non-existing", "xyz")
             .setSize(0)
             .addAggregation(AggregationBuilders.terms("towns").field("town")
                 .subAggregation(AggregationBuilders.terms("parent_names").field("name")
-.subAggregation(AggregationBuilders.children("child_docs", "childType"))
+                    .subAggregation(AggregationBuilders.children("child_docs").childType("childType"))
                 )
             )
             .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
deleted file mode 100644
index 3e0941e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.ChildrenAggregatorBuilder;
-
-public class ChildrenTests extends BaseAggregationTestCase<ParentToChildrenAggregator.ChildrenAggregatorBuilder> {
-
-    @Override
-    protected ChildrenAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String childType = randomAsciiOfLengthBetween(5, 40);
-        ChildrenAggregatorBuilder factory = new ChildrenAggregatorBuilder(name, childType);
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
index 2e4a974..0d28bce 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
@@ -21,6 +21,7 @@ package org.elasticsearch.search.aggregations.bucket;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Scorer;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.joda.DateMathParser;
 import org.elasticsearch.common.joda.Joda;
@@ -38,9 +39,9 @@ import org.elasticsearch.script.ScriptModule;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
+import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.lookup.LeafSearchLookup;
 import org.elasticsearch.search.lookup.SearchLookup;
@@ -161,7 +162,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedField() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -196,7 +197,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithTimeZone() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(1).timeZone(DateTimeZone.forID("+01:00"))).execute()
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(1).timeZone("+01:00")).execute()
                 .actionGet();
         DateTimeZone tz = DateTimeZone.forID("+01:00");
         assertSearchResponse(response);
@@ -254,7 +255,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.KEY_ASC))
                 .execute().actionGet();
 
@@ -277,7 +278,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.KEY_DESC))
                 .execute().actionGet();
 
@@ -299,7 +300,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_ASC))
                 .execute().actionGet();
 
@@ -321,7 +322,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
@@ -341,7 +342,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
                         .subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
@@ -396,11 +397,56 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat((double) propertiesCounts[2], equalTo(15.0));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        DateTime key = new DateTime(2012, 1, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 1, 2, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 2, 15, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 3, 23, 0, 0, DateTimeZone.UTC).getMillis()));
+    }
+
     public void testSingleValuedFieldOrderedBySubAggregationAsc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("sum", true))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
@@ -423,7 +469,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("sum", false))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
@@ -442,11 +488,34 @@ public class DateHistogramIT extends ESIntegTestCase {
         }
     }
 
+    public void testSingleValuedFieldOrderedByMultiValuedSubAggregationAscInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .field("date")
+                        .interval(DateHistogramInterval.MONTH)
+                        .order(Histogram.Order.aggregation("stats", "sum", true))
+                        .subAggregation(stats("stats").field("value")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(3));
+
+        int i = 0;
+        for (Histogram.Bucket bucket : histo.getBuckets()) {
+            assertThat(((DateTime) bucket.getKey()), equalTo(new DateTime(2012, i + 1, 1, 0, 0, DateTimeZone.UTC)));
+            i++;
+        }
+    }
+
     public void testSingleValuedFieldOrderedByMultiValuedSubAggregationDesc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("stats", "sum", false))
                         .subAggregation(stats("stats").field("value")))
                 .execute().actionGet();
@@ -470,7 +539,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -514,7 +583,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testMultiValuedField() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("dates").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("dates").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -558,7 +627,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
@@ -604,7 +673,61 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(4));
+
+        DateTime key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+
+        key = new DateTime(2012, 4, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(5L));
+
+        key = new DateTime(2012, 5, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(3);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+    }
+
+    /**
+     * The script will change to document date values to the following:
+     * <p>
+     * doc 1: [ Feb 2, Mar 3]
+     * doc 2: [ Mar 2, Apr 3]
+     * doc 3: [ Mar 15, Apr 16]
+     * doc 4: [ Apr 2, May 3]
+     * doc 5: [ Apr 15, May 16]
+     * doc 6: [ Apr 23, May 24]
+     */
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .field("dates")
+                        .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
+                        .interval(DateHistogramInterval.MONTH).subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -620,6 +743,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 3, 3, 0, 0, DateTimeZone.UTC).getMillis()));
 
         key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
         bucket = buckets.get(1);
@@ -627,6 +753,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 4, 16, 0, 0, DateTimeZone.UTC).getMillis()));
 
         key = new DateTime(2012, 4, 1, 0, 0, DateTimeZone.UTC);
         bucket = buckets.get(2);
@@ -634,6 +763,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(5L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 5, 24, 0, 0, DateTimeZone.UTC).getMillis()));
 
         key = new DateTime(2012, 5, 1, 0, 0, DateTimeZone.UTC);
         bucket = buckets.get(3);
@@ -641,6 +773,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 5, 24, 0, 0, DateTimeZone.UTC).getMillis()));
     }
 
     /**
@@ -653,7 +788,7 @@ public class DateHistogramIT extends ESIntegTestCase {
      */
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -686,9 +821,54 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(3L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        DateTime key = new DateTime(2012, 1, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 1, 2, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 2, 15, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 3, 23, 0, 0, DateTimeZone.UTC).getMillis()));
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -739,9 +919,64 @@ public class DateHistogramIT extends ESIntegTestCase {
     [ Mar 23, Apr 24]
      */
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(4));
+
+        DateTime key = new DateTime(2012, 1, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 2, 3, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 3, 16, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(5L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 4, 24, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 4, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(3);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 4, 24, 0, 0, DateTimeZone.UTC).getMillis()));
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -754,7 +989,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testPartiallyUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -790,8 +1025,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(dateHistogram("date_histo").field("value").interval(1)))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(dateHistogram("date_histo").interval(1)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -825,8 +1059,8 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(dateHistogram("date_histo")
                         .field("date")
-                        .timeZone(DateTimeZone.forID("-02:00"))
-                        .dateHistogramInterval(DateHistogramInterval.DAY)
+                        .timeZone("-02:00")
+                        .interval(DateHistogramInterval.DAY)
                         .format("yyyy-MM-dd:HH-mm-ssZZ"))
                 .execute().actionGet();
 
@@ -919,10 +1153,10 @@ public class DateHistogramIT extends ESIntegTestCase {
             response = client().prepareSearch("idx2")
                     .addAggregation(dateHistogram("histo")
                             .field("date")
-                            .dateHistogramInterval(DateHistogramInterval.days(interval))
+                            .interval(DateHistogramInterval.days(interval))
                             .minDocCount(0)
                                     // when explicitly specifying a format, the extended bounds should be defined by the same format
-                            .extendedBounds(new ExtendedBounds(format(boundsMin, pattern), format(boundsMax, pattern)))
+                            .extendedBounds(format(boundsMin, pattern), format(boundsMax, pattern))
                             .format(pattern))
                     .execute().actionGet();
 
@@ -995,8 +1229,8 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .prepareSearch(index)
                 .setQuery(QueryBuilders.rangeQuery("date").from("now/d").to("now/d").includeLower(true).includeUpper(true).timeZone(timezone.getID()))
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.hours(1)).timeZone(timezone).minDocCount(0)
-                                .extendedBounds(new ExtendedBounds("now/d", "now/d+23h"))
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.hours(1)).timeZone(timezone.getID()).minDocCount(0)
+                                .extendedBounds("now/d", "now/d+23h")
                 ).execute().actionGet();
         assertSearchResponse(response);
 
@@ -1034,7 +1268,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(dateHistogram("date_histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertSearchHits(response, "0", "1", "2", "3", "4");
@@ -1053,7 +1287,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testIssue6965() {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").timeZone(DateTimeZone.forID("+01:00")).dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0))
+                .addAggregation(dateHistogram("histo").field("date").timeZone("+01:00").interval(DateHistogramInterval.MONTH).minDocCount(0))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -1094,7 +1328,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 client().prepareIndex("test9491", "type").setSource("d", "2014-11-08T13:00:00Z"));
         ensureSearchable("test9491");
         SearchResponse response = client().prepareSearch("test9491")
-                .addAggregation(dateHistogram("histo").field("d").dateHistogramInterval(DateHistogramInterval.YEAR).timeZone(DateTimeZone.forID("Asia/Jerusalem")))
+                .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.YEAR).timeZone("Asia/Jerusalem"))
                 .execute().actionGet();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
@@ -1111,7 +1345,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 client().prepareIndex("test8209", "type").setSource("d", "2014-04-30T00:00:00Z"));
         ensureSearchable("test8209");
         SearchResponse response = client().prepareSearch("test8209")
-                .addAggregation(dateHistogram("histo").field("d").dateHistogramInterval(DateHistogramInterval.MONTH).timeZone(DateTimeZone.forID("CET"))
+                .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.MONTH).timeZone("CET")
                         .minDocCount(0))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -1129,7 +1363,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     }
 
     /**
-     * see issue #9634, negative dateHistogramInterval in date_histogram should raise exception
+     * see issue #9634, negative interval in date_histogram should raise exception
      */
     public void testExceptionOnNegativeInterval() {
         try {
@@ -1137,13 +1371,13 @@ public class DateHistogramIT extends ESIntegTestCase {
                     .addAggregation(dateHistogram("histo").field("date").interval(-TimeUnit.DAYS.toMillis(1)).minDocCount(0)).execute()
                     .actionGet();
             fail();
-        } catch (IllegalArgumentException e) {
-            assertThat(e.toString(), containsString("[interval] must be 1 or greater for histogram aggregation [histo]"));
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("ElasticsearchParseException"));
         }
     }
 
     public void testTimestampField() { // see #11692
-        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("histo").field("_timestamp").dateHistogramInterval(randomFrom(DateHistogramInterval.DAY, DateHistogramInterval.MONTH))).get();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("histo").field("_timestamp").interval(randomFrom(DateHistogramInterval.DAY, DateHistogramInterval.MONTH))).get();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
         assertThat(histo.getBuckets().size(), greaterThan(0));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
index b3b85e0..e1167ce 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
@@ -100,7 +100,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .field("date")
                         .offset("2h")
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(5L));
@@ -122,7 +122,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .field("date")
                         .offset("-2h")
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(5L));
@@ -149,7 +149,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .offset("6h")
                         .minDocCount(0)
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(24L));
@@ -166,7 +166,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
     }
 
     /**
-     * @param bucket the bucket to check asssertions for
+     * @param bucket the bucket to check assertions for
      * @param key the expected key
      * @param expectedSize the expected size of the bucket
      */
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java
deleted file mode 100644
index 0f2411f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramAggregatorBuilder;
-
-public class DateHistogramTests extends BaseAggregationTestCase<DateHistogramAggregatorBuilder> {
-
-    @Override
-    protected DateHistogramAggregatorBuilder createTestAggregatorBuilder() {
-        DateHistogramAggregatorBuilder factory = new DateHistogramAggregatorBuilder("foo");
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.interval(randomIntBetween(1, 100000));
-        } else {
-            if (randomBoolean()) {
-                factory.dateHistogramInterval(randomFrom(DateHistogramInterval.YEAR, DateHistogramInterval.QUARTER,
-                        DateHistogramInterval.MONTH, DateHistogramInterval.WEEK, DateHistogramInterval.DAY, DateHistogramInterval.HOUR,
-                        DateHistogramInterval.MINUTE, DateHistogramInterval.SECOND));
-            } else {
-                int branch = randomInt(4);
-                switch (branch) {
-                case 0:
-                    factory.dateHistogramInterval(DateHistogramInterval.seconds(randomIntBetween(1, 1000)));
-                    break;
-                case 1:
-                    factory.dateHistogramInterval(DateHistogramInterval.minutes(randomIntBetween(1, 1000)));
-                    break;
-                case 2:
-                    factory.dateHistogramInterval(DateHistogramInterval.hours(randomIntBetween(1, 1000)));
-                    break;
-                case 3:
-                    factory.dateHistogramInterval(DateHistogramInterval.days(randomIntBetween(1, 1000)));
-                    break;
-                case 4:
-                    factory.dateHistogramInterval(DateHistogramInterval.weeks(randomIntBetween(1, 1000)));
-                    break;
-                default:
-                    throw new IllegalStateException("invalid branch: " + branch);
-                }
-            }
-        }
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
deleted file mode 100644
index 94156fc..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorBuilder;
-
-public class DateRangeTests extends BaseAggregationTestCase<DateRangeAggregatorBuilder> {
-
-    @Override
-    protected DateRangeAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        DateRangeAggregatorBuilder factory = new DateRangeAggregatorBuilder("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                factory.addRange(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                factory.addRange(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
deleted file mode 100644
index 1b72944..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
+++ /dev/null
@@ -1,241 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.index.query.TermQueryBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.DiversifiedAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-
-/**
- * Tests the Sampler aggregation
- */
-@ESIntegTestCase.SuiteScopeTestCase
-public class DiversifiedSamplerIT extends ESIntegTestCase {
-
-    public static final int NUM_SHARDS = 2;
-
-    public String randomExecutionHint() {
-        return randomBoolean() ? null : randomFrom(SamplerAggregator.ExecutionMode.values()).toString();
-    }
-
-
-    @Override
-    public void setupSuiteScopeCluster() throws Exception {
-        assertAcked(prepareCreate("test").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0).addMapping(
-                "book", "author", "type=string,index=not_analyzed", "name", "type=string,index=analyzed", "genre",
-                "type=string,index=not_analyzed", "price", "type=float"));
-        createIndex("idx_unmapped");
-        // idx_unmapped_author is same as main index but missing author field
-        assertAcked(prepareCreate("idx_unmapped_author").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0)
-                .addMapping("book", "name", "type=string,index=analyzed", "genre", "type=string,index=not_analyzed", "price",
-                        "type=float"));
-
-        ensureGreen();
-        String data[] = {
-                // "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s",
-                "0553573403,book,A Game of Thrones,7.99,true,George R.R. Martin,A Song of Ice and Fire,1,fantasy",
-                "0553579908,book,A Clash of Kings,7.99,true,George R.R. Martin,A Song of Ice and Fire,2,fantasy",
-                "055357342X,book,A Storm of Swords,7.99,true,George R.R. Martin,A Song of Ice and Fire,3,fantasy",
-                "0553293354,book,Foundation,17.99,true,Isaac Asimov,Foundation Novels,1,scifi",
-                "0812521390,book,The Black Company,6.99,false,Glen Cook,The Chronicles of The Black Company,1,fantasy",
-                "0812550706,book,Ender's Game,6.99,true,Orson Scott Card,Ender,1,scifi",
-                "0441385532,book,Jhereg,7.95,false,Steven Brust,Vlad Taltos,1,fantasy",
-                "0380014300,book,Nine Princes In Amber,6.99,true,Roger Zelazny,the Chronicles of Amber,1,fantasy",
-                "0805080481,book,The Book of Three,5.99,true,Lloyd Alexander,The Chronicles of Prydain,1,fantasy",
-                "080508049X,book,The Black Cauldron,5.99,true,Lloyd Alexander,The Chronicles of Prydain,2,fantasy"
-
-            };
-
-        for (int i = 0; i < data.length; i++) {
-            String[] parts = data[i].split(",");
-            client().prepareIndex("test", "book", "" + i)
-                    .setSource("author", parts[5], "name", parts[2], "genre", parts[8], "price", Float.parseFloat(parts[3])).get();
-            client().prepareIndex("idx_unmapped_author", "book", "" + i)
-                    .setSource("name", parts[2], "genre", parts[8], "price", Float.parseFloat(parts[3])).get();
-        }
-        client().admin().indices().refresh(new RefreshRequest("test")).get();
-    }
-
-    public void testIssue10719() throws Exception {
-        // Tests that we can refer to nested elements under a sample in a path
-        // statement
-        boolean asc = randomBoolean();
-        SearchResponse response = client().prepareSearch("test").setTypes("book").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("genres")
-                        .field("genre")
-                        .order(Terms.Order.aggregation("sample>max_price.value", asc))
-                        .subAggregation(sampler("sample").shardSize(100)
-                                .subAggregation(max("max_price").field("price")))
-                ).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        // For this test to be useful we need >1 genre bucket to compare
-        assertThat(genreBuckets.size(), greaterThan(1));
-        double lastMaxPrice = asc ? Double.MIN_VALUE : Double.MAX_VALUE;
-        for (Terms.Bucket genreBucket : genres.getBuckets()) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Max maxPriceInGenre = sample.getAggregations().get("max_price");
-            double price = maxPriceInGenre.getValue();
-            if (asc) {
-                assertThat(price, greaterThanOrEqualTo(lastMaxPrice));
-            } else {
-                assertThat(price, lessThanOrEqualTo(lastMaxPrice));
-            }
-            lastMaxPrice = price;
-        }
-
-    }
-
-    public void testSimpleDiversity() throws Exception {
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("test")
-                .setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy"))
-                .setFrom(0).setSize(60)
-                .addAggregation(sampleAgg)
-                .execute()
-                .actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        Terms authors = sample.getAggregations().get("authors");
-        Collection<Bucket> testBuckets = authors.getBuckets();
-
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testNestedDiversity() throws Exception {
-        // Test multiple samples gathered under buckets made by a parent agg
-        int MAX_DOCS_PER_AUTHOR = 1;
-        TermsAggregatorBuilder rootTerms = terms("genres").field("genre");
-
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-
-        rootTerms.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(rootTerms).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        for (Terms.Bucket genreBucket : genreBuckets) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Terms authors = sample.getAggregations().get("authors");
-            Collection<Bucket> testBuckets = authors.getBuckets();
-
-            for (Terms.Bucket testBucket : testBuckets) {
-                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-            }
-        }
-    }
-
-    public void testNestedSamples() throws Exception {
-        // Test samples nested under samples
-        int MAX_DOCS_PER_AUTHOR = 1;
-        int MAX_DOCS_PER_GENRE = 2;
-        DiversifiedAggregatorBuilder rootSample = new DiversifiedAggregatorBuilder("genreSample").shardSize(100)
-                .field("genre")
-                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
-
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        sampleAgg.subAggregation(terms("genres").field("genre"));
-
-        rootSample.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler genreSample = response.getAggregations().get("genreSample");
-        Sampler sample = genreSample.getAggregations().get("sample");
-
-        Terms genres = sample.getAggregations().get("genres");
-        Collection<Bucket> testBuckets = genres.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
-        }
-
-        Terms authors = sample.getAggregations().get("authors");
-        testBuckets = authors.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testPartiallyUnmappedDiversifyField() throws Exception {
-        // One of the indexes is missing the "author" field used for
-        // diversifying results
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100).field("author")
-                .maxDocsPerValue(1);
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), greaterThan(0L));
-        Terms authors = sample.getAggregations().get("authors");
-        assertThat(authors.getBuckets().size(), greaterThan(0));
-    }
-
-    public void testWhollyUnmappedDiversifyField() throws Exception {
-        //All of the indices are missing the "author" field used for diversifying results
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), equalTo(0L));
-        Terms authors = sample.getAggregations().get("authors");
-        assertNull(authors);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
deleted file mode 100644
index a900496..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.ExecutionMode;
-
-public class DiversifiedSamplerTests extends BaseAggregationTestCase<SamplerAggregator.DiversifiedAggregatorBuilder> {
-
-    @Override
-    protected final SamplerAggregator.DiversifiedAggregatorBuilder createTestAggregatorBuilder() {
-        SamplerAggregator.DiversifiedAggregatorBuilder factory = new SamplerAggregator.DiversifiedAggregatorBuilder("foo");
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.maxDocsPerValue(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(ExecutionMode.values()).toString());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
index 9a4b5ec..38cc24a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
@@ -93,7 +93,7 @@ public class FilterIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("tag1", termQuery("tag", "tag1")))
+                .addAggregation(filter("tag1").filter(termQuery("tag", "tag1")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -109,7 +109,7 @@ public class FilterIT extends ESIntegTestCase {
     // https://github.com/elasticsearch/elasticsearch/issues/8438
     public void testEmptyFilterDeclarations() throws Exception {
         QueryBuilder emptyFilter = new BoolQueryBuilder();
-        SearchResponse response = client().prepareSearch("idx").addAggregation(filter("tag1", emptyFilter)).execute().actionGet();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(filter("tag1").filter(emptyFilter)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -120,7 +120,8 @@ public class FilterIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("tag1", termQuery("tag", "tag1"))
+                .addAggregation(filter("tag1")
+                        .filter(termQuery("tag", "tag1"))
                         .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
@@ -149,7 +150,7 @@ public class FilterIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field("value").interval(2L).subAggregation(
-                                filter("filter", matchAllQuery()))).get();
+                                filter("filter").filter(matchAllQuery()))).get();
 
         assertSearchResponse(response);
 
@@ -167,7 +168,8 @@ public class FilterIT extends ESIntegTestCase {
     public void testWithContextBasedSubAggregation() throws Exception {
         try {
             client().prepareSearch("idx")
-                    .addAggregation(filter("tag1", termQuery("tag", "tag1"))
+                    .addAggregation(filter("tag1")
+                            .filter(termQuery("tag", "tag1"))
                             .subAggregation(avg("avg_value")))
                     .execute().actionGet();
 
@@ -183,7 +185,7 @@ public class FilterIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(filter("filter", matchAllQuery())))
+                        .subAggregation(filter("filter").filter(matchAllQuery())))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
index bf25201..ab44d6c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.filters.Filters;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -110,8 +109,11 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testSimple() throws Exception {
-        SearchResponse response = client().prepareSearch("idx").addAggregation(
-                filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2"))))
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(
+                        filters("tags")
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -134,10 +136,10 @@ public class FiltersIT extends ESIntegTestCase {
     // See NullPointer issue when filters are empty:
     // https://github.com/elasticsearch/elasticsearch/issues/8438
     public void testEmptyFilterDeclarations() throws Exception {
-        QueryBuilder<?> emptyFilter = new BoolQueryBuilder();
+        QueryBuilder emptyFilter = new BoolQueryBuilder();
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("all", emptyFilter), new KeyedFilter("tag1", termQuery("tag", "tag1"))))
-                .execute().actionGet();
+                .addAggregation(filters("tags").filter("all", emptyFilter).filter("tag1", termQuery("tag", "tag1"))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -153,8 +155,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).subAggregation(avg("avg_value").field("value")))
+                .addAggregation(
+                        filters("tags")
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2"))
+                                .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -205,7 +210,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field("value").interval(2L).subAggregation(
-                                filters("filters", matchAllQuery()))).get();
+                                filters("filters").filter(matchAllQuery()))).get();
 
         assertSearchResponse(response);
 
@@ -227,7 +232,9 @@ public class FiltersIT extends ESIntegTestCase {
         try {
             client().prepareSearch("idx")
                     .addAggregation(
-                    filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2")))
+                            filters("tags")
+                                    .filter("tag1", termQuery("tag", "tag1"))
+                                    .filter("tag2", termQuery("tag", "tag2"))
                                     .subAggregation(avg("avg_value"))
                     )
                     .execute().actionGet();
@@ -244,7 +251,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(filters("filters", new KeyedFilter("all", matchAllQuery()))))
+                        .subAggregation(filters("filters").filter("all", matchAllQuery())))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -263,7 +270,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testSimpleNonKeyed() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", termQuery("tag", "tag1"), termQuery("tag", "tag2"))).execute().actionGet();
+                .addAggregation(
+                        filters("tags")
+                                .filter(termQuery("tag", "tag1"))
+                                .filter(termQuery("tag", "tag2")))
+                .execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -286,9 +297,12 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testOtherBucket() throws Exception {
-        SearchResponse response = client().prepareSearch("idx").addAggregation(
-                filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2")))
-                        .otherBucket(true))
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                        .filter("tag1", termQuery("tag", "tag1"))
+                        .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -313,9 +327,12 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testOtherNamedBucket() throws Exception {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).otherBucket(true).otherBucketKey("foobar"))
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        filters("tags").otherBucketKey("foobar")
+                        .filter("tag1", termQuery("tag", "tag1"))
+                        .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -341,8 +358,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testOtherNonKeyed() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", termQuery("tag", "tag1"), termQuery("tag", "tag2")).otherBucket(true)).execute()
-                .actionGet();
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                                .filter(termQuery("tag", "tag1"))
+                                .filter(termQuery("tag", "tag2")))
+                .execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -370,8 +390,10 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testOtherWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).otherBucket(true)
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2"))
                                 .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
index 80211eb..5d52d14 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
@@ -22,7 +22,6 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -141,9 +140,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -188,9 +188,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testSimpleWithCustomKeys() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo("ring1", 500)
                         .addRange("ring2", 500, 1000)
                         .addUnboundedFrom("ring3", 1000))
@@ -237,9 +238,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
         client().admin().cluster().prepareHealth("idx_unmapped").setWaitForYellowStatus().execute().actionGet();
 
         SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -284,9 +286,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testPartiallyUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -331,9 +334,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000)
@@ -418,7 +422,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(geoDistance("geo_dist", new GeoPoint(52.3760, 4.894)).field("location").addRange("0-100", 0.0, 100.0)))
+                        .subAggregation(geoDistance("geo_dist").field("location").point("52.3760, 4.894").addRange("0-100", 0.0, 100.0)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -443,10 +447,11 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testMultiValues() throws Exception {
         SearchResponse response = client().prepareSearch("idx-multi")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
                         .distanceType(org.elasticsearch.common.geo.GeoDistance.ARC)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
deleted file mode 100644
index 39eccf4..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.Range;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-
-public class GeoDistanceRangeTests extends BaseAggregationTestCase<GeoDistanceAggregatorBuilder> {
-
-    @Override
-    protected GeoDistanceAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        GeoPoint origin = RandomShapeGenerator.randomPoint(getRandom());
-        GeoDistanceAggregatorBuilder factory = new GeoDistanceAggregatorBuilder("foo", origin);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? 0 : randomIntBetween(0, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.compare(from, 0) == 0 ? randomIntBetween(0, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            factory.addRange(new Range(key, from, to));
-        }
-        factory.field(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0, 0");
-        }
-        if (randomBoolean()) {
-            factory.unit(randomFrom(DistanceUnit.values()));
-        }
-        if (randomBoolean()) {
-            factory.distanceType(randomFrom(GeoDistance.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
index c8658cc..672447a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
@@ -204,7 +204,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
         for (int precision = 1; precision <= GeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(
-                            AggregationBuilders.filter("filtered", bbox)
+                            AggregationBuilders.filter("filtered").filter(bbox)
                                     .subAggregation(
                                             geohashGrid("geohashgrid")
                                                     .field("location")
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
deleted file mode 100644
index f8f74ed..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridAggregatorBuilder;
-
-public class GeoHashGridTests extends BaseAggregationTestCase<GeoGridAggregatorBuilder> {
-
-    @Override
-    protected GeoGridAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        GeoGridAggregatorBuilder factory = new GeoGridAggregatorBuilder(name);
-        if (randomBoolean()) {
-            int precision = randomIntBetween(1, 12);
-            factory.precision(precision);
-        }
-        if (randomBoolean()) {
-            int size = randomInt(5);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            }
-            factory.size(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(5);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            }
-            factory.shardSize(shardSize);
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
deleted file mode 100644
index c5dd307..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator.GlobalAggregatorBuilder;
-
-public class GlobalTests extends BaseAggregationTestCase<GlobalAggregator.GlobalAggregatorBuilder> {
-
-    @Override
-    protected GlobalAggregatorBuilder createTestAggregatorBuilder() {
-        return new GlobalAggregator.GlobalAggregatorBuilder(randomAsciiOfLengthBetween(3, 20));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
deleted file mode 100644
index dbb54ce..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.HistogramAggregatorBuilder;
-
-public class HistogramTests extends BaseAggregationTestCase<HistogramAggregator.HistogramAggregatorBuilder> {
-
-    @Override
-    protected HistogramAggregatorBuilder createTestAggregatorBuilder() {
-        HistogramAggregatorBuilder factory = new HistogramAggregatorBuilder("foo");
-        factory.field(INT_FIELD_NAME);
-        factory.interval(randomIntBetween(1, 100000));
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
deleted file mode 100644
index 507e877..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorBuilder.Range;
-
-public class IPv4RangeTests extends BaseAggregationTestCase<IPv4RangeAggregatorBuilder> {
-
-    @Override
-    protected IPv4RangeAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        IPv4RangeAggregatorBuilder factory = new IPv4RangeAggregatorBuilder("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            if (randomBoolean()) {
-                double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-                double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                        : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                                : randomIntBetween((int) from, Integer.MAX_VALUE));
-                if (randomBoolean()) {
-                    factory.addRange(new Range(key, from, to));
-                } else {
-                    String fromAsStr = Double.isInfinite(from) ? null : IpFieldMapper.longToIp((long) from);
-                    String toAsStr = Double.isInfinite(to) ? null : IpFieldMapper.longToIp((long) to);
-                    factory.addRange(new Range(key, fromAsStr, toAsStr));
-                }
-            } else {
-                int mask = randomInt(32);
-                long ipAsLong = randomIntBetween(0, Integer.MAX_VALUE);
-
-                long blockSize = 1L << (32 - mask);
-                ipAsLong = ipAsLong - (ipAsLong & (blockSize - 1));
-                String cidr = Cidrs.createCIDR(ipAsLong, mask);
-                factory.addRange(new Range(key, cidr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java
index 97eaf2f..85e0c58 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java
@@ -160,11 +160,32 @@ public class MissingIT extends ESIntegTestCase {
         assertThat((double) missing.getProperty("avg_value.value"), equalTo((double) sum / (numDocsMissing + numDocsUnmapped)));
     }
 
+    public void testWithInheritedSubMissing() throws Exception {
+        SearchResponse response = client().prepareSearch("idx", "unmapped_idx")
+                .addAggregation(missing("top_missing").field("tag")
+                        .subAggregation(missing("sub_missing")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Missing topMissing = response.getAggregations().get("top_missing");
+        assertThat(topMissing, notNullValue());
+        assertThat(topMissing.getName(), equalTo("top_missing"));
+        assertThat(topMissing.getDocCount(), equalTo((long) numDocsMissing + numDocsUnmapped));
+        assertThat(topMissing.getAggregations().asList().isEmpty(), is(false));
+
+        Missing subMissing = topMissing.getAggregations().get("sub_missing");
+        assertThat(subMissing, notNullValue());
+        assertThat(subMissing.getName(), equalTo("sub_missing"));
+        assertThat(subMissing.getDocCount(), equalTo((long) numDocsMissing + numDocsUnmapped));
+    }
+
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(missing("missing").field("value")))
+                        .subAggregation(missing("missing")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
index d303a2e..bfd6837 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
@@ -26,12 +26,9 @@ import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -48,10 +45,8 @@ public class NaNSortingIT extends ESIntegTestCase {
     private enum SubAggregation {
         AVG("avg") {
             @Override
-            public AvgAggregator.AvgAggregatorBuilder builder() {
-                AvgAggregator.AvgAggregatorBuilder factory = avg(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return avg(name).field("numeric_field");
             }
             @Override
             public double getValue(Aggregation aggregation) {
@@ -60,10 +55,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         VARIANCE("variance") {
             @Override
-            public ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder builder() {
-                ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -76,10 +69,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         STD_DEVIATION("std_deviation"){
             @Override
-            public ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder builder() {
-                ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -97,7 +88,7 @@ public class NaNSortingIT extends ESIntegTestCase {
 
         public String name;
 
-        public abstract ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, ? extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, ?>> builder();
+        public abstract MetricsAggregationBuilder<?> builder();
 
         public String sortKey() {
             return name;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
index cd68fab..5317f2e 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
@@ -167,7 +167,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "nested")
+                .addAggregation(nested("nested").path("nested")
                         .subAggregation(stats("nested_value_stats").field("nested.value")))
                 .execute().actionGet();
 
@@ -205,7 +205,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNonExistingNestedField() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "value")
+                .addAggregation(nested("nested").path("value")
                         .subAggregation(stats("nested_value_stats").field("nested.value")))
                 .execute().actionGet();
 
@@ -217,7 +217,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNestedWithSubTermsAgg() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "nested")
+                .addAggregation(nested("nested").path("nested")
                         .subAggregation(terms("values").field("nested.value").size(100)
                                 .collectMode(aggCollectionMode)))
                 .execute().actionGet();
@@ -270,7 +270,7 @@ public class NestedIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(terms("top_values").field("value").size(100)
                         .collectMode(aggCollectionMode)
-                        .subAggregation(nested("nested", "nested")
+                        .subAggregation(nested("nested").path("nested")
                                 .subAggregation(max("max_value").field("nested.value"))))
                 .execute().actionGet();
 
@@ -296,10 +296,10 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNestNestedAggs() throws Exception {
         SearchResponse response = client().prepareSearch("idx_nested_nested_aggs")
-                .addAggregation(nested("level1", "nested1")
+                .addAggregation(nested("level1").path("nested1")
                         .subAggregation(terms("a").field("nested1.a")
                                 .collectMode(aggCollectionMode)
-                        .subAggregation(nested("level2", "nested1.nested2")
+                                .subAggregation(nested("level2").path("nested1.nested2")
                                         .subAggregation(sum("sum").field("nested1.nested2.b")))))
                 .get();
         assertSearchResponse(response);
@@ -333,7 +333,7 @@ public class NestedIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(nested("nested", "nested")))
+                        .subAggregation(nested("nested").path("nested")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -352,7 +352,7 @@ public class NestedIT extends ESIntegTestCase {
         try {
             client().prepareSearch("idx")
                     .setQuery(matchAllQuery())
-                    .addAggregation(nested("object_field", "incorrect"))
+                    .addAggregation(nested("object_field").path("incorrect"))
                     .execute().actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
@@ -407,10 +407,9 @@ public class NestedIT extends ESIntegTestCase {
                         terms("startDate").field("dates.month.start").subAggregation(
                                 terms("endDate").field("dates.month.end").subAggregation(
                                         terms("period").field("dates.month.label").subAggregation(
-                                                nested("ctxt_idfier_nested", "comments")
-                                                .subAggregation(filter("comment_filter", termQuery("comments.identifier", "29111"))
-                                                        .subAggregation(nested("nested_tags", "comments.tags")
-                                                                .subAggregation(
+                                                nested("ctxt_idfier_nested").path("comments").subAggregation(
+                                                        filter("comment_filter").filter(termQuery("comments.identifier", "29111")).subAggregation(
+                                                                nested("nested_tags").path("comments.tags").subAggregation(
                                                                         terms("tag").field("comments.tags.name")
                                                                 )
                                                         )
@@ -489,7 +488,7 @@ public class NestedIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("idx4").setTypes("product")
                 .addAggregation(terms("category").field("categories").subAggregation(
-                        nested("property", "property").subAggregation(
+                        nested("property").path("property").subAggregation(
                                 terms("property_id").field("property.id")
                         )
                 ))
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
deleted file mode 100644
index 159f491..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.RangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-
-public class RangeTests extends BaseAggregationTestCase<RangeAggregator.RangeAggregatorBuilder> {
-
-    @Override
-    protected RangeAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        RangeAggregatorBuilder factory = new RangeAggregatorBuilder("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                factory.addRange(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                factory.addRange(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
index 5f38f1b..e82a737 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
@@ -138,7 +138,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleReverseNestedToRoot() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type1")
-                .addAggregation(nested("nested1", "nested1")
+                .addAggregation(nested("nested1").path("nested1")
                         .subAggregation(
                                 terms("field2").field("nested1.field2")
                                         .subAggregation(
@@ -326,10 +326,10 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleNested1ToRootToNested2() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type2")
-                .addAggregation(nested("nested1", "nested1")
+                .addAggregation(nested("nested1").path("nested1")
                                 .subAggregation(
                                         reverseNested("nested1_to_root")
-                                                .subAggregation(nested("root_to_nested2", "nested1.nested2"))
+                                                .subAggregation(nested("root_to_nested2").path("nested1.nested2"))
                                         )
                                 )
                 .get();
@@ -348,7 +348,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleReverseNestedToNested1() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type2")
-                .addAggregation(nested("nested1", "nested1.nested2")
+                .addAggregation(nested("nested1").path("nested1.nested2")
                                 .subAggregation(
                                         terms("field2").field("nested1.nested2.field2").order(Terms.Order.term(true))
                                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
@@ -470,7 +470,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
     public void testNonExistingNestedField() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(nested("nested2", "nested1.nested2").subAggregation(reverseNested("incorrect").path("nested3")))
+                .addAggregation(nested("nested2").path("nested1.nested2").subAggregation(reverseNested("incorrect").path("nested3")))
                 .execute().actionGet();
 
         Nested nested = searchResponse.getAggregations().get("nested2");
@@ -558,11 +558,11 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("idx3")
                 .addAggregation(
-                        nested("nested_0", "category").subAggregation(
+                        nested("nested_0").path("category").subAggregation(
                                 terms("group_by_category").field("category.name").subAggregation(
                                         reverseNested("to_root").subAggregation(
-                                                nested("nested_1", "sku").subAggregation(
-                                                        filter("filter_by_sku", termQuery("sku.sku_type", "bar1")).subAggregation(
+                                                nested("nested_1").path("sku").subAggregation(
+                                                        filter("filter_by_sku").filter(termQuery("sku.sku_type", "bar1")).subAggregation(
                                                                 count("sku_count").field("sku.sku_type")
                                                         )
                                                 )
@@ -593,13 +593,13 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
         response = client().prepareSearch("idx3")
                 .addAggregation(
-                        nested("nested_0", "category").subAggregation(
+                        nested("nested_0").path("category").subAggregation(
                                 terms("group_by_category").field("category.name").subAggregation(
                                         reverseNested("to_root").subAggregation(
-                                                nested("nested_1", "sku").subAggregation(
-                                                        filter("filter_by_sku", termQuery("sku.sku_type", "bar1")).subAggregation(
-                                                                nested("nested_2", "sku.colors").subAggregation(
-                                                                        filter("filter_sku_color", termQuery("sku.colors.name", "red")).subAggregation(
+                                                nested("nested_1").path("sku").subAggregation(
+                                                        filter("filter_by_sku").filter(termQuery("sku.sku_type", "bar1")).subAggregation(
+                                                                nested("nested_2").path("sku.colors").subAggregation(
+                                                                        filter("filter_sku_color").filter(termQuery("sku.colors.name", "red")).subAggregation(
                                                                                 reverseNested("reverse_to_sku").path("sku").subAggregation(
                                                                                         count("sku_count").field("sku.sku_type")
                                                                                 )
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
index 2a24803..623d27b 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
@@ -23,9 +23,11 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
+import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.test.ESIntegTestCase;
 
@@ -38,8 +40,6 @@ import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
@@ -123,9 +123,9 @@ public class SamplerIT extends ESIntegTestCase {
 
     }
 
-    public void testSimpleSampler() throws Exception {
-        SamplerAggregator.SamplerAggregatorBuilder sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+    public void testNoDiversity() throws Exception {
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
         assertSearchResponse(response);
@@ -140,9 +140,89 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(maxBooksPerAuthor, equalTo(3L));
     }
 
+    public void testSimpleDiversity() throws Exception {
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("test")
+                .setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy"))
+                .setFrom(0).setSize(60)
+                .addAggregation(sampleAgg)
+                .execute()
+                .actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        Terms authors = sample.getAggregations().get("authors");
+        Collection<Bucket> testBuckets = authors.getBuckets();
+
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
+    public void testNestedDiversity() throws Exception {
+        // Test multiple samples gathered under buckets made by a parent agg
+        int MAX_DOCS_PER_AUTHOR = 1;
+        TermsBuilder rootTerms = new TermsBuilder("genres").field("genre");
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+
+        rootTerms.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .addAggregation(rootTerms).execute().actionGet();
+        assertSearchResponse(response);
+        Terms genres = response.getAggregations().get("genres");
+        Collection<Bucket> genreBuckets = genres.getBuckets();
+        for (Terms.Bucket genreBucket : genreBuckets) {
+            Sampler sample = genreBucket.getAggregations().get("sample");
+            Terms authors = sample.getAggregations().get("authors");
+            Collection<Bucket> testBuckets = authors.getBuckets();
+
+            for (Terms.Bucket testBucket : testBuckets) {
+                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+            }
+        }
+    }
+
+    public void testNestedSamples() throws Exception {
+        // Test samples nested under samples
+        int MAX_DOCS_PER_AUTHOR = 1;
+        int MAX_DOCS_PER_GENRE = 2;
+        SamplerAggregationBuilder rootSample = new SamplerAggregationBuilder("genreSample").shardSize(100).field("genre")
+                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        sampleAgg.subAggregation(new TermsBuilder("genres").field("genre"));
+
+        rootSample.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler genreSample = response.getAggregations().get("genreSample");
+        Sampler sample = genreSample.getAggregations().get("sample");
+
+        Terms genres = sample.getAggregations().get("genres");
+        Collection<Bucket> testBuckets = genres.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
+        }
+
+        Terms authors = sample.getAggregations().get("authors");
+        testBuckets = authors.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
     public void testUnmappedChildAggNoDiversity() throws Exception {
-        SamplerAggregator.SamplerAggregatorBuilder sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy"))
@@ -158,8 +238,8 @@ public class SamplerIT extends ESIntegTestCase {
     }
 
     public void testPartiallyUnmappedChildAggNoDiversity() throws Exception {
-        SamplerAggregator.SamplerAggregatorBuilder sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("idx_unmapped", "test")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy"))
@@ -174,4 +254,34 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(authors.getBuckets().size(), greaterThan(0));
     }
 
+    public void testPartiallyUnmappedDiversifyField() throws Exception {
+        // One of the indexes is missing the "author" field used for
+        // diversifying results
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100).field("author").maxDocsPerValue(1);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), greaterThan(0L));
+        Terms authors = sample.getAggregations().get("authors");
+        assertThat(authors.getBuckets().size(), greaterThan(0));
+    }
+
+    public void testWhollyUnmappedDiversifyField() throws Exception {
+        //All of the indices are missing the "author" field used for diversifying results
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), equalTo(0L));
+        Terms authors = sample.getAggregations().get("authors");
+        assertNull(authors);
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
deleted file mode 100644
index dfa2060..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-
-public class SamplerTests extends BaseAggregationTestCase<SamplerAggregator.SamplerAggregatorBuilder> {
-
-    @Override
-    protected final SamplerAggregator.SamplerAggregatorBuilder createTestAggregatorBuilder() {
-        SamplerAggregator.SamplerAggregatorBuilder factory = new SamplerAggregator.SamplerAggregatorBuilder("foo");
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
index 8c6f30c..d138c0c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
@@ -91,7 +91,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(global("global")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -104,8 +104,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testFilter() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(filter("filter", QueryBuilders.matchAllQuery())
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -119,7 +119,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(missing("missing").field("foobar")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -133,9 +133,9 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(global("global")
-                        .subAggregation(filter("filter", QueryBuilders.matchAllQuery())
+                        .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
                                 .subAggregation(missing("missing").field("foobar")
-                                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))))
+                                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -150,8 +150,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testNested() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(nested("nested", "nested")
-                        .subAggregation(dateHistogram("histo").field("nested.date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(nested("nested").path("nested")
+                        .subAggregation(dateHistogram("histo").field("nested.date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -166,7 +166,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-s")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -181,7 +181,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-l")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -196,7 +196,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-d")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -210,7 +210,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(range("range").field("value").addRange("r1", 0, 10)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -224,7 +224,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(dateRange("range").field("date").addRange("r1", "2014-01-01", "2014-01-10")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -238,7 +238,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(ipRange("range").field("ip").addRange("r1", "10.0.0.1", "10.0.0.10")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -252,7 +252,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(histogram("topHisto").field("value").interval(5)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -265,8 +265,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testDateHistogram() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(dateHistogram("topHisto").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(dateHistogram("topHisto").field("date").interval(DateHistogramInterval.MONTH)
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -281,7 +281,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(geohashGrid("grid").field("location")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
index 4c3e548..97a3cfa 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
@@ -27,13 +27,14 @@ import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms.Bucket;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory.ExecutionMode;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.Arrays;
@@ -45,8 +46,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.is;
@@ -117,7 +116,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -133,8 +132,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "paul"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
-                           .minDocCount(1).includeExclude(new IncludeExclude(null, excludeTerms)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                           .minDocCount(1).exclude(excludeTerms))
                 .execute()
                 .actionGet();
         assertSearchResponse(response);
@@ -146,8 +145,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
     public void testIncludeExclude() throws Exception {
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(null, "weller")))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .exclude("weller"))
                 .get();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
@@ -165,8 +164,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude("weller", null)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .include("weller"))
                 .get();
         assertSearchResponse(response);
         topTerms = response.getAggregations().get("mySignificantTerms");
@@ -182,8 +181,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
         String []incExcTerms={"weller","nosuchterm"};
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(null, incExcTerms)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .exclude(incExcTerms))
                 .get();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
@@ -195,8 +194,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(incExcTerms, null)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .include(incExcTerms))
                 .get();
         assertSearchResponse(response);
         topTerms = response.getAggregations().get("mySignificantTerms");
@@ -213,7 +212,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -227,7 +226,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -241,7 +240,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new GND(true))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new GND.GNDBuilder(true))
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -255,7 +254,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new ChiSquare(false,true))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new ChiSquare.ChiSquareBuilder(false,true))
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -273,8 +272,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSize(60)
                 .setExplain(true)
                 .addAggregation(
-                        significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                                .significanceHeuristic(new PercentageScore()).minDocCount(2)).execute().actionGet();
+                        new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                                .significanceHeuristic(new PercentageScore.PercentageScoreBuilder()).minDocCount(2)).execute().actionGet();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
         checkExpectedStringTermsFound(topTerms);
@@ -289,7 +288,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                            .minDocCount(2).backgroundFilter(QueryBuilders.termQuery("fact_category", 1)))
                 .execute()
                 .actionGet();
@@ -313,7 +312,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "weller"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                            .minDocCount(1).backgroundFilter(QueryBuilders.termsQuery("description",  "paul")))
                 .execute()
                 .actionGet();
@@ -336,9 +335,9 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 { "craig", "kelly", "terje", "haakonsen", "burton" }};
         SearchResponse response = client().prepareSearch("test")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("myCategories").field("fact_category").minDocCount(2)
+                .addAggregation(new TermsBuilder("myCategories").field("fact_category").minDocCount(2)
                         .subAggregation(
-                                   significantTerms("mySignificantTerms").field("description")
+                                   new SignificantTermsBuilder("mySignificantTerms").field("description")
                                    .executionHint(randomExecutionHint())
                                    .minDocCount(2)))
                 .execute()
@@ -363,7 +362,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                             .executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
@@ -394,10 +393,10 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("description")
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(new JLHScore())
+                        .significanceHeuristic(new JLHScore.JLHScoreBuilder())
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -411,10 +410,10 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("description")
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(new MutualInformation(false, true))
+                        .significanceHeuristic(new MutualInformation.MutualInformationBuilder(false, true))
                         .minDocCount(1))
                 .execute()
                 .actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index d112957..6c1e7df 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -35,20 +34,25 @@ import org.elasticsearch.script.ScriptModule;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.aggregations.Aggregation;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
 import org.elasticsearch.search.aggregations.bucket.script.NativeSignificanceScoreScriptNoParams;
 import org.elasticsearch.search.aggregations.bucket.script.NativeSignificanceScoreScriptWithParams;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.search.aggregations.bucket.SharedSignificantTermsTestMethods;
 
@@ -65,9 +69,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.filter;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
 import static org.hamcrest.Matchers.closeTo;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
@@ -98,12 +99,11 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
         SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
-                                .subAggregation((significantTerms("sig_terms"))
+                        .subAggregation((new SignificantTermsBuilder("sig_terms"))
                                 .field(TEXT_FIELD)
-                                .significanceHeuristic(new SimpleHeuristic())
+                                .significanceHeuristic(new SimpleHeuristic.SimpleHeuristicBuilder())
                                 .minDocCount(1)
                         )
                 )
@@ -131,12 +131,11 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         // the reason is that this would trigger toXContent and we would like to check that this has no potential side effects
 
         response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
-                                .subAggregation((significantTerms("sig_terms"))
+                        .subAggregation((new SignificantTermsBuilder("sig_terms"))
                                 .field(TEXT_FIELD)
-                                .significanceHeuristic(new SimpleHeuristic())
+                                .significanceHeuristic(new SimpleHeuristic.SimpleHeuristicBuilder())
                                 .minDocCount(1)
                         )
                 )
@@ -164,7 +163,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
     public static class CustomSignificanceHeuristicPlugin extends Plugin {
 
         static {
-            SignificanceHeuristicStreams.registerPrototype(SimpleHeuristic.PROTOTYPE);
+            SignificanceHeuristicStreams.registerStream(SimpleHeuristic.STREAM);
         }
 
         @Override
@@ -178,7 +177,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
 
         public void onModule(SearchModule significanceModule) {
-            significanceModule.registerHeuristicParser(new SimpleHeuristic.SimpleHeuristicParser());
+            significanceModule.registerHeuristicParser(SimpleHeuristic.SimpleHeuristicParser.class);
         }
         public void onModule(ScriptModule module) {
             module.registerScript(NativeSignificanceScoreScriptNoParams.NATIVE_SIGNIFICANCE_SCORE_SCRIPT_NO_PARAMS, NativeSignificanceScoreScriptNoParams.Factory.class);
@@ -188,46 +187,24 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public static class SimpleHeuristic extends SignificanceHeuristic {
 
-        static final SimpleHeuristic PROTOTYPE = new SimpleHeuristic();
+        protected static final String[] NAMES = {"simple"};
 
-        protected static final ParseField NAMES_FIELD = new ParseField("simple");
+        public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+            @Override
+            public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+                return readFrom(in);
+            }
 
-        @Override
-        public String getWriteableName() {
-            return NAMES_FIELD.getPreferredName();
-        }
+            @Override
+            public String getName() {
+                return NAMES[0];
+            }
+        };
 
-        @Override
-        public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
             return new SimpleHeuristic();
         }
 
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return 1;
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            return true;
-        }
-
         /**
          * @param subsetFreq   The frequency of the term in the selected sample
          * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -240,10 +217,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
             return subsetFreq / subsetSize > supersetFreq / supersetSize ? 2.0 : 1.0;
         }
 
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(STREAM.getName());
+        }
+
         public static class SimpleHeuristicParser implements SignificanceHeuristicParser {
 
             @Override
-            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                     throws IOException, QueryShardException {
                 parser.nextToken();
                 return new SimpleHeuristic();
@@ -251,7 +233,16 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public String[] getNames() {
-                return NAMES_FIELD.getAllNamesIncludedDeprecated();
+                return NAMES;
+            }
+        }
+
+        public static class SimpleHeuristicBuilder implements SignificanceHeuristicBuilder {
+
+            @Override
+            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+                builder.startObject(STREAM.getName()).endObject();
+                return builder;
             }
         }
     }
@@ -261,7 +252,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
         SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(terms("class").field(CLASS_FIELD).subAggregation(significantTerms("sig_terms").field(TEXT_FIELD)))
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("sig_terms").field(TEXT_FIELD)))
                 .execute()
                 .actionGet();
         assertSearchResponse(response);
@@ -321,11 +312,10 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         indexRandom(true, false, indexRequestBuilderList);
 
         SearchResponse response1 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
                         .subAggregation(
-                                significantTerms("sig_terms")
+                                new SignificantTermsBuilder("sig_terms")
                                         .field(TEXT_FIELD)
                                         .minDocCount(1)))
                 .execute()
@@ -336,22 +326,22 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String type = randomBoolean() ? "string" : "long";
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
-        testBackgroundVsSeparateSet(new MutualInformation(true, true), new MutualInformation(true, false));
-        testBackgroundVsSeparateSet(new ChiSquare(true, true), new ChiSquare(true, false));
-        testBackgroundVsSeparateSet(new GND(true), new GND(false));
+        testBackgroundVsSeparateSet(new MutualInformation.MutualInformationBuilder(true, true), new MutualInformation.MutualInformationBuilder(true, false));
+        testBackgroundVsSeparateSet(new ChiSquare.ChiSquareBuilder(true, true), new ChiSquare.ChiSquareBuilder(true, false));
+        testBackgroundVsSeparateSet(new GND.GNDBuilder(true), new GND.GNDBuilder(false));
     }
 
     // compute significance score by
     // 1. terms agg on class and significant terms
     // 2. filter buckets and set the background to the other class and set is_background false
     // both should yield exact same result
-    public void testBackgroundVsSeparateSet(SignificanceHeuristic significanceHeuristicExpectingSuperset, SignificanceHeuristic significanceHeuristicExpectingSeparateSets) throws Exception {
+    public void testBackgroundVsSeparateSet(SignificanceHeuristicBuilder significanceHeuristicExpectingSuperset, SignificanceHeuristicBuilder significanceHeuristicExpectingSeparateSets) throws Exception {
 
         SearchResponse response1 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
                         .subAggregation(
-                                significantTerms("sig_terms")
+                                new SignificantTermsBuilder("sig_terms")
                                         .field(TEXT_FIELD)
                                         .minDocCount(1)
                                         .significanceHeuristic(
@@ -360,14 +350,16 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
                 .actionGet();
         assertSearchResponse(response1);
         SearchResponse response2 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(filter("0", QueryBuilders.termQuery(CLASS_FIELD, "0"))
-                        .subAggregation(significantTerms("sig_terms")
+                .addAggregation((new FilterAggregationBuilder("0"))
+                        .filter(QueryBuilders.termQuery(CLASS_FIELD, "0"))
+                        .subAggregation(new SignificantTermsBuilder("sig_terms")
                                 .field(TEXT_FIELD)
                                 .minDocCount(1)
                                 .backgroundFilter(QueryBuilders.termQuery(CLASS_FIELD, "1"))
                                 .significanceHeuristic(significanceHeuristicExpectingSeparateSets)))
-                .addAggregation(filter("1", QueryBuilders.termQuery(CLASS_FIELD, "1"))
-                        .subAggregation(significantTerms("sig_terms")
+                .addAggregation((new FilterAggregationBuilder("1"))
+                        .filter(QueryBuilders.termQuery(CLASS_FIELD, "1"))
+                        .subAggregation(new SignificantTermsBuilder("sig_terms")
                                 .field(TEXT_FIELD)
                                 .minDocCount(1)
                                 .backgroundFilter(QueryBuilders.termQuery(CLASS_FIELD, "0"))
@@ -396,15 +388,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public void testScoresEqualForPositiveAndNegative() throws Exception {
         indexEqualTestData();
-        testScoresEqualForPositiveAndNegative(new MutualInformation(true, true));
-        testScoresEqualForPositiveAndNegative(new ChiSquare(true, true));
+        testScoresEqualForPositiveAndNegative(new MutualInformation.MutualInformationBuilder(true, true));
+        testScoresEqualForPositiveAndNegative(new ChiSquare.ChiSquareBuilder(true, true));
     }
 
-    public void testScoresEqualForPositiveAndNegative(SignificanceHeuristic heuristic) throws Exception {
+    public void testScoresEqualForPositiveAndNegative(SignificanceHeuristicBuilder heuristic) throws Exception {
 
         //check that results for both classes are the same with exclude negatives = false and classes are routing ids
         SearchResponse response = client().prepareSearch("test")
-                .addAggregation(terms("class").field("class").subAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new TermsBuilder("class").field("class").subAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("text")
                         .executionHint(randomExecutionHint())
                         .significanceHeuristic(heuristic)
@@ -461,14 +453,13 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public void testScriptScore() throws ExecutionException, InterruptedException, IOException {
         indexRandomFrequencies01(randomBoolean() ? "string" : "long");
-        ScriptHeuristic scriptHeuristic = getScriptSignificanceHeuristic();
+        ScriptHeuristic.ScriptHeuristicBuilder scriptHeuristicBuilder = getScriptSignificanceHeuristicBuilder();
         ensureYellow();
         SearchResponse response = client().prepareSearch(INDEX_NAME)
-                .addAggregation(terms("class").field(CLASS_FIELD)
-                        .subAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field(TEXT_FIELD)
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(scriptHeuristic)
+                        .significanceHeuristic(scriptHeuristicBuilder)
                         .minDocCount(1).shardSize(2).size(2)))
                 .execute()
                 .actionGet();
@@ -480,7 +471,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
     }
 
-    private ScriptHeuristic getScriptSignificanceHeuristic() throws IOException {
+    private ScriptHeuristic.ScriptHeuristicBuilder getScriptSignificanceHeuristicBuilder() throws IOException {
         Script script = null;
         if (randomBoolean()) {
             Map<String, Object> params = null;
@@ -490,9 +481,9 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         } else {
             script = new Script("native_significance_score_script_no_params", ScriptType.INLINE, "native", null);
         }
-        ScriptHeuristic scriptHeuristic = new ScriptHeuristic(script);
+        ScriptHeuristic.ScriptHeuristicBuilder builder = new ScriptHeuristic.ScriptHeuristicBuilder().setScript(script);
 
-        return scriptHeuristic;
+        return builder;
     }
 
     private void indexRandomFrequencies01(String type) throws ExecutionException, InterruptedException {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
deleted file mode 100644
index 1db778c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class SignificantTermsTests extends BaseAggregationTestCase<SignificantTermsAggregatorBuilder> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected SignificantTermsAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        SignificantTermsAggregatorBuilder factory = new SignificantTermsAggregatorBuilder(name, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            SignificanceHeuristic significanceHeuristic = null;
-            switch (randomInt(5)) {
-            case 0:
-                significanceHeuristic = PercentageScore.PROTOTYPE;
-                break;
-            case 1:
-                significanceHeuristic = new ChiSquare(randomBoolean(), randomBoolean());
-                break;
-            case 2:
-                significanceHeuristic = new GND(randomBoolean());
-                break;
-            case 3:
-                significanceHeuristic = new MutualInformation(randomBoolean(), randomBoolean());
-                break;
-            case 4:
-                significanceHeuristic = new ScriptHeuristic(new Script("foo"));
-                break;
-            case 5:
-                significanceHeuristic = JLHScore.PROTOTYPE;
-                break;
-            default:
-                fail();
-            }
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        if (randomBoolean()) {
-            factory.backgroundFilter(QueryBuilders.termsQuery("foo", "bar"));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
index 4d61f45..9a7b337 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
@@ -21,10 +21,13 @@ package org.elasticsearch.search.aggregations.bucket;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.ArrayList;
@@ -34,9 +37,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.filter;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -71,8 +71,8 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
         // first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
         SearchResponse response = client().prepareSearch(index)
                 .addAggregation(
-                        (filter("inclass", QueryBuilders.termQuery("class", true)))
-                                .subAggregation(significantTerms("mySignificantTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()))
+                        (new FilterAggregationBuilder("inclass").filter(QueryBuilders.termQuery("class", true)))
+                                .subAggregation(new SignificantTermsBuilder("mySignificantTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()))
                 )
                 .execute()
                 .actionGet();
@@ -84,8 +84,8 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
 
         response = client().prepareSearch(index)
                 .addAggregation(
-                        (filter("inclass", QueryBuilders.termQuery("class", true)))
-                                .subAggregation(significantTerms("mySignificantTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()))
+                        (new FilterAggregationBuilder("inclass").filter(QueryBuilders.termQuery("class", true)))
+                                .subAggregation(new SignificantTermsBuilder("mySignificantTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()))
                 )
                 .execute()
                 .actionGet();
@@ -127,7 +127,7 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
         // first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
         SearchResponse response = client().prepareSearch(index)
                 .addAggregation(
-                        terms("myTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
+                        new TermsBuilder("myTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
                 )
                 .execute()
                 .actionGet();
@@ -138,7 +138,7 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
 
         response = client().prepareSearch(index)
                 .addAggregation(
-                        terms("myTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
+                        new TermsBuilder("myTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
                 )
                 .execute()
                 .actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
deleted file mode 100644
index ea12f6a..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class TermsTests extends BaseAggregationTestCase<TermsAggregatorBuilder> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected TermsAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        TermsAggregatorBuilder factory = new TermsAggregatorBuilder(name, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.collectMode(randomFrom(SubAggCollectionMode.values()));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            List<Terms.Order> order = randomOrder();
-            factory.order(order);
-        }
-        if (randomBoolean()) {
-            factory.showTermDocCountError(randomBoolean());
-        }
-        return factory;
-    }
-
-    private List<Terms.Order> randomOrder() {
-        List<Terms.Order> orders = new ArrayList<>();
-        switch (randomInt(4)) {
-        case 0:
-            orders.add(Terms.Order.term(randomBoolean()));
-            break;
-        case 1:
-            orders.add(Terms.Order.count(randomBoolean()));
-            break;
-        case 2:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 3:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 4:
-            int numOrders = randomIntBetween(1, 3);
-            for (int i = 0; i < numOrders; i++) {
-                orders.addAll(randomOrder());
-            }
-            break;
-        default:
-            fail();
-        }
-        return orders;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
index 4ffa860..ce26728 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
@@ -18,52 +18,40 @@
  */
 package org.elasticsearch.search.aggregations.bucket.geogrid;
 
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.TestSearchContext;
 
 public class GeoHashGridParserTests extends ESTestCase {
     public void testParseValidFromInts() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":" + precision + ", \"size\": 500, \"shard_size\": 550}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseValidFromStrings() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":\"" + precision + "\", \"size\": \"500\", \"shard_size\": \"550\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseErrorOnNonIntPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"2.0\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (NumberFormatException ex) {
             assertEquals("For input string: \"2.0\"", ex.getMessage());
@@ -71,34 +59,26 @@ public class GeoHashGridParserTests extends ESTestCase {
     }
 
     public void testParseErrorOnBooleanPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":false}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
-        } catch (ParsingException ex) {
-            assertEquals("Unexpected token VALUE_BOOLEAN [precision] in [geohash_grid].", ex.getMessage());
+        } catch (SearchParseException ex) {
+            assertEquals("Unexpected token VALUE_BOOLEAN in [geohash_grid].", ex.getMessage());
         }
     }
 
     public void testParseErrorOnPrecisionOutOfRange() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"13\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (IllegalArgumentException ex) {
             assertEquals("Invalid geohash aggregation precision of 13. Must be between 1 and 12.", ex.getMessage());
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
index cfe7e00..a9cdb6c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -123,11 +123,10 @@ public class NestedAggregatorTests extends ESSingleNodeTestCase {
         AggregationContext context = new AggregationContext(searchContext);
 
         AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        NestedAggregator.NestedAggregatorBuilder factory = new NestedAggregator.NestedAggregatorBuilder("test", "nested_field");
-        builder.addAggregator(factory);
-        AggregatorFactories factories = builder.build(context, null);
+        builder.addAggregator(new NestedAggregator.Factory("test", "nested_field"));
+        AggregatorFactories factories = builder.build();
         searchContext.aggregations(new SearchContextAggregations(factories));
-        Aggregator[] aggs = factories.createTopLevelAggregators();
+        Aggregator[] aggs = factories.createTopLevelAggregators(context);
         BucketCollector collector = BucketCollector.wrap(Arrays.asList(aggs));
         collector.preCollection();
         // A regular search always exclude nested docs, so we use NonNestedDocsFilter.INSTANCE here (otherwise MatchAllDocsQuery would be sufficient)
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
deleted file mode 100644
index 5aa26cd..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator.NestedAggregatorBuilder;
-
-public class NestedTests extends BaseAggregationTestCase<NestedAggregator.NestedAggregatorBuilder> {
-
-    @Override
-    protected NestedAggregatorBuilder createTestAggregatorBuilder() {
-        return new NestedAggregatorBuilder(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(3, 40));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
deleted file mode 100644
index 56eb412..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator.ReverseNestedAggregatorBuilder;
-
-public class ReverseNestedTests extends BaseAggregationTestCase<ReverseNestedAggregator.ReverseNestedAggregatorBuilder> {
-
-    @Override
-    protected ReverseNestedAggregatorBuilder createTestAggregatorBuilder() {
-        ReverseNestedAggregatorBuilder factory = new ReverseNestedAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.path(randomAsciiOfLengthBetween(3, 40));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
index 789c220..0a094c3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
@@ -21,18 +21,13 @@ package org.elasticsearch.search.aggregations.bucket.significant;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.Version;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.InputStreamStreamInput;
 import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
@@ -42,6 +37,7 @@ import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHSc
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -67,7 +63,6 @@ import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.hamcrest.Matchers.lessThan;
 import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
 
 /**
  *
@@ -138,7 +133,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
 
     SignificanceHeuristic getRandomSignificanceheuristic() {
         List<SignificanceHeuristic> heuristics = new ArrayList<>();
-        heuristics.add(JLHScore.PROTOTYPE);
+        heuristics.add(JLHScore.INSTANCE);
         heuristics.add(new MutualInformation(randomBoolean(), randomBoolean()));
         heuristics.add(new GND(randomBoolean()));
         heuristics.add(new ChiSquare(randomBoolean(), randomBoolean()));
@@ -201,7 +196,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
     public void testBuilderAndParser() throws Exception {
 
         Set<SignificanceHeuristicParser> parsers = new HashSet<>();
-        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers);
+        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers, null);
         SearchContext searchContext = new SignificantTermsTestSearchContext();
 
         // test jlh with string
@@ -215,10 +210,10 @@ public class SignificanceHeuristicTests extends ESTestCase {
         assertThat(parseFromString(heuristicParserMapper, searchContext, "\"chi_square\":{\"include_negatives\": " + includeNegatives + ", \"background_is_superset\":" + backgroundIsSuperset + "}"), equalTo((SignificanceHeuristic) (new ChiSquare(includeNegatives, backgroundIsSuperset))));
 
         // test with builders
-        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new JLHScore()) instanceof JLHScore);
-        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new GND(backgroundIsSuperset)) instanceof GND);
-        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new MutualInformation(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new MutualInformation(includeNegatives, backgroundIsSuperset)));
-        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new ChiSquare(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new ChiSquare(includeNegatives, backgroundIsSuperset)));
+        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new JLHScore.JLHScoreBuilder()) instanceof JLHScore);
+        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new GND.GNDBuilder(backgroundIsSuperset)) instanceof GND);
+        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new MutualInformation.MutualInformationBuilder(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new MutualInformation(includeNegatives, backgroundIsSuperset)));
+        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new ChiSquare.ChiSquareBuilder(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new ChiSquare(includeNegatives, backgroundIsSuperset)));
 
         // test exceptions
         String faultyHeuristicdefinition = "\"mutual_information\":{\"include_negatives\": false, \"some_unknown_field\": false}";
@@ -238,46 +233,34 @@ public class SignificanceHeuristicTests extends ESTestCase {
         checkParseException(heuristicParserMapper, searchContext, faultyHeuristicdefinition, expectedError);
     }
 
-    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext,
-            String faultyHeuristicDefinition, String expectedError) throws IOException {
-
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
+    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, String faultyHeuristicDefinition, String expectedError) throws IOException {
         try {
             XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"text\", " + faultyHeuristicDefinition + ",\"min_doc_count\":200}");
-            QueryParseContext parseContext = new QueryParseContext(registry);
-            parseContext.reset(stParser);
-            parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
             stParser.nextToken();
-            new SignificantTermsParser(heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+            new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
             fail();
         } catch (ElasticsearchParseException e) {
             assertTrue(e.getMessage().contains(expectedError));
         }
     }
 
-    protected SignificanceHeuristic parseFromBuilder(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, SignificanceHeuristic significanceHeuristic) throws IOException {
-        SignificantTermsAggregatorBuilder stBuilder = significantTerms("testagg");
-        stBuilder.significanceHeuristic(significanceHeuristic).field("text").minDocCount(200);
+    protected SignificanceHeuristic parseFromBuilder(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, SignificanceHeuristicBuilder significanceHeuristicBuilder) throws IOException {
+        SignificantTermsBuilder stBuilder = new SignificantTermsBuilder("testagg");
+        stBuilder.significanceHeuristic(significanceHeuristicBuilder).field("text").minDocCount(200);
         XContentBuilder stXContentBuilder = XContentFactory.jsonBuilder();
         stBuilder.internalXContent(stXContentBuilder, null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(stXContentBuilder.string());
         return parseSignificanceHeuristic(heuristicParserMapper, searchContext, stParser);
     }
 
-    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper,
-            SearchContext searchContext, XContentParser stParser) throws IOException {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
-        QueryParseContext parseContext = new QueryParseContext(registry);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
+    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, XContentParser stParser) throws IOException {
         stParser.nextToken();
-        SignificantTermsAggregatorBuilder aggregatorFactory = (SignificantTermsAggregatorBuilder) new SignificantTermsParser(
-                heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+        SignificantTermsAggregatorFactory aggregatorFactory = (SignificantTermsAggregatorFactory) new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
         stParser.nextToken();
         assertThat(aggregatorFactory.getBucketCountThresholds().getMinDocCount(), equalTo(200L));
         assertThat(stParser.currentToken(), equalTo(null));
         stParser.close();
-        return aggregatorFactory.significanceHeuristic();
+        return aggregatorFactory.getSignificanceHeuristic();
     }
 
     protected SignificanceHeuristic parseFromString(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, String heuristicString) throws IOException {
@@ -388,14 +371,14 @@ public class SignificanceHeuristicTests extends ESTestCase {
         testBackgroundAssertions(new MutualInformation(true, true), new MutualInformation(true, false));
         testBackgroundAssertions(new ChiSquare(true, true), new ChiSquare(true, false));
         testBackgroundAssertions(new GND(true), new GND(false));
-        testAssertions(PercentageScore.PROTOTYPE);
-        testAssertions(JLHScore.PROTOTYPE);
+        testAssertions(PercentageScore.INSTANCE);
+        testAssertions(JLHScore.INSTANCE);
     }
 
     public void testBasicScoreProperties() {
-        basicScoreProperties(JLHScore.PROTOTYPE, true);
+        basicScoreProperties(JLHScore.INSTANCE, true);
         basicScoreProperties(new GND(true), true);
-        basicScoreProperties(PercentageScore.PROTOTYPE, true);
+        basicScoreProperties(PercentageScore.INSTANCE, true);
         basicScoreProperties(new MutualInformation(true, true), false);
         basicScoreProperties(new ChiSquare(true, true), false);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
deleted file mode 100644
index 58d7fa7..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-
-public abstract class AbstractNumericMetricTestCase<AF extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, AF>>
-        extends BaseAggregationTestCase<AF> {
-
-    @Override
-    protected final AF createTestAggregatorBuilder() {
-        AF factory = doCreateTestAggregatorFactory();
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-    protected abstract AF doCreateTestAggregatorFactory();
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
index d7e8736..2ce78e4 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
@@ -72,7 +72,7 @@ public class AvgIT extends AbstractNumericTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(avg("avg").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(avg("avg")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
deleted file mode 100644
index df1b78c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-
-public class AvgTests extends AbstractNumericMetricTestCase<AvgAggregator.AvgAggregatorBuilder> {
-
-    @Override
-    protected AvgAggregator.AvgAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new AvgAggregator.AvgAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
deleted file mode 100644
index 7a9e30b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-
-public class ExtendedStatsTests extends AbstractNumericMetricTestCase<ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder> {
-
-    @Override
-    protected ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder doCreateTestAggregatorFactory() {
-        ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder("foo");
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, true));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
deleted file mode 100644
index 4b0389b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.FilterAggregatorBuilder;
-
-public class FilterTests extends BaseAggregationTestCase<FilterAggregator.FilterAggregatorBuilder> {
-
-    @Override
-    protected FilterAggregatorBuilder createTestAggregatorBuilder() {
-        FilterAggregatorBuilder factory = new FilterAggregatorBuilder(randomAsciiOfLengthBetween(1, 20),
-                QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        // NORELEASE make RandomQueryBuilder work outside of the
-        // AbstractQueryTestCase
-        // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
deleted file mode 100644
index 5ab9109..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.FiltersAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
-
-public class FiltersTests extends BaseAggregationTestCase<FiltersAggregator.FiltersAggregatorBuilder> {
-
-    @Override
-    protected FiltersAggregatorBuilder createTestAggregatorBuilder() {
-
-        int size = randomIntBetween(1, 20);
-        FiltersAggregatorBuilder factory;
-        if (randomBoolean()) {
-            KeyedFilter[] filters = new KeyedFilter[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = new KeyedFilter(randomAsciiOfLengthBetween(1, 20),
-                        QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-            }
-            factory = new FiltersAggregatorBuilder(randomAsciiOfLengthBetween(1, 20), filters);
-        } else {
-            QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-            }
-            factory = new FiltersAggregatorBuilder(randomAsciiOfLengthBetween(1, 20), filters);
-        }
-        if (randomBoolean()) {
-            factory.otherBucket(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.otherBucketKey(randomAsciiOfLengthBetween(1, 20));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
deleted file mode 100644
index 58fe5ca..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.GeoBoundsAggregatorBuilder;
-
-public class GeoBoundsTests extends BaseAggregationTestCase<GeoBoundsAggregator.GeoBoundsAggregatorBuilder> {
-
-    @Override
-    protected GeoBoundsAggregatorBuilder createTestAggregatorBuilder() {
-        GeoBoundsAggregatorBuilder factory = new GeoBoundsAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        String field = randomAsciiOfLengthBetween(3, 20);
-        factory.field(field);
-        if (randomBoolean()) {
-            factory.wrapLongitude(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java
index da86991..8c21cbd 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java
@@ -147,7 +147,7 @@ public class GeoCentroidIT extends AbstractGeoTestCase {
     public void testSingleValueFieldAsSubAggToGeohashGrid() throws Exception {
         SearchResponse response = client().prepareSearch(HIGH_CARD_IDX_NAME)
                 .addAggregation(geohashGrid("geoGrid").field(SINGLE_VALUED_FIELD_NAME)
-                .subAggregation(geoCentroid(aggName).field(SINGLE_VALUED_FIELD_NAME)))
+                .subAggregation(geoCentroid(aggName)))
                 .execute().actionGet();
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
deleted file mode 100644
index 7bf2151..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator.GeoCentroidAggregatorBuilder;
-
-public class GeoCentroidTests extends BaseAggregationTestCase<GeoCentroidAggregator.GeoCentroidAggregatorBuilder> {
-
-    @Override
-    protected GeoCentroidAggregatorBuilder createTestAggregatorBuilder() {
-        GeoCentroidAggregatorBuilder factory = new GeoCentroidAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
deleted file mode 100644
index 89b5560..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-
-public class MaxTests extends AbstractNumericMetricTestCase<MaxAggregator.MaxAggregatorBuilder> {
-
-    @Override
-    protected MaxAggregator.MaxAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new MaxAggregator.MaxAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
deleted file mode 100644
index 5ee5db1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator.MinAggregatorBuilder;
-
-public class MinTests extends AbstractNumericMetricTestCase<MinAggregator.MinAggregatorBuilder> {
-
-    @Override
-    protected MinAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new MinAggregator.MinAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
deleted file mode 100644
index 846579f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator;
-
-public class MissingTests extends BaseAggregationTestCase<MissingAggregator.MissingAggregatorBuilder> {
-
-    @Override
-    protected final MissingAggregator.MissingAggregatorBuilder createTestAggregatorBuilder() {
-        MissingAggregator.MissingAggregatorBuilder factory = new MissingAggregator.MissingAggregatorBuilder("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java
deleted file mode 100644
index 4636e4e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorBuilder;
-
-public class PercentileRanksTests extends BaseAggregationTestCase<PercentileRanksAggregatorBuilder> {
-
-    @Override
-    protected PercentileRanksAggregatorBuilder createTestAggregatorBuilder() {
-        PercentileRanksAggregatorBuilder factory = new PercentileRanksAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        int valuesSize = randomIntBetween(1, 20);
-        double[] values = new double[valuesSize];
-        for (int i = 0; i < valuesSize; i++) {
-            values[i] = randomDouble() * 100;
-        }
-        factory.values(values);
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            factory.compression(randomIntBetween(1, 50000));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java
deleted file mode 100644
index 674197c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorBuilder;
-
-public class PercentilesTests extends BaseAggregationTestCase<PercentilesAggregatorBuilder> {
-
-    @Override
-    protected PercentilesAggregatorBuilder createTestAggregatorBuilder() {
-        PercentilesAggregatorBuilder factory = new PercentilesAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int percentsSize = randomIntBetween(1, 20);
-            double[] percents = new double[percentsSize];
-            for (int i = 0; i < percentsSize; i++) {
-                percents[i] = randomDouble() * 100;
-            }
-            factory.percentiles(percents);
-        }
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            factory.compression(randomIntBetween(1, 50000));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.format("###.00");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
deleted file mode 100644
index 6d4ae48..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class ScriptedMetricTests extends BaseAggregationTestCase<ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder> {
-
-    @Override
-    protected ScriptedMetricAggregatorBuilder createTestAggregatorBuilder() {
-        ScriptedMetricAggregatorBuilder factory = new ScriptedMetricAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.initScript(randomScript("initScript"));
-        }
-        factory.mapScript(randomScript("mapScript"));
-        if (randomBoolean()) {
-            factory.combineScript(randomScript("combineScript"));
-        }
-        if (randomBoolean()) {
-            factory.reduceScript(randomScript("reduceScript"));
-        }
-        if (randomBoolean()) {
-            Map<String, Object> params = new HashMap<String, Object>();
-            params.put("foo", "bar");
-            factory.params(params);
-        }
-        return factory;
-    }
-
-    private Script randomScript(String script) {
-        if (randomBoolean()) {
-            return new Script(script);
-        } else {
-            return new Script(script, randomFrom(ScriptType.values()), randomFrom("my_lang", null), null);
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
deleted file mode 100644
index eaeca9c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator;
-
-public class StatsTests extends AbstractNumericMetricTestCase<StatsAggregator.StatsAggregatorBuilder> {
-
-    @Override
-    protected StatsAggregator.StatsAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new StatsAggregator.StatsAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
index adafc55..f2c05ee 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
@@ -69,7 +69,7 @@ public class SumIT extends AbstractNumericTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(sum("sum").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(sum("sum")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
deleted file mode 100644
index 78eba34..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-
-public class SumTests extends AbstractNumericMetricTestCase<SumAggregator.SumAggregatorBuilder> {
-
-    @Override
-    protected SumAggregator.SumAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new SumAggregator.SumAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
index 2287e2d..90a52ed 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
@@ -256,7 +256,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .field(TERMS_AGGS_FIELD)
                         .subAggregation(
-                                topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                         )
                 )
                 .get();
@@ -352,7 +352,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .collectMode(SubAggCollectionMode.BREADTH_FIRST)
                         .field(TERMS_AGGS_FIELD)
-                        .subAggregation(topHits("hits").size(3))
+                        .subAggregation(topHits("hits").setSize(3))
                 ).get();
 
         assertSearchResponse(response);
@@ -403,9 +403,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
-                                                .from(from)
-                                                .size(size)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                                .setFrom(from)
+                                                .setSize(size)
                                 )
                 )
                 .get();
@@ -447,7 +447,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .order(Terms.Order.aggregation("max_sort", false))
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).trackScores(true)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).setTrackScores(true)
                                 )
                                 .subAggregation(
                                         max("max_sort").field(SORT_FIELD)
@@ -487,7 +487,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(matchQuery("text", "term rare"))
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field("group")
-                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").size(1))
+                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").setSize(1))
                                 .subAggregation(max("max_score").field("value"))).get();
         assertSearchResponse(response);
 
@@ -529,14 +529,14 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(1)
+                                        topHits("hits").setSize(1)
                                             .highlighter(new HighlightBuilder().field("text"))
-                                            .explain(true)
-                                            .field("text")
-                                            .fieldDataField("field1")
-                                            .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                                            .fetchSource("text", null)
-                                            .version(true)
+                                            .setExplain(true)
+                                            .addField("text")
+                                            .addFieldDataField("field1")
+                                            .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
+                                            .setFetchSource("text", null)
+                                            .setVersion(true)
                                 )
                 )
                 .get();
@@ -586,7 +586,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .executionHint(randomExecutionHint())
                                     .field(TERMS_AGGS_FIELD)
                                     .subAggregation(
-                                            topHits("hits").sort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
+                                            topHits("hits").addSort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
                                     )
                     ).get();
             fail();
@@ -595,6 +595,39 @@ public class TopHitsIT extends ESIntegTestCase {
         }
     }
 
+    // public void testFailWithSubAgg() throws Exception {
+    // String source = "{\n" +
+    // "  \"aggs\": {\n" +
+    // "    \"top-tags\": {\n" +
+    // "      \"terms\": {\n" +
+    // "        \"field\": \"tags\"\n" +
+    // "      },\n" +
+    // "      \"aggs\": {\n" +
+    // "        \"top_tags_hits\": {\n" +
+    // "          \"top_hits\": {},\n" +
+    // "          \"aggs\": {\n" +
+    // "            \"max\": {\n" +
+    // "              \"max\": {\n" +
+    // "                \"field\": \"age\"\n" +
+    // "              }\n" +
+    // "            }\n" +
+    // "          }\n" +
+    // "        }\n" +
+    // "      }\n" +
+    // "    }\n" +
+    // "  }\n" +
+    // "}";
+    // try {
+    // client().prepareSearch("idx").setTypes("type")
+    // .setSource(new BytesArray(source))
+    // .get();
+    // fail();
+    // } catch (SearchPhaseExecutionException e) {
+    // assertThat(e.toString(),
+    // containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
+    // }
+    // } NORELEASE this needs to be tested in a top_hits aggregations unit test
+
     public void testEmptyIndex() throws Exception {
         SearchResponse response = client().prepareSearch("empty").setTypes("type")
                 .addAggregation(topHits("hits"))
@@ -617,9 +650,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .field("group")
                                     .subAggregation(
                                             topHits("hits")
-                                                    .trackScores(trackScore)
-                                                    .size(1)
-                                                    .sort("_id", SortOrder.DESC)
+                                                    .setTrackScores(trackScore)
+                                                    .setSize(1)
+                                                    .addSort("_id", SortOrder.DESC)
                                     )
                     )
                     .get();
@@ -657,12 +690,13 @@ public class TopHitsIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("articles")
                 .setQuery(matchQuery("title", "title"))
                 .addAggregation(
-                        nested("to-comments", "comments")
+                        nested("to-comments")
+                                .path("comments")
                                 .subAggregation(
                                         terms("users")
                                                 .field("comments.user")
                                                 .subAggregation(
-                                                        topHits("top-comments").sort("comments.date", SortOrder.ASC)
+                                                        topHits("top-comments").addSort("comments.date", SortOrder.ASC)
                                                 )
                                 )
                 )
@@ -707,14 +741,15 @@ public class TopHitsIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("articles")
                 .setQuery(matchQuery("title", "title"))
                 .addAggregation(
-                        nested("to-comments", "comments")
+                        nested("to-comments")
+                                .path("comments")
                                 .subAggregation(
-                                    nested("to-reviewers", "comments.reviewers").subAggregation(
+                                    nested("to-reviewers").path("comments.reviewers").subAggregation(
                                             // Also need to sort on _doc because there are two reviewers with the same name
-                                            topHits("top-reviewers").sort("comments.reviewers.name", SortOrder.ASC).sort("_doc", SortOrder.DESC).size(7)
+                                            topHits("top-reviewers").addSort("comments.reviewers.name", SortOrder.ASC).addSort("_doc", SortOrder.DESC).setSize(7)
                                     )
                                 )
-                                .subAggregation(topHits("top-comments").sort("comments.date", SortOrder.DESC).size(4))
+                                .subAggregation(topHits("top-comments").addSort("comments.date", SortOrder.DESC).setSize(4))
                 ).get();
         assertNoFailures(searchResponse);
 
@@ -813,11 +848,11 @@ public class TopHitsIT extends ESIntegTestCase {
                 .prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
-                        nested("to-comments", "comments").subAggregation(
-                                topHits("top-comments").size(1).highlighter(new HighlightBuilder().field(hlField)).explain(true)
-                                                .fieldDataField("comments.user")
-                                        .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).fetchSource("message", null)
-                                        .version(true).sort("comments.date", SortOrder.ASC))).get();
+                        nested("to-comments").path("comments").subAggregation(
+                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                                .addFieldDataField("comments.user")
+                                        .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).setFetchSource("message", null)
+                                        .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
         assertHitCount(searchResponse, 2);
         Nested nested = searchResponse.getAggregations().get("to-comments");
         assertThat(nested.getDocCount(), equalTo(4L));
@@ -862,10 +897,11 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .interval(5)
                                 .order(Histogram.Order.aggregation("to-comments", true))
                                 .subAggregation(
-                                        nested("to-comments", "comments")
+                                        nested("to-comments")
+                                                .path("comments")
                                                 .subAggregation(topHits("comments")
                                                         .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
-                                                        .sort("comments.id", SortOrder.ASC))
+                                                        .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
                 .get();
@@ -902,7 +938,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(ArrayUtil.MAX_ARRAY_LENGTH - 1).sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                        topHits("hits").setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1).addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                                 )
                 )
                 .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
deleted file mode 100644
index 7636ccb..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
+++ /dev/null
@@ -1,186 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.AggregationInitializationException;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilderTests;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.containsString;
-
-public class TopHitsTests extends BaseAggregationTestCase<TopHitsAggregator.TopHitsAggregatorBuilder> {
-
-    @Override
-    protected final TopHitsAggregator.TopHitsAggregatorBuilder createTestAggregatorBuilder() {
-        TopHitsAggregator.TopHitsAggregatorBuilder factory = new TopHitsAggregator.TopHitsAggregatorBuilder("foo");
-        if (randomBoolean()) {
-            factory.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            factory.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                factory.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                if (randomBoolean()) {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"), randomBoolean());
-                } else {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-                }
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes);
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            factory.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    factory.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    factory.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20), AbstractQueryTestCase.randomGeohash(1, 12))
-                            .order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    factory.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    factory.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            factory.highlighter(HighlightBuilderTests.randomHighlighterBuilder());
-        }
-        return factory;
-    }
-
-
-    public void testFailWithSubAgg() throws Exception {
-        String source = "{\n" +
-            "    \"top-tags\": {\n" +
-            "      \"terms\": {\n" +
-            "        \"field\": \"tags\"\n" +
-            "      },\n" +
-            "      \"aggs\": {\n" +
-            "        \"top_tags_hits\": {\n" +
-            "          \"top_hits\": {},\n" +
-            "          \"aggs\": {\n" +
-            "            \"max\": {\n" +
-            "              \"max\": {\n" +
-            "                \"field\": \"age\"\n" +
-            "              }\n" +
-            "            }\n" +
-            "          }\n" +
-            "        }\n" +
-            "      }\n" +
-            "    }\n" +
-            "}";
-        try {
-            XContentParser parser = XContentFactory.xContent(source).createParser(source);
-            QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-            parseContext.reset(parser);
-            parseContext.parseFieldMatcher(parseFieldMatcher);
-            assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-            aggParsers.parseAggregators(parser, parseContext);
-            fail();
-        } catch (AggregationInitializationException e) {
-            assertThat(e.toString(), containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
deleted file mode 100644
index a942aa4..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator;
-
-public class ValueCountTests extends BaseAggregationTestCase<ValueCountAggregator.ValueCountAggregatorBuilder> {
-
-    @Override
-    protected final ValueCountAggregator.ValueCountAggregatorBuilder createTestAggregatorBuilder() {
-        ValueCountAggregator.ValueCountAggregatorBuilder factory = new ValueCountAggregator.ValueCountAggregatorBuilder("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
deleted file mode 100644
index a769a71..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.cardinality;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-
-public class CardinalityTests extends BaseAggregationTestCase<CardinalityAggregatorBuilder> {
-
-    @Override
-    protected final CardinalityAggregatorBuilder createTestAggregatorBuilder() {
-        CardinalityAggregatorBuilder factory = new CardinalityAggregatorBuilder("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java
deleted file mode 100644
index eb08e6f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class PercentilesMethodTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(PercentilesMethod.TDIGEST.ordinal(), equalTo(0));
-        assertThat(PercentilesMethod.HDR.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            PercentilesMethod.TDIGEST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            PercentilesMethod.HDR.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(PercentilesMethod.TDIGEST.readFrom(in), equalTo(PercentilesMethod.TDIGEST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(PercentilesMethod.TDIGEST.readFrom(in), equalTo(PercentilesMethod.HDR));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                PercentilesMethod.TDIGEST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown PercentilesMethod ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
index 17f06be..47c0c96 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -94,8 +92,8 @@ public class AvgBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(avgBucket("avg_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -132,8 +130,8 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .subAggregation(avgBucket("avg_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -175,7 +173,7 @@ public class AvgBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(avgBucket("avg_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -214,9 +212,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .subAggregation(avgBucket("avg_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -267,9 +265,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .subAggregation(avgBucket("avg_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -313,9 +311,8 @@ public class AvgBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(avgBucket("avg_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -340,9 +337,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .subAggregation(avgBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(avgBucket("avg_terms_bucket", "terms>avg_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(avgBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(avgBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
deleted file mode 100644
index 4226c5b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketScriptTests
-        extends BasePipelineAggregationTestCase<BucketScriptPipelineAggregator.BucketScriptPipelineAggregatorBuilder> {
-
-    @Override
-    protected BucketScriptPipelineAggregatorBuilder createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        BucketScriptPipelineAggregatorBuilder factory = new BucketScriptPipelineAggregatorBuilder(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
deleted file mode 100644
index 311ab3b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketSelectorTests
-        extends BasePipelineAggregationTestCase<BucketSelectorPipelineAggregator.BucketSelectorPipelineAggregatorBuilder> {
-
-    @Override
-    protected BucketSelectorPipelineAggregatorBuilder createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        BucketSelectorPipelineAggregatorBuilder factory = new BucketSelectorPipelineAggregatorBuilder(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
index 906aa3d..6f10e5d 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -89,8 +88,8 @@ public class CumulativeSumIT extends ESIntegTestCase {
     public void testDocCount() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
-                                .subAggregation(cumulativeSum("cumulative_sum", "_count"))).execute().actionGet();
+                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -120,9 +119,9 @@ public class CumulativeSumIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(cumulativeSum("cumulative_sum", "sum"))).execute().actionGet();
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -154,7 +153,7 @@ public class CumulativeSumIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(cumulativeSum("cumulative_sum", "sum"))).execute().actionGet();
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
deleted file mode 100644
index 8c3d1f6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder;
-
-public class CumulativeSumTests
-        extends BasePipelineAggregationTestCase<CumulativeSumPipelineAggregator.CumulativeSumPipelineAggregatorBuilder> {
-
-    @Override
-    protected CumulativeSumPipelineAggregatorBuilder createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        CumulativeSumPipelineAggregatorBuilder factory = new CumulativeSumPipelineAggregatorBuilder(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
index 73ec2c9..f2da1db 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
@@ -108,8 +108,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -150,8 +150,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count").unit(DateHistogramInterval.DAY))).execute()
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").unit(DateHistogramInterval.DAY))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -195,8 +195,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                            .subAggregation(sum("sum").field("value")).subAggregation(derivative("deriv", "sum")))
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum")).subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -264,8 +264,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("dates").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("dates").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -319,8 +319,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx_unmapped")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -334,8 +334,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
index 14e99f0..6a4f548 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -172,8 +172,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))
-                                .subAggregation(derivative("2nd_deriv", "deriv"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))
+                                .subAggregation(derivative("2nd_deriv").setBucketsPaths("deriv"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -211,8 +211,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count").unit("1ms"))
-                                .subAggregation(derivative("2nd_deriv", "deriv").unit("10ms"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").unit("1ms"))
+                                .subAggregation(derivative("2nd_deriv").setBucketsPaths("deriv").unit("10ms"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -250,7 +250,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -295,7 +295,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(stats("stats").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "stats.sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("stats.sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -339,7 +339,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -354,7 +354,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -383,7 +383,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
 
@@ -411,8 +411,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .extendedBounds(new ExtendedBounds(0L, (long) numBuckets_empty_rnd - 1))
-                                .subAggregation(derivative("deriv", "_count").gapPolicy(randomFrom(GapPolicy.values()))))
+                                .extendedBounds(0L, (long) numBuckets_empty_rnd - 1)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").gapPolicy(randomFrom(GapPolicy.values()))))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx_rnd));
@@ -441,7 +441,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .subAggregation(derivative("deriv", "_count").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
                                 .actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
@@ -471,7 +471,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum"))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
 
@@ -512,7 +512,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
                 .actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
@@ -550,9 +550,9 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .extendedBounds(new ExtendedBounds(0L, (long) numBuckets_empty_rnd - 1))
+                                .extendedBounds(0L, (long) numBuckets_empty_rnd - 1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum").gapPolicy(gapPolicy))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum").gapPolicy(gapPolicy))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx_rnd));
 
@@ -594,21 +594,28 @@ public class DerivativeIT extends ESIntegTestCase {
                                     .field(SINGLE_VALUED_FIELD_NAME)
                                     .interval(interval)
                                     .subAggregation(
-                                            filters("filters", QueryBuilders.termQuery("tag", "foo")).subAggregation(
+                                            filters("filters").filter(QueryBuilders.termQuery("tag", "foo")).subAggregation(
                                                     sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                    .subAggregation(derivative("deriv", "filters>get>sum"))).execute().actionGet();
+                                    .subAggregation(derivative("deriv").setBucketsPaths("filters>get>sum"))).execute().actionGet();
             fail("Expected an Exception but didn't get one");
         } catch (Exception e) {
             Throwable cause = ExceptionsHelper.unwrapCause(e);
             if (cause == null) {
                 throw e;
             } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
+                ElasticsearchException[] rootCauses = ((SearchPhaseExecutionException) cause).guessRootCauses();
+                // If there is more than one root cause then something
+                // unexpected happened and we should re-throw the original
+                // exception
+                if (rootCauses.length > 1) {
                     throw e;
                 }
-            } else if (!(cause instanceof IllegalArgumentException)) {
+                ElasticsearchException rootCauseWrapper = rootCauses[0];
+                Throwable rootCause = rootCauseWrapper.getCause();
+                if (rootCause == null || !(rootCause instanceof IllegalArgumentException)) {
+                    throw e;
+                }
+            } else {
                 throw e;
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
deleted file mode 100644
index e1aa45f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder;
-
-public class DerivativeTests extends BasePipelineAggregationTestCase<DerivativePipelineAggregator.DerivativePipelineAggregatorBuilder> {
-
-    @Override
-    protected DerivativePipelineAggregatorBuilder createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        DerivativePipelineAggregatorBuilder factory = new DerivativePipelineAggregatorBuilder(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                factory.unit(String.valueOf(randomInt()));
-            } else {
-                factory.unit(String.valueOf(randomIntBetween(1, 10) + randomFrom("s", "m", "h", "d", "w", "M", "y")));
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
index f9b8062..3a82b68 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
@@ -19,16 +19,13 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucket;
@@ -97,8 +94,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -144,8 +141,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -196,7 +193,7 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -244,9 +241,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -306,9 +303,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -361,9 +358,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -389,24 +385,13 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                     .order(Order.term(true))
                                     .subAggregation(
                                             histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                    .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                    .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                     .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                    .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum")
-                                            .sigma(-1.0))).execute().actionGet();
+                                    .subAggregation(extendedStatsBucket("extended_stats_bucket")
+                                            .setBucketsPaths("histo>sum").sigma(-1.0))).execute().actionGet();
             fail("Illegal sigma was provided but no exception was thrown.");
-        } catch (Exception e) {
-            Throwable cause = ExceptionsHelper.unwrapCause(e);
-            if (cause == null) {
-                throw e;
-            } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
-                    throw e;
-                }
-            } else if (!(cause instanceof IllegalArgumentException)) {
-                throw e;
-            }
+        } catch (SearchPhaseExecutionException exception) {
+            // All good
         }
     }
 
@@ -419,9 +404,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(extendedStatsBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(extendedStatsBucket("avg_terms_bucket", "terms>avg_histo_bucket.avg")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(extendedStatsBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(extendedStatsBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket.avg")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
index 434c45d..bf8c23a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
@@ -22,12 +22,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
@@ -98,8 +96,8 @@ public class MaxBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(maxBucket("max_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -141,8 +139,8 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(maxBucket("max_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -189,7 +187,7 @@ public class MaxBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(maxBucket("max_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -233,9 +231,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -286,12 +284,13 @@ public class MaxBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        filter("filter", termQuery("tag", "tag0"))
+                        filter("filter")
+                                .filter(termQuery("tag", "tag0"))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -338,9 +337,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -388,9 +387,8 @@ public class MaxBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(maxBucket("max_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -416,9 +414,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(maxBucket("max_histo_bucket", "histo>_count")))
-                .addAggregation(maxBucket("max_terms_bucket", "terms>max_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(maxBucket("max_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(maxBucket("max_terms_bucket").setBucketsPaths("terms>max_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
index 55d4cb0..ff865e2 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
@@ -95,8 +93,8 @@ public class MinBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(minBucket("min_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -138,8 +136,8 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(minBucket("min_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -186,7 +184,7 @@ public class MinBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(minBucket("min_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -230,9 +228,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(minBucket("min_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -288,9 +286,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(minBucket("min_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -338,9 +336,8 @@ public class MinBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(minBucket("min_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -366,9 +363,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(minBucket("min_histo_bucket", "histo>_count")))
-                .addAggregation(minBucket("min_terms_bucket", "terms>min_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(minBucket("min_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(minBucket("min_terms_bucket").setBucketsPaths("terms>min_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
index 0e256a8..27f12bd 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
@@ -20,14 +20,11 @@ package org.elasticsearch.search.aggregations.pipeline;
  */
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucket;
@@ -54,7 +51,7 @@ import static org.hamcrest.core.IsNull.notNullValue;
 public class PercentilesBucketIT extends ESIntegTestCase {
 
     private static final String SINGLE_VALUED_FIELD_NAME = "l_value";
-    private static final double[] PERCENTS = {1.0, 25.0, 50.0, 75.0, 99.0};
+    private static final Double[] PERCENTS = {1.0, 25.0, 50.0, 75.0, 99.0};
     static int numDocs;
     static int interval;
     static int minRandomValue;
@@ -99,8 +96,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     public void testDocCountopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("histo>_count")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -141,8 +139,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>_count")
                                         .percents(PERCENTS))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -187,7 +186,8 @@ public class PercentilesBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -224,7 +224,8 @@ public class PercentilesBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -266,9 +267,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>sum")
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>sum")
                                         .percents(PERCENTS))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -322,9 +324,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>sum")
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>sum")
                                         .gapPolicy(BucketHelpers.GapPolicy.INSERT_ZEROS)
                                         .percents(PERCENTS)))
                 .execute().actionGet();
@@ -372,9 +375,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -395,9 +398,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
 
     public void testWrongPercents() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -421,34 +424,27 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testBadPercents() throws Exception {
-        double[] badPercents = {-1.0, 110.0};
+        Double[] badPercents = {-1.0, 110.0};
 
         try {
             client().prepareSearch("idx")
                     .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                    .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                    .addAggregation(percentilesBucket("percentiles_bucket")
+                            .setBucketsPaths("terms>sum")
                             .percents(badPercents)).execute().actionGet();
 
             fail("Illegal percent's were provided but no exception was thrown.");
-        } catch (Exception e) {
-            Throwable cause = ExceptionsHelper.unwrapCause(e);
-            if (cause == null) {
-                throw e;
-            } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
-                    throw e;
-                }
-            } else if (!(cause instanceof IllegalArgumentException)) {
-                throw e;
-            }
+        } catch (SearchPhaseExecutionException exception) {
+            ElasticsearchException[] rootCauses = exception.guessRootCauses();
+            assertThat(rootCauses.length, equalTo(1));
+            ElasticsearchException rootCause = rootCauses[0];
+            assertThat(rootCause.getMessage(), containsString("must only contain non-null doubles from 0.0-100.0 inclusive"));
         }
 
     }
 
     public void testBadPercents_asSubAgg() throws Exception {
-        double[] badPercents = {-1.0, 110.0};
+        Double[] badPercents = {-1.0, 110.0};
 
         try {
             client()
@@ -459,24 +455,17 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>_count")
                                         .percents(badPercents))).execute().actionGet();
 
             fail("Illegal percent's were provided but no exception was thrown.");
-        } catch (Exception e) {
-            Throwable cause = ExceptionsHelper.unwrapCause(e);
-            if (cause == null) {
-                throw e;
-            } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
-                    throw e;
-                }
-            } else if (!(cause instanceof IllegalArgumentException)) {
-                throw e;
-            }
+        } catch (SearchPhaseExecutionException exception) {
+            ElasticsearchException[] rootCauses = exception.guessRootCauses();
+            assertThat(rootCauses.length, equalTo(1));
+            ElasticsearchException rootCause = rootCauses[0];
+            assertThat(rootCause.getMessage(), containsString("must only contain non-null doubles from 0.0-100.0 inclusive"));
         }
 
     }
@@ -490,9 +479,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentile_histo_bucket", "histo>_count")))
-                .addAggregation(percentilesBucket("percentile_terms_bucket", "terms>percentile_histo_bucket.50")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentile_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(percentilesBucket("percentile_terms_bucket")
+                        .setBucketsPaths("terms>percentile_histo_bucket.50")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -546,7 +536,7 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testNestedWithDecimal() throws Exception {
-        double[] percent = {99.9};
+        Double[] percent = {99.9};
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
@@ -555,10 +545,12 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentile_histo_bucket", "histo>_count")
-                                        .percents(percent)))
-                .addAggregation(percentilesBucket("percentile_terms_bucket", "terms>percentile_histo_bucket[99.9]")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentile_histo_bucket")
+                                        .percents(percent)
+                                        .setBucketsPaths("histo>_count")))
+                .addAggregation(percentilesBucket("percentile_terms_bucket")
+                        .setBucketsPaths("terms>percentile_histo_bucket[99.9]")
                         .percents(percent)).execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
index 1739b2b..e962e90 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
@@ -20,11 +20,11 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
@@ -109,27 +109,27 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      * @param values Array of values to compute metric for
      * @param metric A metric builder which defines what kind of metric should be returned for the values
      */
-    public static double calculateMetric(double[] values, ValuesSourceAggregatorBuilder<?, ?> metric) {
+    public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder metric) {
 
-        if (metric instanceof MinAggregator.MinAggregatorBuilder) {
+        if (metric instanceof MinBuilder) {
             double accumulator = Double.POSITIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.min(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof MaxAggregator.MaxAggregatorBuilder) {
+        } else if (metric instanceof MaxBuilder) {
             double accumulator = Double.NEGATIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.max(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof SumAggregator.SumAggregatorBuilder) {
+        } else if (metric instanceof SumBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
             }
             return accumulator;
-        } else if (metric instanceof AvgAggregator.AvgAggregatorBuilder) {
+        } else if (metric instanceof AvgBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
deleted file mode 100644
index 9f03516..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder;
-
-public class SerialDifferenceTests
-        extends BasePipelineAggregationTestCase<SerialDiffPipelineAggregator.SerialDiffPipelineAggregatorBuilder> {
-
-    @Override
-    protected SerialDiffPipelineAggregatorBuilder createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        SerialDiffPipelineAggregatorBuilder factory = new SerialDiffPipelineAggregatorBuilder(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            factory.lag(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
index a16a430..949350c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucket;
@@ -95,8 +93,8 @@ public class StatsBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(statsBucket("stats_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -139,8 +137,8 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -188,7 +186,7 @@ public class StatsBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(statsBucket("stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -233,9 +231,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -292,9 +290,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -344,9 +342,8 @@ public class StatsBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(statsBucket("stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -371,9 +368,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(statsBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(statsBucket("avg_terms_bucket", "terms>avg_histo_bucket.avg")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(statsBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(statsBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket.avg")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
index e372f66..0d9a324 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -94,8 +92,8 @@ public class SumBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(sumBucket("sum_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -129,8 +127,8 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -169,7 +167,7 @@ public class SumBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(sumBucket("sum_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -205,9 +203,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -255,9 +253,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -298,9 +296,8 @@ public class SumBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(sumBucket("sum_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -325,9 +322,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(sumBucket("sum_histo_bucket", "histo>_count")))
-                .addAggregation(sumBucket("sum_terms_bucket", "terms>sum_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(sumBucket("sum_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(sumBucket("sum_terms_bucket").setBucketsPaths("terms>sum_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
deleted file mode 100644
index fdc52c3..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-
-public abstract class AbstractBucketMetricsTestCase<PAF extends BucketMetricsPipelineAggregatorBuilder>
-        extends BasePipelineAggregationTestCase<PAF> {
-
-    @Override
-    protected final PAF createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        PAF factory = doCreateTestAggregatorFactory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-    protected abstract PAF doCreateTestAggregatorFactory(String name, String bucketsPath);
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
deleted file mode 100644
index 6a7f013..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator.AvgBucketPipelineAggregatorBuilder;
-
-public class AvgBucketTests extends AbstractBucketMetricsTestCase<AvgBucketPipelineAggregator.AvgBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected AvgBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new AvgBucketPipelineAggregatorBuilder(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
deleted file mode 100644
index dce0eb6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-
-public class ExtendedStatsBucketTests
-        extends AbstractBucketMetricsTestCase<ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name,
-            String bucketsPath) {
-        ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder factory =
-                new ExtendedStatsBucketPipelineAggregator.ExtendedStatsBucketPipelineAggregatorBuilder(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, false));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
deleted file mode 100644
index f7bf7fd..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator.MaxBucketPipelineAggregatorBuilder;
-
-public class MaxBucketTests extends AbstractBucketMetricsTestCase<MaxBucketPipelineAggregator.MaxBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected MaxBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new MaxBucketPipelineAggregatorBuilder(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
deleted file mode 100644
index 0df284c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator.MinBucketPipelineAggregatorBuilder;
-
-public class MinBucketTests extends AbstractBucketMetricsTestCase<MinBucketPipelineAggregator.MinBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected MinBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new MinBucketPipelineAggregatorBuilder(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
deleted file mode 100644
index b78515f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-
-public class PercentilesBucketTests
-        extends AbstractBucketMetricsTestCase<PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name,
-            String bucketsPath) {
-        PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder factory = 
-                new PercentilesBucketPipelineAggregator.PercentilesBucketPipelineAggregatorBuilder(name, bucketsPath);
-        if (randomBoolean()) {
-            int numPercents = randomIntBetween(1, 20);
-            double[] percents = new double[numPercents];
-            for (int i = 0; i < numPercents; i++) {
-                percents[i] = randomDoubleBetween(0.0, 100.0, false);
-            }
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
deleted file mode 100644
index 2145ac0..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-
-public class StatsBucketTests extends AbstractBucketMetricsTestCase<StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name,
-            String bucketsPath) {
-        return new StatsBucketPipelineAggregator.StatsBucketPipelineAggregatorBuilder(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
deleted file mode 100644
index 0c0db16..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator.SumBucketPipelineAggregatorBuilder;
-
-public class SumBucketTests extends AbstractBucketMetricsTestCase<SumBucketPipelineAggregator.SumBucketPipelineAggregatorBuilder> {
-
-    @Override
-    protected SumBucketPipelineAggregatorBuilder doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new SumBucketPipelineAggregatorBuilder(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index cfec885..9ccc033 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -23,10 +23,10 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
@@ -38,8 +38,6 @@ import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersM
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -79,7 +77,7 @@ public class MovAvgIT extends ESIntegTestCase {
     static int period;
     static HoltWintersModel.SeasonalityType seasonalityType;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -275,7 +273,7 @@ public class MovAvgIT extends ESIntegTestCase {
     }
 
     /**
-     * Exponentionally weighted (EWMA, Single exponential) moving avg
+     * Exponentially weighted (EWMA, Single exponential) moving avg
      *
      * @param window Window of values to compute movavg for
      */
@@ -410,16 +408,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts","_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values","the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -458,16 +458,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new LinearModel.LinearModelBuilder())
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new LinearModel.LinearModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -506,16 +508,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(alpha))
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(alpha))
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -554,16 +558,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -602,20 +608,22 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .alpha(alpha).beta(beta).gamma(gamma).period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(false))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(false)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .alpha(alpha).beta(beta).gamma(gamma).period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(false))
+                                        .minimize(false)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -660,8 +668,8 @@ public class MovAvgIT extends ESIntegTestCase {
                                 .interval(1)
                                 .subAggregation(avg("avg").field(VALUE_FIELD))
                                 .subAggregation(
-                                        movingAvg("movavg_values", "avg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(5))).execute().actionGet();
+                                        movingAvg("movavg_values").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(5).setBucketsPaths("avg"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -706,16 +714,17 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(0)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a window that is zero");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), is("[window] must be a positive integer: [movavg_counts]"));
+        } catch (SearchPhaseExecutionException e) {
+           assertThat(e.getMessage(), is("all shards failed"));
         }
     }
 
@@ -726,10 +735,11 @@ public class MovAvgIT extends ESIntegTestCase {
                     .addAggregation(
                             range("histo").field(INTERVAL_FIELD).addRange(0, 10)
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
-                                            .window(windowSize)
+                                    .subAggregation(movingAvg("movavg_counts")
+                                            .window(0)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept non-histogram as parent");
 
@@ -744,16 +754,20 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(-10)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a window that is negative");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), is("[window] must be a positive integer: [movavg_counts]"));
+
+        } catch (SearchPhaseExecutionException exception) {
+            //Throwable rootCause = exception.unwrapCause();
+            //assertThat(rootCause, instanceOf(SearchParseException.class));
+            //assertThat("[window] value must be a positive, non-zero integer.  Value supplied was [0] in [movingAvg].", equalTo(exception.getMessage()));
         }
     }
 
@@ -764,10 +778,11 @@ public class MovAvgIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field("test").interval(interval)
                                 .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -786,10 +801,11 @@ public class MovAvgIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field("test").interval(interval)
                                 .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
                                         .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric")
                                         .predict(numPredictions))
                 ).execute().actionGet();
 
@@ -808,17 +824,18 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(windowSize)
                                             .modelBuilder(randomModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .predict(0))
+                                            .predict(0)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a prediction size that is zero");
 
-        } catch (IllegalArgumentException exception) {
+        } catch (SearchPhaseExecutionException exception) {
             // All Good
         }
     }
@@ -829,17 +846,18 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(windowSize)
                                             .modelBuilder(randomModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .predict(-10))
+                                            .predict(-10)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a prediction size that is negative");
 
-        } catch (IllegalArgumentException exception) {
+        } catch (SearchPhaseExecutionException exception) {
             // All Good
         }
     }
@@ -850,18 +868,20 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(10)
                                             .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                     .alpha(alpha).beta(beta).gamma(gamma).period(20).seasonalityType(seasonalityType))
-                                            .gapPolicy(gapPolicy))
-                                    .subAggregation(movingAvg("movavg_values", "the_metric")
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
+                                    .subAggregation(movingAvg("movavg_values")
                                             .window(windowSize)
                                             .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                     .alpha(alpha).beta(beta).gamma(gamma).period(20).seasonalityType(seasonalityType))
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             // All good
@@ -878,13 +898,14 @@ public class MovAvgIT extends ESIntegTestCase {
                                 .field(INTERVAL_FIELD)
                                 .interval(1)
                                 .subAggregation(avg("avg").field(VALUE_FIELD))
-                                .subAggregation(derivative("deriv", "avg").gapPolicy(gapPolicy))
+                                .subAggregation(derivative("deriv")
+                                        .setBucketsPaths("avg").gapPolicy(gapPolicy))
                                 .subAggregation(
-                                        movingAvg("avg_movavg", "avg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(12))
+                                        movingAvg("avg_movavg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(12).setBucketsPaths("avg"))
                                 .subAggregation(
-                                        movingAvg("deriv_movavg", "deriv").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(12))
+                                        movingAvg("deriv_movavg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(12).setBucketsPaths("deriv"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -986,12 +1007,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(10)
                                             .modelBuilder(randomModelBuilder(100))
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             // All good
@@ -1004,20 +1026,22 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1090,18 +1114,20 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(numBuckets)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(numBuckets)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1144,13 +1170,14 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(numBuckets)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
                 ).execute().actionGet();
             fail("Simple Model cannot be minimized, but an exception was not thrown");
         } catch (SearchPhaseExecutionException e) {
@@ -1162,13 +1189,14 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(numBuckets)
                                             .modelBuilder(new LinearModel.LinearModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .minimize(true))
+                                            .minimize(true)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
             fail("Linear Model cannot be minimized, but an exception was not thrown");
         } catch (SearchPhaseExecutionException e) {
@@ -1192,13 +1220,14 @@ public class MovAvgIT extends ESIntegTestCase {
                         .prepareSearch("idx").setTypes("type")
                         .addAggregation(
                                 histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                        .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                        .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                         .subAggregation(metric)
-                                        .subAggregation(movingAvg("movavg_counts", "_count")
+                                        .subAggregation(movingAvg("movavg_counts")
                                                 .window(numBuckets)
                                                 .modelBuilder(builder)
                                                 .gapPolicy(gapPolicy)
-                                                .minimize(true))
+                                                .minimize(true)
+                                                .setBucketsPaths("_count"))
                         ).execute().actionGet();
             } catch (SearchPhaseExecutionException e) {
                 fail("Model [" + builder.toString() + "] can be minimized, but an exception was thrown");
@@ -1206,6 +1235,43 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
+    public void testUnrecognizedParams() {
+        MovAvgModelBuilder[] builders = new MovAvgModelBuilder[]{
+                new SimpleModel.SimpleModelBuilder(),
+                new LinearModel.LinearModelBuilder(),
+                new EwmaModel.EWMAModelBuilder(),
+                new HoltLinearModel.HoltLinearModelBuilder(),
+                new HoltWintersModel.HoltWintersModelBuilder()
+        };
+        Map<String, Object> badSettings = new HashMap<>(1);
+        badSettings.put("abc", 1.2);
+
+        for (MovAvgModelBuilder builder : builders) {
+            try {
+                SearchResponse response = client()
+                        .prepareSearch("idx").setTypes("type")
+                        .addAggregation(
+                                histogram("histo").field(INTERVAL_FIELD).interval(interval)
+                                        .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
+                                        .subAggregation(metric)
+                                        .subAggregation(movingAvg("movavg_counts")
+                                                .window(10)
+                                                .modelBuilder(builder)
+                                                .gapPolicy(gapPolicy)
+                                                .settings(badSettings)
+                                                .setBucketsPaths("_count"))
+                        ).execute().actionGet();
+            } catch (SearchPhaseExecutionException e) {
+                // All good
+            }
+        }
+
+
+
+
+    }
+
+
     private void assertValidIterators(Iterator expectedBucketIter, Iterator expectedCountsIter, Iterator expectedValuesIter) {
         if (!expectedBucketIter.hasNext()) {
             fail("`expectedBucketIter` iterator ended before `actual` iterator, size mismatch");
@@ -1289,8 +1355,7 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> randomMetric(String name,
-            String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
deleted file mode 100644
index db52a74..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.moving.avg;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.EwmaModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltLinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel.SeasonalityType;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;;
-
-public class MovAvgTests extends BasePipelineAggregationTestCase<MovAvgPipelineAggregator.MovAvgPipelineAggregatorBuilder> {
-
-    @Override
-    protected MovAvgPipelineAggregatorBuilder createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        MovAvgPipelineAggregatorBuilder factory = new MovAvgPipelineAggregatorBuilder(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            switch (randomInt(4)) {
-            case 0:
-                factory.modelBuilder(new SimpleModel.SimpleModelBuilder());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 1:
-                factory.modelBuilder(new LinearModel.LinearModelBuilder());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new EwmaModel.EWMAModelBuilder());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new HoltLinearModel.HoltLinearModelBuilder());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(randomDouble()).beta(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 4:
-            default:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new HoltWintersModel.HoltWintersModelBuilder());
-                    factory.window(randomIntBetween(2, 100));
-                } else {
-                    int period = randomIntBetween(1, 100);
-                    factory.modelBuilder(
-                            new HoltWintersModel.HoltWintersModelBuilder().alpha(randomDouble()).beta(randomDouble()).gamma(randomDouble())
-                                    .period(period).seasonalityType(randomFrom(SeasonalityType.values())).pad(randomBoolean()));
-                    factory.window(randomIntBetween(2 * period, 200 * period));
-                }
-                break;
-            }
-        }
-        factory.predict(randomIntBetween(1, 50));
-        if (factory.model().canBeMinimized() && randomBoolean()) {
-            factory.minimize(randomBoolean());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index 66961c2..aebd6a7 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -20,16 +20,15 @@
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
 import org.elasticsearch.search.aggregations.pipeline.SimpleValue;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -61,7 +60,7 @@ public class SerialDiffIT extends ESIntegTestCase {
     static int numBuckets;
     static int lag;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -81,7 +80,7 @@ public class SerialDiffIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> randomMetric(String name, String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
@@ -232,14 +231,16 @@ public class SerialDiffIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(diff("diff_counts", "_count")
+                                .subAggregation(diff("diff_counts")
                                         .lag(lag)
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(diff("diff_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(diff("diff_values")
                                         .lag(lag)
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -279,14 +280,15 @@ public class SerialDiffIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(diff("diff_counts", "_count")
+                                .subAggregation(diff("diff_counts")
                                         .lag(-1)
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
                 ).execute().actionGet();
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), is("[lag] must be a positive integer: [diff_counts]"));
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.getMessage(), is("all shards failed"));
         }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
deleted file mode 100644
index a297181..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ValuesSourceTypeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(ValuesSourceType.ANY.ordinal(), equalTo(0));
-        assertThat(ValuesSourceType.NUMERIC.ordinal(), equalTo(1));
-        assertThat(ValuesSourceType.BYTES.ordinal(), equalTo(2));
-        assertThat(ValuesSourceType.GEOPOINT.ordinal(), equalTo(3));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.ANY.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.NUMERIC.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.BYTES.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(2));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.GEOPOINT.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(3));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.ANY));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.NUMERIC));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(2);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.BYTES));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(3);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.GEOPOINT));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(4, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                ValuesSourceType.ANY.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown ValuesSourceType ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index fec1ca7..fc1ee7a 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -344,8 +344,8 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
                 .query(termQuery("multi", "test"))
                 .from(0).size(20).explain(true)
                 .aggregation(AggregationBuilders.global("global").subAggregation(
-                        AggregationBuilders.filter("all", termQuery("multi", "test"))))
-                .aggregation(AggregationBuilders.filter("test1", termQuery("name", "test1")));
+                        AggregationBuilders.filter("all").filter(termQuery("multi", "test"))))
+                .aggregation(AggregationBuilders.filter("test1").filter(termQuery("name", "test1")));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(sourceBuilder)).actionGet();
         assertNoFailures(searchResponse);
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 96a7448..786d594 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -19,11 +19,6 @@
 
 package org.elasticsearch.search.builder;
 
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -31,7 +26,6 @@ import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Injector;
 import org.elasticsearch.common.inject.ModulesBuilder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -46,28 +40,15 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
 import org.elasticsearch.index.query.AbstractQueryTestCase;
 import org.elasticsearch.index.query.EmptyQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.MockScriptEngine;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder.InnerHit;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
@@ -79,10 +60,6 @@ import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.InternalSettingsPlugin;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.threadpool.ThreadPoolModule;
 import org.junit.AfterClass;
@@ -90,10 +67,7 @@ import org.junit.BeforeClass;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
 import java.util.List;
-import java.util.Set;
 import java.util.concurrent.TimeUnit;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -105,113 +79,47 @@ public class SearchSourceBuilderTests extends ESTestCase {
 
     private static IndicesQueriesRegistry indicesQueriesRegistry;
 
-    private static Index index;
-
-    private static AggregatorParsers aggParsers;
-
-    private static String[] currentTypes;
-
-    private static ParseFieldMatcher parseFieldMatcher;
-
     @BeforeClass
     public static void init() throws IOException {
-        // we have to prefer CURRENT since with the range of versions we support
-        // it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT
-                : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
         Settings settings = Settings.settingsBuilder()
-                .put("node.name", AbstractQueryTestCase.class.toString())
+                .put("node.name", SearchSourceBuilderTests.class.toString())
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-
+                .build();
         namedWriteableRegistry = new NamedWriteableRegistry();
-        index = new Index(randomAsciiOfLengthBetween(1, 10), "_na_");
-        Settings indexSettings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder()
-                .put(new IndexMetaData.Builder(index.getName()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        SettingsModule settingsModule = new SettingsModule(settings);
-        settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                        // no file watching, so we don't need a
-                        // ResourceWatcherService
-                        .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections
-                        .singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null,
-                            scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch (IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
         injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)), settingsModule,
+                new SettingsModule(settings),
                 new ThreadPoolModule(new ThreadPool(settings)),
-                scriptModule, new IndicesModule() {
-
+                new SearchModule(settings, namedWriteableRegistry) {
                     @Override
-                    protected void configure() {
-                        bindMapperExtension();
+                    protected void configureSearch() {
+                        // skip me so we don't need transport
                     }
-                }, new SearchModule(settings, namedWriteableRegistry) {
                     @Override
-                    protected void configureSearch() {
-                        // Skip me
+                    protected void configureAggs() {
+                        // skip me so we don't need scripting
                     }
                     @Override
                     protected void configureSuggesters() {
-                        // Skip me
+                        // skip me so we don't need IndicesService
                     }
                 },
-                new IndexSettingsModule(index, settings),
-
                 new AbstractModule() {
                     @Override
                     protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
+                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
                         bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
                     }
                 }
         ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        // create some random type with some default field, those types will
-        // stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
         indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
     }
 
     @AfterClass
     public static void afterClass() throws Exception {
         terminate(injector.getInstance(ThreadPool.class));
         injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
         namedWriteableRegistry = null;
+        indicesQueriesRegistry = null;
     }
 
     protected final SearchSourceBuilder createSearchSourceBuilder() throws IOException {
@@ -431,7 +339,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
             // NORELEASE need a random aggregation builder method
             builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
         }
-        if (randomBoolean()) {
+        if (true) {
             // NORELEASE need a method to randomly build content for ext
             XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
             xContentBuilder.startObject();
@@ -459,7 +367,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
         if (randomBoolean()) {
             parser.nextToken(); // sometimes we move it on the START_OBJECT to test the embedded case
         }
-        SearchSourceBuilder newBuilder = SearchSourceBuilder.parseSearchSource(parser, parseContext, aggParsers);
+        SearchSourceBuilder newBuilder = SearchSourceBuilder.parseSearchSource(parser, parseContext);
         assertNotSame(testBuilder, newBuilder);
         assertEquals(testBuilder, newBuilder);
         assertEquals(testBuilder.hashCode(), newBuilder.hashCode());
@@ -468,7 +376,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
     private static QueryParseContext createParseContext(XContentParser parser) {
         QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
         context.reset(parser);
-        context.parseFieldMatcher(parseFieldMatcher);
+        context.parseFieldMatcher(ParseFieldMatcher.STRICT);
         return context;
     }
 
@@ -523,8 +431,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
         {
             String restContent = " { \"_source\": { \"includes\": \"include\", \"excludes\": \"*.field2\"}}";
             try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser),
-                        aggParsers);
+                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
                 assertArrayEquals(new String[]{"*.field2" }, searchSourceBuilder.fetchSource().excludes());
                 assertArrayEquals(new String[]{"include" }, searchSourceBuilder.fetchSource().includes());
             }
@@ -532,8 +439,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
         {
             String restContent = " { \"_source\": false}";
             try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser),
-                        aggParsers);
+                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
                 assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().excludes());
                 assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().includes());
                 assertFalse(searchSourceBuilder.fetchSource().fetchSource());
@@ -545,8 +451,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
         {
             String restContent = " { \"sort\": \"foo\"}";
             try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser),
-                        aggParsers);
+                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
                 assertEquals(1, searchSourceBuilder.sorts().size());
                 assertEquals("{\"foo\":{}}", searchSourceBuilder.sorts().get(0).toUtf8());
             }
@@ -561,8 +466,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
                     "        \"_score\"\n" +
                     "    ]}";
             try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser),
-                        aggParsers);
+                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
                 assertEquals(5, searchSourceBuilder.sorts().size());
                 assertEquals("{\"post_date\":{\"order\":\"asc\"}}", searchSourceBuilder.sorts().get(0).toUtf8());
                 assertEquals("\"user\"", searchSourceBuilder.sorts().get(1).toUtf8());
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index d8f76c0..67420bd 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -429,7 +429,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                 .prepareSearch("test")
                 .setQuery(hasChildQuery("child", boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))))
                 .addAggregation(AggregationBuilders.global("global").subAggregation(
-                        AggregationBuilders.filter("filter", boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))).subAggregation(
+                        AggregationBuilders.filter("filter").filter(boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))).subAggregation(
                                 AggregationBuilders.terms("facet1").field("c_field")))).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index eaad153..25dd81a 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -1727,7 +1727,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         response = search.get();
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
 
-        // Make sure the the highlightQuery is taken into account when it is set on the highlight context instead of the field
+        // Make sure the highlightQuery is taken into account when it is set on the highlight context instead of the field
         highlightBuilder.highlightQuery(matchQuery("text", "query"));
         field.highlighterType("fvh").highlightQuery(null);
         response = search.get();
diff --git a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
index 05ff048..be0fd8a 100644
--- a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
@@ -115,7 +115,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
         innerHitsBuilder.addNestedInnerHits("comment", "comments",
                 new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox")));
-        // Inner hits can be defined in two ways: 1) with the query 2) as seperate inner_hit definition
+        // Inner hits can be defined in two ways: 1) with the query 2) as separate inner_hit definition
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
@@ -142,7 +142,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         innerHitsBuilder.addNestedInnerHits("comment", "comments",
                 new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant")));
         // Inner hits can be defined in two ways: 1) with the query 2) as
-        // seperate inner_hit definition
+        // separate inner_hit definition
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")))
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreModeTests.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreModeTests.java
index 7b4cafe..8254c7b 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreModeTests.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreModeTests.java
@@ -30,7 +30,7 @@ import java.io.IOException;
 public class QueryRescoreModeTests extends ESTestCase {
 
     /**
-     * Test @link {@link QueryRescoreMode} enum ordinals and names, since serilaization relies on it
+     * Test @link {@link QueryRescoreMode} enum ordinals and names, since serialization relies on it
      */
     public void testQueryRescoreMode() throws IOException {
         float primary = randomFloat();
diff --git a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
index 7739124..1df9659 100644
--- a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
+++ b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
@@ -23,8 +23,10 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Assert;
@@ -38,8 +40,6 @@ import static org.elasticsearch.test.ESIntegTestCase.client;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 
 public class SharedSignificantTermsTestMethods {
     public static final String INDEX_NAME = "testidx";
@@ -57,9 +57,13 @@ public class SharedSignificantTermsTestMethods {
     }
 
     private static void checkSignificantTermsAggregationCorrect(ESIntegTestCase testCase) {
-        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE).addAggregation(
-                terms("class").field(CLASS_FIELD).subAggregation(significantTerms("sig_terms").field(TEXT_FIELD)))
-                .execute().actionGet();
+
+        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(
+                        new SignificantTermsBuilder("sig_terms")
+                                .field(TEXT_FIELD)))
+                .execute()
+                .actionGet();
         assertSearchResponse(response);
         StringTerms classes = response.getAggregations().get("class");
         Assert.assertThat(classes.getBuckets().size(), equalTo(2));
diff --git a/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-classes.jar b/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-classes.jar
index a7d9c55..8286a6e 100644
Binary files a/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-classes.jar and b/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-classes.jar differ
diff --git a/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-xmlbeans-classes.jar b/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-xmlbeans-classes.jar
index a5d3abc..8bebe92 100644
Binary files a/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-xmlbeans-classes.jar and b/core/src/test/resources/org/elasticsearch/bootstrap/duplicate-xmlbeans-classes.jar differ
diff --git a/docs/reference/aggregations/bucket.asciidoc b/docs/reference/aggregations/bucket.asciidoc
index 66ce2d8..2d185dd 100644
--- a/docs/reference/aggregations/bucket.asciidoc
+++ b/docs/reference/aggregations/bucket.asciidoc
@@ -19,8 +19,6 @@ include::bucket/datehistogram-aggregation.asciidoc[]
 
 include::bucket/daterange-aggregation.asciidoc[]
 
-include::bucket/diversified-sampler-aggregation.asciidoc[]
-
 include::bucket/filter-aggregation.asciidoc[]
 
 include::bucket/filters-aggregation.asciidoc[]
diff --git a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
deleted file mode 100644
index 92effce..0000000
--- a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
+++ /dev/null
@@ -1,154 +0,0 @@
-[[search-aggregations-bucket-sampler-aggregation]]
-=== Sampler Aggregation
-
-experimental[]
-
-A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents. Diversity settings are 
-used to limit the number of matches that share a common value such as an "author".
-
-.Example use cases:
-* Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
-* Removing bias from analytics by ensuring fair representation of content from different sources
-* Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
- 
-
-Example:
-
-[source,js]
---------------------------------------------------
-{
-    "query": {
-        "match": {
-            "text": "iphone"
-        }
-    },
-    "aggs": {
-        "sample": {
-            "sampler": {
-                "shard_size": 200,
-                "field" : "user.id"   
-            },
-            "aggs": {
-                "keywords": {
-                    "significant_terms": {
-                        "field": "text"
-                    }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Response:
-
-[source,js]
---------------------------------------------------
-{
-    ...
-        "aggregations": {
-        "sample": {
-            "doc_count": 1000,<1>
-            "keywords": {<2>
-                "doc_count": 1000,
-                "buckets": [
-                    ...
-                    {
-                        "key": "bend",
-                        "doc_count": 58,
-                        "score": 37.982536582524276,
-                        "bg_count": 103
-                    },
-                    ....
-}
---------------------------------------------------
-
-<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
-<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
-
-
-==== shard_size
-
-The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
-The default value is 100.
-
-==== Controlling diversity
-=`field` or `script` and `max_docs_per_value` settings are used to control the maximum number of documents collected on any one shard which share a common value.
-The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
-
-The aggregation will throw an error if the choice of field or script produces multiple values for a document.
-It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
-
-NOTE: Any good market researcher will tell you that when working with samples of data it is important
-that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
-The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
-
-==== Field
-
-Controlling diversity using a field:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "field" : "author",
-                "max_docs_per_value" : 3
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
-It is not intended as a way of providing a global de-duplication feature on search results.
-
-
-
-==== Script
-
-Controlling diversity using a script:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "script" : "doc['author'].value + '/' + doc['genre'].value"
-            }
-        }
-    }
-}
---------------------------------------------------
-Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
-each shard sample has, at most, one match for an author/genre pair.
-
-
-==== execution_hint
-
-When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
-Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
- 
- - hold field values directly (`map`)
- - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
- - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
- 
-The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
-The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
-Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
-
-==== Limitations
-
-===== Cannot be nested under `breadth_first` aggregations
-Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
-It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
-
-===== Limited de-dup logic.
-The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
-
-===== No specialized syntax for geo/date fields
-Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
-This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
index 741edc8..2974270 100644
--- a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
@@ -4,9 +4,11 @@
 experimental[]
 
 A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents.
+Optionally, diversity settings can be used to limit the number of matches that share a common value such as an "author".
 
 .Example use cases:
 * Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
+* Removing bias from analytics by ensuring fair representation of content from different sources
 * Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
  
 
@@ -23,7 +25,8 @@ Example:
     "aggs": {
         "sample": {
             "sampler": {
-                "shard_size": 200
+                "shard_size": 200,
+                "field" : "user.id"   
             },
             "aggs": {
                 "keywords": {
@@ -60,7 +63,8 @@ Response:
 }
 --------------------------------------------------
 
-<1> 1000 documents were sampled in total because we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
 
 
 ==== shard_size
@@ -68,9 +72,83 @@ Response:
 The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
 The default value is 100.
 
+==== Controlling diversity
+Optionally, you can use the `field` or `script` and `max_docs_per_value` settings to control the maximum number of documents collected on any one shard which share a common value.
+The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
+
+The aggregation will throw an error if the choice of field or script produces multiple values for a document.
+It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
+
+NOTE: Any good market researcher will tell you that when working with samples of data it is important
+that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
+The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
+
+==== Field
+
+Controlling diversity using a field:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "field" : "author",
+                "max_docs_per_value" : 3
+            }
+        }
+    }
+}
+--------------------------------------------------
+
+Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
+It is not intended as a way of providing a global de-duplication feature on search results.
+
+
+
+==== Script
+
+Controlling diversity using a script:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "script" : "doc['author'].value + '/' + doc['genre'].value"
+            }
+        }
+    }
+}
+--------------------------------------------------
+Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
+each shard sample has, at most, one match for an author/genre pair.
+
+
+==== execution_hint
+
+When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
+Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
+ 
+ - hold field values directly (`map`)
+ - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
+ - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
+ 
+The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
+The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
+Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
+
 ==== Limitations
 
 ===== Cannot be nested under `breadth_first` aggregations
 Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
 It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
\ No newline at end of file
+In this situation an error will be thrown.
+
+===== Limited de-dup logic.
+The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
+
+===== No specialized syntax for geo/date fields
+Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
+This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
index 5246d0d..a866f33 100644
--- a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
+++ b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
@@ -530,10 +530,9 @@ public class MoreExpressionTests extends ESIntegTestCase {
                                 .subAggregation(sum("twoSum").field("two"))
                                 .subAggregation(sum("threeSum").field("three"))
                                 .subAggregation(sum("fourSum").field("four"))
-                                .subAggregation(bucketScript("totalSum",
-                                    new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null),
-                                    "twoSum", "threeSum", "fourSum")))
-                .execute().actionGet();
+                                .subAggregation(
+                                        bucketScript("totalSum").setBucketsPaths("twoSum", "threeSum", "fourSum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))).execute().actionGet();
 
         InternalHistogram<Bucket> histogram = response.getAggregations().get("histogram");
         assertThat(histogram, notNullValue());
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
index 28549a1..06119fd 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
@@ -113,8 +113,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)
-                                                , "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -157,8 +157,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 / _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 / _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -199,8 +199,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .interval(interval)
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0", ScriptType.INLINE, null, null),
-                                                "field2Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum").script(
+                                                new Script("_value0", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -241,7 +241,7 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", bucketsPathsMap, 
+                                        bucketScript("seriesArithmetic").setBucketsPathsMap(bucketsPathsMap ).script(
                                                 new Script("foo + bar + baz", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -287,8 +287,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("(_value0 + _value1 + _value2) * factor", ScriptType.INLINE, null, params),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("(_value0 + _value1 + _value2) * factor", ScriptType.INLINE, null, params)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -331,8 +331,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)).gapPolicy(GapPolicy.INSERT_ZEROS))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -377,8 +377,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("my_script", ScriptType.INDEXED, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("my_script", ScriptType.INDEXED, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -421,8 +421,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum")))
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null))))
                                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -444,8 +444,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
index ffee8c7..e0eb23a 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
@@ -27,7 +27,6 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -46,8 +45,8 @@ import java.util.Map;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
-import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.bucketSelector;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.derivative;
+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.having;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
@@ -117,13 +116,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testInlineScript() {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector",
-                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE, null, null),
-                                "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -154,8 +159,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? true : (_value0 < 10000)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? true : (_value0 < 10000)", ScriptType.INLINE, null,
+                                                        null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -187,8 +193,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 > 10000)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 > 10000)", ScriptType.INLINE, null,
+                                                        null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -209,8 +216,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 < _value1)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 < _value1)", ScriptType.INLINE, null,
+                                                        null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -240,8 +248,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .interval(interval)
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 > 100)",
-                                                ScriptType.INLINE,null, null), "field2Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum")
+                                                .script(new Script("Double.isNaN(_value0) ? false : (_value0 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -264,12 +273,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
         bucketPathsMap.put("my_value1", "field2Sum");
         bucketPathsMap.put("my_value2", "field3Sum");
 
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector", bucketPathsMap, new Script(
-                                "Double.isNaN(my_value1) ? false : (my_value1 + my_value2 > 100)", ScriptType.INLINE, null, null))))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPathsMap(bucketPathsMap).script(
+                                                new Script("Double.isNaN(my_value1) ? false : (my_value1 + my_value2 > 100)",
+                                                        ScriptType.INLINE, null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -293,12 +309,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     public void testInlineScriptWithParams() {
         Map<String, Object> params = new HashMap<>();
         params.put("threshold", 100);
-        SearchResponse response = client().prepareSearch("idx").addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                .subAggregation(bucketSelector("bucketSelector",
-                        new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > threshold)", ScriptType.INLINE, null, params),
-                        "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > threshold)",
+                                                        ScriptType.INLINE, null, params)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -320,13 +343,17 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testInlineScriptInsertZeros() {
-        SearchResponse response = client().prepareSearch("idx")
+        SearchResponse response = client()
+                .prepareSearch("idx")
                 .addAggregation(
-                        histogram("histo").field(FIELD_1_NAME).interval(interval).subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                                .subAggregation(bucketSelector("bucketSelector",
-                                        new Script("_value0 + _value1 > 100", ScriptType.INLINE, null, null), "field2Sum", "field3Sum")
-                                                .gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").gapPolicy(GapPolicy.INSERT_ZEROS)
+                                                .script(new Script("_value0 + _value1 > 100", ScriptType.INLINE, null, null))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -358,8 +385,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("my_script", ScriptType.INDEXED, null, null),
-                                                "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("my_script", ScriptType.INDEXED, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -381,13 +408,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testUnmapped() throws Exception {
-        SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector",
-                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE, null, null),
-                                "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx_unmapped")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -398,13 +431,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testPartiallyUnmapped() throws Exception {
-        SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector",
-                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE, null, null),
-                                "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx", "idx_unmapped")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -428,8 +467,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
     public void testEmptyBuckets() {
         SearchResponse response = client().prepareSearch("idx_with_gaps")
                 .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(1)
-                        .subAggregation(histogram("inner_histo").field(FIELD_1_NAME).interval(1).extendedBounds(new ExtendedBounds(1L, 4L))
-                                .minDocCount(0).subAggregation(derivative("derivative", "_count").gapPolicy(GapPolicy.INSERT_ZEROS))))
+                        .subAggregation(histogram("inner_histo").field(FIELD_1_NAME).interval(1).extendedBounds(1L, 4L).minDocCount(0)
+                                .subAggregation(derivative("derivative").setBucketsPaths("_count").gapPolicy(GapPolicy.INSERT_ZEROS))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
index 44f7a93..37b1e11 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
@@ -26,7 +26,9 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.Max;
+import org.elasticsearch.search.aggregations.metrics.min.Min;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
@@ -43,6 +45,8 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.dateRange;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.min;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
@@ -108,7 +112,7 @@ public class DateRangeTests extends ESIntegTestCase {
     }
 
     public void testDateMath() throws Exception {
-        DateRangeAggregatorBuilder rangeBuilder = dateRange("range");
+        DateRangeBuilder rangeBuilder = dateRange("range");
         if (randomBoolean()) {
             rangeBuilder.field("date");
         } else {
@@ -452,8 +456,62 @@ public class DateRangeTests extends ESIntegTestCase {
         assertThat((long) propertiesDocCounts[2], equalTo(numDocs - 4L));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateRange("range")
+                        .field("date")
+                        .addUnboundedTo("r1", date(2, 15))
+                        .addRange("r2", date(2, 15), date(3, 15))
+                        .addUnboundedFrom("r3", date(3, 15))
+                        .subAggregation(min("min")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
 
 
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r1"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Min min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(1, 2).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r2"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(2, 15).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r3"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(3, 15).getMillis()));
+    }
+
     /*
         Jan 2,  Feb 3,      1
         Feb 2,  Mar 3,      2
@@ -574,10 +632,62 @@ public class DateRangeTests extends ESIntegTestCase {
         Apr 23, May 24      6
      */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateRange("range")
+                        .field("dates")
+                                .script(new Script("new DateTime(_value.longValue(), DateTimeZone.UTC).plusMonths(1).getMillis()"))
+                                .addUnboundedTo(date(2, 15)).addRange(date(2, 15), date(3, 15)).addUnboundedFrom(date(3, 15))
+                                .subAggregation(max("max"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-2012-02-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(3, 3).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-02-15T00:00:00.000Z-2012-03-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(4, 3).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-03-15T00:00:00.000Z-*"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 1L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateRange("range")
-                        .script(new Script("doc['date'].value"))
+.script(new Script("doc['date'].value"))
                         .addUnboundedTo(date(2, 15))
                         .addRange(date(2, 15), date(3, 15))
                         .addUnboundedFrom(date(3, 15)))
@@ -620,7 +730,57 @@ public class DateRangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        dateRange("range").script(new Script("doc['date'].value")).addUnboundedTo(date(2, 15))
+                                .addRange(date(2, 15), date(3, 15)).addUnboundedFrom(date(3, 15)).subAggregation(max("max"))).execute()
+                .actionGet();
 
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-2012-02-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(2, 2).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-02-15T00:00:00.000Z-2012-03-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(3, 2).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-03-15T00:00:00.000Z-*"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+    }
 
 
 
@@ -676,6 +836,58 @@ public class DateRangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(numDocs - 2L));
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateRange("range")
+.script(new Script("doc['dates'].values")).addUnboundedTo(date(2, 15))
+                                .addRange(date(2, 15), date(3, 15)).addUnboundedFrom(date(3, 15)).subAggregation(min("min"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-2012-02-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Min min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(1, 2).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-02-15T00:00:00.000Z-2012-03-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(2, 2).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-03-15T00:00:00.000Z-*"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 2L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(2, 15).getMillis()));
+    }
+
     public void testUnmapped() throws Exception {
         client().admin().cluster().prepareHealth("idx_unmapped").setWaitForYellowStatus().execute().actionGet();
 
@@ -819,8 +1031,7 @@ public class DateRangeTests extends ESIntegTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(dateRange("date_range").field("value").addRange("0-1", 0, 1)))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(dateRange("date_range").addRange("0-1", 0, 1)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
index ced13b8..749eab9 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
@@ -31,7 +31,6 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
@@ -314,7 +313,8 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
                         .field(SINGLE_VALUED_FIELD_NAME)
-                        .includeExclude(new IncludeExclude(includes, excludes))
+                        .include(includes)
+                        .exclude(excludes)
                         .collectMode(randomFrom(SubAggCollectionMode.values())))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -416,6 +416,34 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (double) i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (double) i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -538,6 +566,43 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
 
     */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(MULTI_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("_value + 1"))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (i + 1d));
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (i+1d)));
+            assertThat(bucket.getKeyAsNumber().doubleValue(), equalTo(i + 1d));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j + 1;
+                    s += j+1 + 1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -562,6 +627,34 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (double) i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (double) i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -590,6 +683,65 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInheritedNoExplicitType() throws Exception {
+        // since no type is explicitly defined, es will assume all values returned by the script to be strings (bytes),
+        // so the aggregation should fail, since the "sum" aggregation can only operation on numeric values.
+
+        try {
+
+            SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                    .addAggregation(terms("terms")
+                            .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                    .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                            .subAggregation(sum("sum")))
+                    .execute().actionGet();
+
+
+            fail("expected to fail as sub-aggregation sum requires a numeric value source context, but there is none");
+
+        } catch (Exception e) {
+            // expected
+        }
+
+    }
+
+    public void testScriptMultiValuedWithAggregatorInheritedWithExplicitType() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                        .valueType(Terms.ValueType.DOUBLE)
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i + ".0");
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i + ".0"));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j;
+                    s += j+1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped").setTypes("type")
                 .addAggregation(terms("terms")
@@ -635,7 +787,7 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(terms("terms").field(SINGLE_VALUED_FIELD_NAME)))
+                        .subAggregation(terms("terms")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -732,7 +884,7 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                 .addAggregation(
                         terms("num_tags").field("num_tag").collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter", asc))
-                                .subAggregation(filter("filter", QueryBuilders.matchAllQuery()))).execute().actionGet();
+                                .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))).execute().actionGet();
 
 
         assertSearchResponse(response);
@@ -772,8 +924,8 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>filter2>max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter("filter2", QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter("filter2").filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         max("max").field(SINGLE_VALUED_FIELD_NAME))))).execute().actionGet();
 
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
index 032e340..3c062f8 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
@@ -35,8 +35,8 @@ import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
+import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -122,7 +122,7 @@ public class EquivalenceTests extends ESIntegTestCase {
             }
         }
 
-        RangeAggregator.RangeAggregatorBuilder query = range("range").field("values");
+        RangeBuilder query = range("range").field("values");
         for (int i = 0; i < ranges.length; ++i) {
             String key = Integer.toString(i);
             if (ranges[i][0] == Double.NEGATIVE_INFINITY) {
@@ -143,7 +143,7 @@ public class EquivalenceTests extends ESIntegTestCase {
             if (ranges[i][1] != Double.POSITIVE_INFINITY){
                 filter = filter.to(ranges[i][1]);
             }
-            reqBuilder = reqBuilder.addAggregation(filter("filter" + i, filter));
+            reqBuilder = reqBuilder.addAggregation(filter("filter" + i).filter(filter));
         }
 
         SearchResponse resp = reqBuilder.execute().actionGet();
@@ -236,16 +236,15 @@ public class EquivalenceTests extends ESIntegTestCase {
 
         assertNoFailures(client().admin().indices().prepareRefresh("idx").setIndicesOptions(IndicesOptions.lenientExpandOpen()).execute().get());
 
-        TermsAggregatorFactory.ExecutionMode[] globalOrdinalModes = new TermsAggregatorFactory.ExecutionMode[] {
-                TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS_HASH, TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS
+        TermsAggregatorFactory.ExecutionMode[] globalOrdinalModes = new TermsAggregatorFactory.ExecutionMode[]{
+                TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS_HASH,
+                TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS
         };
 
         SearchResponse resp = client().prepareSearch("idx")
                 .addAggregation(terms("long").field("long_values").size(maxNumTerms).collectMode(randomFrom(SubAggCollectionMode.values())).subAggregation(min("min").field("num")))
                 .addAggregation(terms("double").field("double_values").size(maxNumTerms).collectMode(randomFrom(SubAggCollectionMode.values())).subAggregation(max("max").field("num")))
-                .addAggregation(terms("string_map").field("string_values").collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .executionHint(TermsAggregatorFactory.ExecutionMode.MAP.toString()).size(maxNumTerms)
-                        .subAggregation(stats("stats").field("num")))
+                .addAggregation(terms("string_map").field("string_values").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(TermsAggregatorFactory.ExecutionMode.MAP.toString()).size(maxNumTerms).subAggregation(stats("stats").field("num")))
                 .addAggregation(terms("string_global_ordinals").field("string_values").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(globalOrdinalModes[randomInt(globalOrdinalModes.length - 1)].toString()).size(maxNumTerms).subAggregation(extendedStats("stats").field("num")))
                 .addAggregation(terms("string_global_ordinals_doc_values").field("string_values.doc_values").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(globalOrdinalModes[randomInt(globalOrdinalModes.length - 1)].toString()).size(maxNumTerms).subAggregation(extendedStats("stats").field("num")))
                 .execute().actionGet();
@@ -353,7 +352,7 @@ public class EquivalenceTests extends ESIntegTestCase {
         indexRandom(true, client().prepareIndex("idx", "type").setSource("f", value));
         ensureYellow("idx"); // only one document let's make sure all shards have an active primary
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("filter", QueryBuilders.matchAllQuery())
+                .addAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
                 .subAggregation(range("range")
                         .field("f")
                         .addUnboundedTo(6)
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java
index 7d018ad..855c13b 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java
@@ -71,8 +71,7 @@ public class ExtendedStatsTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(
-                        histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(extendedStats("stats").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(extendedStats("stats")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
index 4fe11c6..ff4f5d6 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
@@ -77,7 +77,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
             }
         }
         Arrays.sort(percents);
-        Loggers.getLogger(HDRPercentileRanksTests.class).info("Using values={}", Arrays.toString(percents));
+        Loggers.getLogger(HDRPercentileRanksTests.class).info("Using percentiles={}", Arrays.toString(percents));
         return percents;
     }
 
@@ -85,8 +85,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         return randomIntBetween(0, 5);
     }
 
-    private void assertConsistent(double[] pcts, PercentileRanks values, long minValue, long maxValue, int numberSigDigits) {
-        final List<Percentile> percentileList = iterableAsArrayList(values);
+    private void assertConsistent(double[] pcts, PercentileRanks percentiles, long minValue, long maxValue, int numberSigDigits) {
+        final List<Percentile> percentileList = iterableAsArrayList(percentiles);
         assertEquals(pcts.length, percentileList.size());
         for (int i = 0; i < pcts.length; ++i) {
             final Percentile percentile = percentileList.get(i);
@@ -121,9 +121,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                                 .interval(1L)
                                 .minDocCount(0)
                                 .subAggregation(
-                                        percentileRanks("percentile_ranks").field("value").method(PercentilesMethod.HDR)
-                                        .numberOfSignificantValueDigits(sigDigits).values(10, 15)))
-                .execute().actionGet();
+                                        percentileRanks("percentile_ranks").method(PercentilesMethod.HDR)
+                                                .numberOfSignificantValueDigits(sigDigits).percentiles(10, 15))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
         Histogram histo = searchResponse.getAggregations().get("histo");
@@ -146,8 +145,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(0, 10, 15, 100))
-                .execute().actionGet();
+                                .field("value").percentiles(0, 10, 15, 100)).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(0L));
 
@@ -169,13 +167,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -188,8 +185,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .addAggregation(
                         global("global").subAggregation(
                                 percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                                .field("value").values(pcts)))
-                .execute().actionGet();
+                                        .field("value").percentiles(pcts))).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
@@ -200,10 +196,10 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         assertThat(global.getAggregations(), notNullValue());
         assertThat(global.getAggregations().asMap().size(), equalTo(1));
 
-        PercentileRanks values = global.getAggregations().get("percentile_ranks");
-        assertThat(values, notNullValue());
-        assertThat(values.getName(), equalTo("percentile_ranks"));
-        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(values));
+        PercentileRanks percentiles = global.getAggregations().get("percentile_ranks");
+        assertThat(percentiles, notNullValue());
+        assertThat(percentiles.getName(), equalTo("percentile_ranks"));
+        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(percentiles));
 
     }
 
@@ -215,13 +211,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -233,13 +228,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -251,13 +245,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").script(new Script("_value - 1")).values(pcts))
-                .execute().actionGet();
+                                .field("value").script(new Script("_value - 1")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -271,13 +264,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").script(new Script("_value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .field("value").script(new Script("_value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -289,13 +282,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").values(pcts))
-                .execute().actionGet();
+                                .field("values").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues, sigDigits);
     }
 
     @Override
@@ -307,13 +299,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("_value - 1")).values(pcts))
-                .execute().actionGet();
+                                .field("values").script(new Script("_value - 1")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     public void testMultiValuedFieldWithValueScriptReverse() throws Exception {
@@ -324,13 +315,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("20 - _value")).values(pcts))
-                .execute().actionGet();
+                                .field("values").script(new Script("20 - _value")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, 20 - maxValues, 20 - minValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, 20 - maxValues, 20 - minValues, sigDigits);
     }
 
     @Override
@@ -344,13 +334,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("_value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .field("values").script(new Script("_value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     @Override
@@ -362,13 +352,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['value'].value")).values(pcts))
-                .execute().actionGet();
+                                .script(new Script("doc['value'].value")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -382,13 +371,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['value'].value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .script(new Script("doc['value'].value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -400,13 +389,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['values'].values")).values(pcts))
-                .execute().actionGet();
+                                .script(new Script("doc['values'].values")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues, sigDigits);
     }
 
     @Override
@@ -424,14 +412,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                                 .numberOfSignificantValueDigits(sigDigits)
                                 .script(new Script(
                                         "List values = doc['values'].values; double[] res = new double[values.size()]; for (int i = 0; i < res.length; i++) { res[i] = values.get(i) - dec; }; return res;",
-                                ScriptType.INLINE, null, params))
-                        .values(pcts))
-                .execute().actionGet();
+                                        ScriptType.INLINE, null, params)).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     public void testOrderBySubAggregation() {
@@ -443,8 +429,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
                                 .subAggregation(
-                                        percentileRanks("percentile_ranks").field("value").method(PercentilesMethod.HDR)
-                                .numberOfSignificantValueDigits(sigDigits).values(99))
+                                        percentileRanks("percentile_ranks").method(PercentilesMethod.HDR)
+                                                .numberOfSignificantValueDigits(sigDigits).percentiles(99))
                                 .order(Order.aggregation("percentile_ranks", "99", asc))).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
@@ -452,8 +438,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         Histogram histo = searchResponse.getAggregations().get("histo");
         double previous = asc ? Double.NEGATIVE_INFINITY : Double.POSITIVE_INFINITY;
         for (Histogram.Bucket bucket : histo.getBuckets()) {
-            PercentileRanks values = bucket.getAggregations().get("percentile_ranks");
-            double p99 = values.percent(99);
+            PercentileRanks percentiles = bucket.getAggregations().get("percentile_ranks");
+            double p99 = percentiles.percent(99);
             if (asc) {
                 assertThat(p99, greaterThanOrEqualTo(previous));
             } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java
index aec3c43..8f9370e 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java
@@ -122,9 +122,7 @@ public class HDRPercentilesTests extends AbstractNumericTestCase {
                                 .interval(1L)
                                 .minDocCount(0)
                                 .subAggregation(
-                                        percentiles("percentiles").field("value")
-                                                .numberOfSignificantValueDigits(sigDigits)
-                                                .method(PercentilesMethod.HDR)
+                                        percentiles("percentiles").numberOfSignificantValueDigits(sigDigits).method(PercentilesMethod.HDR)
                                                 .percentiles(10,
                                                 15))).execute().actionGet();
 
@@ -421,9 +419,7 @@ public class HDRPercentilesTests extends AbstractNumericTestCase {
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
                                 .subAggregation(
-                                        percentiles("percentiles").field("value")
-                                                .method(PercentilesMethod.HDR)
-                                                .numberOfSignificantValueDigits(sigDigits)
+                                        percentiles("percentiles").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
                                                 .percentiles(99))
                                 .order(Order.aggregation("percentiles", "99", asc))).execute().actionGet();
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
index 5ada804..87bcc22 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
@@ -20,14 +20,16 @@ package org.elasticsearch.messy.tests;
 
 import com.carrotsearch.hppc.LongHashSet;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -37,6 +39,7 @@ import org.hamcrest.Matchers;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -46,6 +49,7 @@ import static org.elasticsearch.search.aggregations.AggregationBuilders.histogra
 import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.stats;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.containsString;
@@ -354,6 +358,40 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(numValueBuckets));
+
+        // TODO: use diamond once JI-9019884 is fixed
+        List<Histogram.Bucket> buckets = new ArrayList<Histogram.Bucket>(histo.getBuckets());
+        for (int i = 0; i < numValueBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval));
+            assertThat(bucket.getDocCount(), equalTo(valueCounts[i]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == i) {
+                    s += j + 1;
+                }
+            }
+            assertThat(sum.getValue(), equalTo((double) s));
+        }
+    }
+
     public void testSingleValuedFieldOrderedBySubAggregationAsc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("sum", true))
@@ -434,6 +472,46 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testSingleValuedFieldOrderedByMultiValuedSubAggregationAscInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("stats.sum", true))
+                        .subAggregation(stats("stats")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(numValueBuckets));
+
+        LongHashSet visited = new LongHashSet();
+        double previousSum = Double.NEGATIVE_INFINITY;
+        // TODO: use diamond once JI-9019884 is fixed
+        List<Histogram.Bucket> buckets = new ArrayList<Histogram.Bucket>(histo.getBuckets());
+        for (int i = 0; i < numValueBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            long key = ((Number) bucket.getKey()).longValue();
+            assertTrue(visited.add(key));
+            int b = (int) (key / interval);
+            assertThat(bucket.getDocCount(), equalTo(valueCounts[b]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Stats stats = bucket.getAggregations().get("stats");
+            assertThat(stats, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == b) {
+                    s += j + 1;
+                }
+            }
+            assertThat(stats.getSum(), equalTo((double) s));
+            assertThat(stats.getSum(), greaterThanOrEqualTo(previousSum));
+            previousSum = s;
+        }
+    }
+
     public void testSingleValuedFieldOrderedByMultiValuedSubAggregationDesc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("stats.sum", false))
@@ -478,7 +556,7 @@ public class HistogramTests extends ESIntegTestCase {
         boolean asc = randomBoolean();
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("filter>max", asc))
-                        .subAggregation(filter("filter", matchAllQuery())
+                        .subAggregation(filter("filter").filter(matchAllQuery())
                                 .subAggregation(max("max").field(SINGLE_VALUED_FIELD_NAME))))
                 .execute().actionGet();
 
@@ -618,6 +696,55 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(MULTI_VALUED_FIELD_NAME)
+                                .script(new Script("_value + 1"))
+                                .interval(interval)
+                                .subAggregation(
+                                        terms(MULTI_VALUED_FIELD_NAME).collectMode(randomFrom(SubAggCollectionMode.values())).order(
+                                                Terms.Order.term(true)))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        final int numBuckets = (numDocs + 2) / interval - 2 / interval + 1;
+        final long[] counts = new long[(numDocs + 2) / interval + 1];
+        for (int i = 0; i < numDocs; ++i) {
+            final int bucket1 = (i + 2) / interval;
+            final int bucket2 = (i + 3) / interval;
+            ++counts[bucket1];
+            if (bucket1 != bucket2) {
+                ++counts[bucket2];
+            }
+        }
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(numBuckets));
+
+        for (int i = 0; i < numBuckets; i++) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            int key = ((2 / interval) + i) * interval;
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) key));
+            assertThat(bucket.getDocCount(), equalTo(counts[key / interval]));
+            Terms terms = bucket.getAggregations().get(MULTI_VALUED_FIELD_NAME);
+            assertThat(terms, notNullValue());
+            assertThat(terms.getName(), equalTo(MULTI_VALUED_FIELD_NAME));
+            int minTerm = Math.max(2, key - 1);
+            int maxTerm = Math.min(numDocs + 2, (key / interval + 1) * interval);
+            assertThat(terms.getBuckets().size(), equalTo(maxTerm - minTerm + 1));
+            Iterator<Terms.Bucket> iter = terms.getBuckets().iterator();
+            for (int j = minTerm; j <= maxTerm; ++j) {
+                assertThat(iter.next().getKeyAsNumber().longValue(), equalTo((long) j));
+            }
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).interval(interval))
@@ -639,6 +766,40 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo").script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).interval(interval)
+                                .subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(numValueBuckets));
+
+        // TODO: use diamond once JI-9019884 is fixed
+        List<Histogram.Bucket> buckets = new ArrayList<Histogram.Bucket>(histo.getBuckets());
+        for (int i = 0; i < numValueBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval));
+            assertThat(bucket.getDocCount(), equalTo(valueCounts[i]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == i) {
+                    s += j + 1;
+                }
+            }
+            assertThat(sum.getValue(), equalTo((double) s));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']")).interval(interval))
@@ -660,6 +821,40 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo").script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']")).interval(interval)
+                                .subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(numValuesBuckets));
+
+        for (int i = 0; i < numValuesBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval));
+            assertThat(bucket.getDocCount(), equalTo(valuesCounts[i]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == i || (j + 2) / interval == i) {
+                    s += j + 1;
+                    s += j + 2;
+                }
+            }
+            assertThat(sum.getValue(), equalTo((double) s));
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval))
@@ -701,7 +896,7 @@ public class HistogramTests extends ESIntegTestCase {
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) -1 * 2 * interval, (long) valueCounts.length * interval))).execute().actionGet();
+                                .extendedBounds((long) -1 * 2 * interval, (long) valueCounts.length * interval)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -733,7 +928,7 @@ public class HistogramTests extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(histogram("sub_histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L)))
+                        .subAggregation(histogram("sub_histo").interval(1L)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -791,7 +986,7 @@ public class HistogramTests extends ESIntegTestCase {
                             .field(SINGLE_VALUED_FIELD_NAME)
                             .interval(interval)
                             .minDocCount(0)
-                            .extendedBounds(new ExtendedBounds(boundsMin, boundsMax)))
+                            .extendedBounds(boundsMin, boundsMax))
                     .execute().actionGet();
 
             if (invalidBoundsError) {
@@ -833,8 +1028,8 @@ public class HistogramTests extends ESIntegTestCase {
             client().prepareSearch("empty_bucket_idx")
                     .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(-1).minDocCount(0)).execute().actionGet();
             fail();
-        } catch (IllegalArgumentException e) {
-            assertThat(e.toString(), containsString("[interval] must be 1 or greater for histogram aggregation [histo]"));
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("Missing required field [interval]"));
         }
     }
 }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java
index 1fb16f6..b1861b5 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java
@@ -27,6 +27,7 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
+import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
@@ -40,6 +41,7 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.ipRange;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
@@ -315,6 +317,62 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat((double) propertiesCounts[2], equalTo((double) 55 * 3));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(ipRange("range")
+                        .field("ip")
+                        .addUnboundedTo("10.0.0.100")
+                        .addRange("10.0.0.100", "10.0.0.200")
+                        .addUnboundedFrom("10.0.0.200")
+                        .subAggregation(max("max")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.99")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.199")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(55L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.254")));
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -465,6 +523,59 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(56L));
     }
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        ipRange("range").field("ips").script(new Script("_value")).addUnboundedTo("10.0.0.100")
+                                .addRange("10.0.0.100", "10.0.0.200").addUnboundedFrom("10.0.0.200").subAggregation(max("max"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.100")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(101L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.200")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(56L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.255")));
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -509,6 +620,60 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(55L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        ipRange("range").script(new Script("doc['ip'].value")).addUnboundedTo("10.0.0.100")
+                                .addRange("10.0.0.100", "10.0.0.200").addUnboundedFrom("10.0.0.200").subAggregation(max("max"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.99")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.199")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(55L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.254")));
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -553,6 +718,60 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(56L));
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        ipRange("range").script(new Script("doc['ips'].values")).addUnboundedTo("10.0.0.100")
+                                .addRange("10.0.0.100", "10.0.0.200").addUnboundedFrom("10.0.0.200").subAggregation(max("max"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.100")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(101L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.200")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(56L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.255")));
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .addAggregation(ipRange("range")
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
index 65f2881..fcff4b9 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
@@ -30,7 +30,6 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
@@ -290,7 +289,8 @@ public class LongTermsTests extends AbstractTermsTestCase {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
                         .field(SINGLE_VALUED_FIELD_NAME)
-                        .includeExclude(new IncludeExclude(includes, excludes))
+                        .include(includes)
+                        .exclude(excludes)
                         .collectMode(randomFrom(SubAggCollectionMode.values())))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -416,6 +416,34 @@ public class LongTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -538,6 +566,43 @@ public class LongTermsTests extends AbstractTermsTestCase {
 
     */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(MULTI_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("_value + 1"))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (i + 1d));
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (i+1d)));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i+1));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j + 1;
+                    s += j+1 + 1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -563,6 +628,34 @@ public class LongTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -592,6 +685,59 @@ public class LongTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInheritedNoExplicitType() throws Exception {
+        // since no type ie explicitly defined, es will assume all values returned by the script to be strings (bytes),
+        // so the aggregation should fail, since the "sum" aggregation can only operation on numeric values.
+        try {
+            client().prepareSearch("idx").setTypes("type")
+                    .addAggregation(terms("terms")
+                            .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                    .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                            .subAggregation(sum("sum")))
+                    .execute().actionGet();
+            fail("expected to fail as sub-aggregation sum requires a numeric value source context, but there is none");
+        } catch (Exception e) {
+            // expected
+        }
+    }
+
+    public void testScriptMultiValuedWithAggregatorInheritedWithExplicitType() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                        .valueType(Terms.ValueType.LONG)
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j;
+                    s += j+1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped").setTypes("type")
                 .addAggregation(terms("terms")
@@ -637,7 +783,7 @@ public class LongTermsTests extends AbstractTermsTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(terms("terms").field(SINGLE_VALUED_FIELD_NAME)))
+                        .subAggregation(terms("terms")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -730,7 +876,7 @@ public class LongTermsTests extends AbstractTermsTestCase {
                         .field("num_tag")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                         .order(Terms.Order.aggregation("filter", asc))
-.subAggregation(filter("filter", QueryBuilders.matchAllQuery()))
+                        .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))
                 ).execute().actionGet();
 
 
@@ -767,8 +913,8 @@ public class LongTermsTests extends AbstractTermsTestCase {
                         .field("num_tag")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                         .order(Terms.Order.aggregation("filter1>filter2>max", asc))
-                .subAggregation(filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                        filter("filter2", QueryBuilders.matchAllQuery())
+                        .subAggregation(filter("filter1").filter(QueryBuilders.matchAllQuery())
+                                .subAggregation(filter("filter2").filter(QueryBuilders.matchAllQuery())
                                         .subAggregation(max("max").field(SINGLE_VALUED_FIELD_NAME))))
                 ).execute().actionGet();
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java
index ee20202..d2e0d20 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java
@@ -54,7 +54,7 @@ public class MaxTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(max("max").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(max("max")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
index 9a92a61..9b3e1a3 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
@@ -35,8 +35,7 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -111,17 +110,17 @@ public class MinDocCountTests extends AbstractTermsTestCase {
     private enum Script {
         NO {
             @Override
-            TermsAggregatorBuilder apply(TermsAggregatorBuilder builder, String field) {
+            TermsBuilder apply(TermsBuilder builder, String field) {
                 return builder.field(field);
             }
         },
         YES {
             @Override
-            TermsAggregatorBuilder apply(TermsAggregatorBuilder builder, String field) {
+            TermsBuilder apply(TermsBuilder builder, String field) {
                 return builder.script(new org.elasticsearch.script.Script("doc['" + field + "'].values"));
             }
         };
-        abstract TermsAggregatorBuilder apply(TermsAggregatorBuilder builder, String field);
+        abstract TermsBuilder apply(TermsBuilder builder, String field);
     }
 
     // check that terms2 is a subset of terms1
@@ -298,7 +297,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
                             .executionHint(randomExecutionHint())
                             .order(order)
                             .size(size)
-                            .includeExclude(include == null ? null : new IncludeExclude(include, null))
+                            .include(include)
                             .shardSize(cardinality + randomInt(10))
                             .minDocCount(minDocCount)).request();
             final SearchResponse response = client().search(request).get();
@@ -378,7 +377,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
         final SearchResponse allResponse = client().prepareSearch("idx").setTypes("type")
                 .setSize(0)
                 .setQuery(QUERY)
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).order(order).minDocCount(0))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).order(order).minDocCount(0))
                 .execute().actionGet();
 
         final Histogram allHisto = allResponse.getAggregations().get("histo");
@@ -387,7 +386,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
             final SearchResponse response = client().prepareSearch("idx").setTypes("type")
                     .setSize(0)
                     .setQuery(QUERY)
-                    .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).order(order).minDocCount(minDocCount))
+                    .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).order(order).minDocCount(minDocCount))
                     .execute().actionGet();
             assertSubset(allHisto, (Histogram) response.getAggregations().get("histo"), minDocCount);
         }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java
index 36bf5c2..caa7b2f 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java
@@ -54,7 +54,7 @@ public class MinTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(min("min").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(min("min")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
index 9955d24..725751f 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
@@ -28,6 +28,7 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
@@ -39,9 +40,10 @@ import java.util.List;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.avg;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.range;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
@@ -361,6 +363,66 @@ public class RangeTests extends ESIntegTestCase {
         assertThat((double) propertiesCounts[2], equalTo((double) total));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(range("range")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .addUnboundedTo(3)
+                        .addRange(3, 6)
+                        .addUnboundedFrom(6)
+                        .subAggregation(avg("avg")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-3.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Avg avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(1.5)); // (1 + 2) / 2
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("3.0-6.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(4.0)); // (3 + 4 + 5) / 3
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("6.0-*"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 5L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        long total = 0;
+        for (int i = 5; i < numDocs; ++i) {
+            total += i + 1;
+        }
+        assertThat(avg.getValue(), equalTo((double) total / (numDocs - 5))); // (6 + 7 + 8 + 9 + 10) / 5
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -537,6 +599,66 @@ public class RangeTests extends ESIntegTestCase {
     r3: 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12
      */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        range("range").field(MULTI_VALUED_FIELD_NAME).script(new Script("_value + 1")).addUnboundedTo(3).addRange(3, 6)
+                                .addUnboundedFrom(6).subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-3.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Sum sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo(2d+3d));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("3.0-6.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(4L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo((double) 2+3+3+4+4+5+5+6));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("6.0-*"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 3L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        long total = 0;
+        for (int i = 3; i < numDocs; ++i) {
+            total += ((i + 1) + 1) + ((i + 1) + 2);
+        }
+        assertThat(sum.getValue(), equalTo((double) total));
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -581,6 +703,63 @@ public class RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(numDocs - 5L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        range("range").script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).addUnboundedTo(3).addRange(3, 6)
+                                .addUnboundedFrom(6).subAggregation(avg("avg"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-3.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Avg avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(1.5)); // (1 + 2) / 2
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("3.0-6.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(4.0)); // (3 + 4 + 5) / 3
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("6.0-*"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 5L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        long total = 0;
+        for (int i = 5; i < numDocs; ++i) {
+            total += i + 1;
+        }
+        assertThat(avg.getValue(), equalTo((double) total / (numDocs - 5))); // (6 + 7 + 8 + 9 + 10) / 5
+    }
+
     public void testEmptyRange() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(range("range")
@@ -678,6 +857,66 @@ public class RangeTests extends ESIntegTestCase {
     r3: 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11
      */
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        range("range").script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "'].values")).addUnboundedTo("r1", 3)
+                                .addRange("r2", 3, 6).addUnboundedFrom("r3", 6).subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r1"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Sum sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo((double) 1+2+2+3));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r2"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(4L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo((double) 2+3+3+4+4+5+5+6));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r3"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        long total = 0;
+        for (int i = 4; i < numDocs; ++i) {
+            total += (i + 1) + (i + 2);
+        }
+        assertThat(sum.getValue(), equalTo((double) total));
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .addAggregation(range("range")
@@ -832,7 +1071,7 @@ public class RangeTests extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(range("range").field(SINGLE_VALUED_FIELD_NAME).addRange("0-2", 0.0, 2.0)))
+                        .subAggregation(range("range").addRange("0-2", 0.0, 2.0)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java
index 4934d2a..2f53d5c 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java
@@ -57,7 +57,7 @@ public class StatsTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(stats("stats").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(stats("stats")))
                 .execute().actionGet();
 
         assertShardExecutionState(searchResponse, 0);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
index edf8be4..1f43794 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.messy.tests;
 
-import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
@@ -37,7 +36,6 @@ import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
@@ -275,8 +273,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude("val00.+", null)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include("val00.+")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -300,8 +297,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .includeExclude(new IncludeExclude("val00.+", "(val000|val001)")))
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include("val00.+").exclude("(val000|val001)"))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -326,9 +322,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .includeExclude(new IncludeExclude(null, new RegExp("val0[1-9]+.+"))))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).exclude("val0[1-9]+.+")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -353,8 +347,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(incVals, null)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include(incVals)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -381,8 +374,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(incVals, excVals)))
-                .execute()
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include(incVals).exclude(excVals)).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -405,8 +397,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(null, excVals)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).exclude(excVals)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -536,6 +527,33 @@ public class StringTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregation_Inherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).subAggregation(count("count"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            ValueCount valueCount = bucket.getAggregations().get("count");
+            assertThat(valueCount, notNullValue());
+            assertThat(valueCount.getValue(), equalTo(1L));
+        }
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -676,6 +694,40 @@ public class StringTermsTests extends AbstractTermsTestCase {
      * doc_count: 1 - val_count: 2
      */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").executionHint(randomExecutionHint()).field(MULTI_VALUED_FIELD_NAME)
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).script(new Script("'foo_' + _value"))
+                                .subAggregation(count("count"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("foo_val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("foo_val" + i));
+            if (i == 0 | i == 5) {
+                assertThat(bucket.getDocCount(), equalTo(1L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat(valueCount.getValue(), equalTo(2L));
+            } else {
+                assertThat(bucket.getDocCount(), equalTo(2L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat("term[" + key(bucket) + "]", valueCount.getValue(), equalTo(4L));
+            }
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -722,6 +774,33 @@ public class StringTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(randomExecutionHint())
+                                .script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).subAggregation(count("count")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            ValueCount valueCount = bucket.getAggregations().get("count");
+            assertThat(valueCount, notNullValue());
+            assertThat(valueCount.getValue(), equalTo(1L));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -749,6 +828,40 @@ public class StringTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(randomExecutionHint())
+                                .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']")).subAggregation(count("count"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("val" + i));
+            if (i == 0 | i == 5) {
+                assertThat(bucket.getDocCount(), equalTo(1L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat(valueCount.getValue(), equalTo(2L));
+            } else {
+                assertThat(bucket.getDocCount(), equalTo(2L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat(valueCount.getValue(), equalTo(4L));
+            }
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx_unmapped")
@@ -794,7 +907,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .prepareSearch("idx")
                 .setTypes("type")
                 .addAggregation(
-                        filter("filter", termQuery(MULTI_VALUED_FIELD_NAME, "val3")).subAggregation(
+                        filter("filter").filter(termQuery(MULTI_VALUED_FIELD_NAME, "val3")).subAggregation(
                                 terms("terms").field(MULTI_VALUED_FIELD_NAME).collectMode(randomFrom(SubAggCollectionMode.values()))))
                 .execute().actionGet();
 
@@ -820,7 +933,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(
-                        histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0).subAggregation(terms("terms").field("value")))
+                        histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0).subAggregation(terms("terms")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -901,7 +1014,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .addAggregation(
                         terms("tags").executionHint(randomExecutionHint()).field("tag")
                                 .collectMode(randomFrom(SubAggCollectionMode.values())).order(Terms.Order.aggregation("filter", asc))
-                                .subAggregation(filter("filter", QueryBuilders.matchAllQuery()))).execute().actionGet();
+                                .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -941,8 +1054,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>filter2>stats.max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter("filter2", QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter("filter2").filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats("stats").field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1004,8 +1117,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>" + filter2Name + ">" + statsName + ".max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter(filter2Name, QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter(filter2Name).filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats(statsName).field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1067,8 +1180,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>" + filter2Name + ">" + statsName + "[max]", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter(filter2Name, QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter(filter2Name).filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats(statsName).field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
index 540d0cf..0a2d103 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
@@ -31,7 +31,7 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
 import org.elasticsearch.search.aggregations.metrics.AbstractNumericTestCase;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 
 import java.util.Arrays;
 import java.util.Collection;
@@ -78,19 +78,19 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
             }
         }
         Arrays.sort(percents);
-        Loggers.getLogger(TDigestPercentileRanksTests.class).info("Using values={}", Arrays.toString(percents));
+        Loggers.getLogger(TDigestPercentileRanksTests.class).info("Using percentiles={}", Arrays.toString(percents));
         return percents;
     }
 
-    private static PercentileRanksAggregatorBuilder randomCompression(PercentileRanksAggregatorBuilder builder) {
+    private static PercentileRanksBuilder randomCompression(PercentileRanksBuilder builder) {
         if (randomBoolean()) {
             builder.compression(randomIntBetween(20, 120) + randomDouble());
         }
         return builder;
     }
 
-    private void assertConsistent(double[] pcts, PercentileRanks values, long minValue, long maxValue) {
-        final List<Percentile> percentileList = CollectionUtils.iterableAsArrayList(values);
+    private void assertConsistent(double[] pcts, PercentileRanks percentiles, long minValue, long maxValue) {
+        final List<Percentile> percentileList = CollectionUtils.iterableAsArrayList(percentiles);
         assertEquals(pcts.length, percentileList.size());
         for (int i = 0; i < pcts.length; ++i) {
             final Percentile percentile = percentileList.get(i);
@@ -116,8 +116,8 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(randomCompression(percentileRanks("percentile_ranks").field("value"))
-                                .values(10, 15)))
+                        .subAggregation(randomCompression(percentileRanks("percentile_ranks"))
+                                .percentiles(10, 15)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -139,7 +139,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(0, 10, 15, 100))
+                        .percentiles(0, 10, 15, 100))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(0L));
@@ -160,13 +160,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -177,7 +177,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         global("global").subAggregation(
-                                randomCompression(percentileRanks("percentile_ranks")).field("value").values(pcts))).execute()
+                                randomCompression(percentileRanks("percentile_ranks")).field("value").percentiles(pcts))).execute()
                 .actionGet();
 
         assertHitCount(searchResponse, 10);
@@ -189,10 +189,10 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         assertThat(global.getAggregations(), notNullValue());
         assertThat(global.getAggregations().asMap().size(), equalTo(1));
 
-        PercentileRanks values = global.getAggregations().get("percentile_ranks");
-        assertThat(values, notNullValue());
-        assertThat(values.getName(), equalTo("percentile_ranks"));
-        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(values));
+        PercentileRanks percentiles = global.getAggregations().get("percentile_ranks");
+        assertThat(percentiles, notNullValue());
+        assertThat(percentiles.getName(), equalTo("percentile_ranks"));
+        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(percentiles));
 
     }
 
@@ -202,13 +202,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -218,13 +218,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -233,14 +233,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("value").script(new Script("_value - 1"))
-                        .values(pcts))
+.field("value").script(new Script("_value - 1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -251,15 +251,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("value")
+.field("value")
                                 .script(new Script("_value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -269,13 +269,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("values")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues);
     }
 
     @Override
@@ -284,14 +284,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values").script(new Script("_value - 1"))
-                        .values(pcts))
+.field("values").script(new Script("_value - 1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     public void testMultiValuedFieldWithValueScriptReverse() throws Exception {
@@ -299,14 +299,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values").script(new Script("_value * -1"))
-                        .values(pcts))
+.field("values").script(new Script("_value * -1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, -maxValues, -minValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, -maxValues, -minValues);
     }
 
     @Override
@@ -317,15 +317,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values")
+.field("values")
                                 .script(new Script("_value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     @Override
@@ -334,14 +334,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(new Script("doc['value'].value"))
-                        .values(pcts))
+.script(new Script("doc['value'].value"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -352,15 +352,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(
+.script(
                                 new Script("doc['value'].value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -369,14 +369,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(new Script("doc['values'].values"))
-                        .values(pcts))
+.script(new Script("doc['values'].values"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues);
     }
 
     @Override
@@ -390,13 +390,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                                 .script(new Script(
                                         "List values = doc['values'].values; double[] res = new double[values.size()]; for (int i = 0; i < res.length; i++) { res[i] = values.get(i) - dec; }; return res;",
                                         ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     public void testOrderBySubAggregation() {
@@ -405,7 +405,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
-                            .subAggregation(randomCompression(percentileRanks("percentile_ranks").field("value").values(99)))
+                            .subAggregation(randomCompression(percentileRanks("percentile_ranks").percentiles(99)))
                             .order(Order.aggregation("percentile_ranks", "99", asc)))
                 .execute().actionGet();
 
@@ -414,8 +414,8 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         Histogram histo = searchResponse.getAggregations().get("histo");
         double previous = asc ? Double.NEGATIVE_INFINITY : Double.POSITIVE_INFINITY;
         for (Histogram.Bucket bucket : histo.getBuckets()) {
-            PercentileRanks values = bucket.getAggregations().get("percentile_ranks");
-            double p99 = values.percent(99);
+            PercentileRanks percentiles = bucket.getAggregations().get("percentile_ranks");
+            double p99 = percentiles.percent(99);
             if (asc) {
                 assertThat(p99, greaterThanOrEqualTo(previous));
             } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
index 1bf0b00..0fa5f66 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
@@ -31,7 +31,8 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
 import org.elasticsearch.search.aggregations.metrics.AbstractNumericTestCase;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
+
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
@@ -80,7 +81,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
         return percentiles;
     }
 
-    private static PercentilesAggregatorBuilder randomCompression(PercentilesAggregatorBuilder builder) {
+    private static PercentilesBuilder randomCompression(PercentilesBuilder builder) {
         if (randomBoolean()) {
             builder.compression(randomIntBetween(20, 120) + randomDouble());
         }
@@ -115,7 +116,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(randomCompression(percentiles("percentiles").field("value"))
+                        .subAggregation(randomCompression(percentiles("percentiles"))
                                 .percentiles(10, 15)))
                 .execute().actionGet();
 
@@ -388,7 +389,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
-                        .subAggregation(randomCompression(percentiles("percentiles").field("value").percentiles(99)))
+                            .subAggregation(randomCompression(percentiles("percentiles").percentiles(99)))
                             .order(Order.aggregation("percentiles", "99", asc)))
                 .execute().actionGet();
 
diff --git a/plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/AttachmentProcessor.java b/plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/AttachmentProcessor.java
index cb9ef78..f7edb46 100644
--- a/plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/AttachmentProcessor.java
+++ b/plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/AttachmentProcessor.java
@@ -32,6 +32,7 @@ import org.elasticsearch.ingest.core.IngestDocument;
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.EnumSet;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
@@ -66,66 +67,69 @@ public final class AttachmentProcessor extends AbstractProcessor {
     @Override
     public void execute(IngestDocument ingestDocument) {
         String base64Input = ingestDocument.getFieldValue(sourceField, String.class);
+        Map<String, Object> additionalFields = new HashMap<>();
 
-        Metadata metadata = new Metadata();
         try {
             byte[] decodedContent = Base64.decode(base64Input.getBytes(UTF_8));
+            Metadata metadata = new Metadata();
             String parsedContent = TikaImpl.parse(decodedContent, metadata, indexedChars);
 
             if (fields.contains(Field.CONTENT) && Strings.hasLength(parsedContent)) {
                 // somehow tika seems to append a newline at the end automatically, lets remove that again
-                ingestDocument.setFieldValue(targetField + "." + Field.CONTENT.toLowerCase(), parsedContent.trim());
+                additionalFields.put(Field.CONTENT.toLowerCase(), parsedContent.trim());
             }
 
             if (fields.contains(Field.LANGUAGE) && Strings.hasLength(parsedContent)) {
                 LanguageIdentifier identifier = new LanguageIdentifier(parsedContent);
                 String language = identifier.getLanguage();
-                ingestDocument.setFieldValue(targetField + "." + Field.LANGUAGE.toLowerCase(), language);
+                additionalFields.put(Field.LANGUAGE.toLowerCase(), language);
             }
 
             if (fields.contains(Field.DATE)) {
                 String createdDate = metadata.get(TikaCoreProperties.CREATED);
                 if (createdDate != null) {
-                    ingestDocument.setFieldValue(targetField + "." + Field.DATE.toLowerCase(), createdDate);
+                    additionalFields.put(Field.DATE.toLowerCase(), createdDate);
                 }
             }
 
             if (fields.contains(Field.TITLE)) {
                 String title = metadata.get(TikaCoreProperties.TITLE);
                 if (Strings.hasLength(title)) {
-                    ingestDocument.setFieldValue(targetField + "." + Field.TITLE.toLowerCase(), title);
+                    additionalFields.put(Field.TITLE.toLowerCase(), title);
                 }
             }
 
             if (fields.contains(Field.AUTHOR)) {
                 String author = metadata.get("Author");
                 if (Strings.hasLength(author)) {
-                    ingestDocument.setFieldValue(targetField + "." + Field.AUTHOR.toLowerCase(), author);
+                    additionalFields.put(Field.AUTHOR.toLowerCase(), author);
                 }
             }
 
             if (fields.contains(Field.KEYWORDS)) {
                 String keywords = metadata.get("Keywords");
                 if (Strings.hasLength(keywords)) {
-                    ingestDocument.setFieldValue(targetField + "." + Field.KEYWORDS.toLowerCase(), keywords);
+                    additionalFields.put(Field.KEYWORDS.toLowerCase(), keywords);
                 }
             }
 
             if (fields.contains(Field.CONTENT_TYPE)) {
                 String contentType = metadata.get(Metadata.CONTENT_TYPE);
                 if (Strings.hasLength(contentType)) {
-                    ingestDocument.setFieldValue(targetField + "." + Field.CONTENT_TYPE.toLowerCase(), contentType);
+                    additionalFields.put(Field.CONTENT_TYPE.toLowerCase(), contentType);
                 }
             }
 
             if (fields.contains(Field.CONTENT_LENGTH)) {
                 String contentLength = metadata.get(Metadata.CONTENT_LENGTH);
                 String length = Strings.hasLength(contentLength) ? contentLength : String.valueOf(parsedContent.length());
-                ingestDocument.setFieldValue(targetField + "." + Field.CONTENT_LENGTH.toLowerCase(), length);
+                additionalFields.put(Field.CONTENT_LENGTH.toLowerCase(), length);
             }
         } catch (Throwable e) {
             throw new ElasticsearchParseException("Error parsing document in field [{}]", e, sourceField);
         }
+
+        ingestDocument.setFieldValue(targetField, additionalFields);
     }
 
     @Override
@@ -183,7 +187,6 @@ public final class AttachmentProcessor extends AbstractProcessor {
 
         CONTENT,
         TITLE,
-        NAME,
         AUTHOR,
         KEYWORDS,
         DATE,
diff --git a/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorFactoryTests.java b/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorFactoryTests.java
index 469a0f8..7d48b21 100644
--- a/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorFactoryTests.java
+++ b/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorFactoryTests.java
@@ -32,6 +32,7 @@ import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 
+import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.sameInstance;
 import static org.hamcrest.core.Is.is;
@@ -101,8 +102,11 @@ public class AttachmentProcessorFactoryTests extends ESTestCase {
             factory.create(config);
             fail("exception expected");
         } catch (ElasticsearchParseException e) {
-            assertThat(e.getMessage(), equalTo("[fields] illegal field option [invalid]. valid values are " +
-                "[CONTENT, TITLE, NAME, AUTHOR, KEYWORDS, DATE, CONTENT_TYPE, CONTENT_LENGTH, LANGUAGE]"));
+            assertThat(e.getMessage(), containsString("[fields] illegal field option [invalid]"));
+            // ensure allowed fields are mentioned
+            for (AttachmentProcessor.Field field : AttachmentProcessor.Field.values()) {
+                assertThat(e.getMessage(), containsString(field.name()));
+            }
         }
 
         config = new HashMap<>();
diff --git a/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorTests.java b/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorTests.java
index d9c96de..5470638e 100644
--- a/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorTests.java
+++ b/plugins/ingest-attachment/src/test/java/org/elasticsearch/ingest/attachment/AttachmentProcessorTests.java
@@ -27,18 +27,15 @@ import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Before;
 
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
 import java.util.ArrayList;
 import java.util.EnumSet;
 import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 
-import static org.hamcrest.Matchers.contains;
 import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.hasSize;
@@ -68,22 +65,31 @@ public class AttachmentProcessorTests extends ESTestCase {
         assertThat(attachmentData.get("content_length"), is(notNullValue()));
     }
 
-    public void testEnglishTextDocumentWithRandomFields() throws Exception {
-        Set<AttachmentProcessor.Field> fields = EnumSet.noneOf(AttachmentProcessor.Field.class);
-        List<String> fieldNames = new ArrayList<>();
-        int numFields = scaledRandomIntBetween(1, AttachmentProcessor.Field.values().length);
+    public void testHtmlDocumentWithRandomFields() throws Exception {
+        //date is not present in the html doc
+        ArrayList<AttachmentProcessor.Field> fieldsList = new ArrayList<>(EnumSet.complementOf(EnumSet.of
+            (AttachmentProcessor.Field.DATE)));
+        Set<AttachmentProcessor.Field> selectedFields = new HashSet<>();
+
+        int numFields = randomIntBetween(1, fieldsList.size());
+        String[] selectedFieldNames = new String[numFields];
         for (int i = 0; i < numFields; i++) {
-            AttachmentProcessor.Field field = AttachmentProcessor.Field.values()[i];
-            fields.add(field);
-            fieldNames.add(field.name().toLowerCase(Locale.ROOT));
-        }
+            AttachmentProcessor.Field field;
+            do {
+                field = randomFrom(fieldsList);
+            } while (selectedFields.add(field) == false);
 
+            selectedFieldNames[i] = field.toLowerCase();
+        }
+        if (randomBoolean()) {
+            selectedFields.add(AttachmentProcessor.Field.DATE);
+        }
         processor = new AttachmentProcessor(randomAsciiOfLength(10), "source_field",
-            "target_field", EnumSet.copyOf(fields), 10000);
+            "target_field", selectedFields, 10000);
 
-        Map<String, Object> attachmentData = parseDocument("text-in-english.txt", processor);
-        assertThat(attachmentData.keySet(), hasSize(1));
-        assertThat(attachmentData.keySet(), contains("content"));
+        Map<String, Object> attachmentData = parseDocument("htmlWithEmptyDateMeta.html", processor);
+        assertThat(attachmentData.keySet(), hasSize(selectedFieldNames.length));
+        assertThat(attachmentData.keySet(), containsInAnyOrder(selectedFieldNames));
     }
 
     public void testFrenchTextDocument() throws Exception {
diff --git a/test/framework/src/main/java/org/elasticsearch/search/MockSearchService.java b/test/framework/src/main/java/org/elasticsearch/search/MockSearchService.java
index 06d4a42..e9d78e4 100644
--- a/test/framework/src/main/java/org/elasticsearch/search/MockSearchService.java
+++ b/test/framework/src/main/java/org/elasticsearch/search/MockSearchService.java
@@ -26,10 +26,8 @@ import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.indices.IndicesService;
-import org.elasticsearch.indices.cache.request.IndicesRequestCache;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.aggregations.AggregatorParsers;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.SearchContext;
@@ -67,12 +65,11 @@ public class MockSearchService extends SearchService {
     }
 
     @Inject
-    public MockSearchService(Settings settings, ClusterSettings clusterSettings, ClusterService clusterService,
-            IndicesService indicesService, ThreadPool threadPool, ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
-            BigArrays bigArrays, DfsPhase dfsPhase, QueryPhase queryPhase, FetchPhase fetchPhase, IndicesRequestCache indicesQueryCache,
-            AggregatorParsers aggParsers) {
+    public MockSearchService(Settings settings, ClusterSettings clusterSettings, ClusterService clusterService, IndicesService indicesService,
+                             ThreadPool threadPool, ScriptService scriptService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
+                             DfsPhase dfsPhase, QueryPhase queryPhase, FetchPhase fetchPhase) {
         super(settings, clusterSettings, clusterService, indicesService, threadPool, scriptService, pageCacheRecycler, bigArrays, dfsPhase,
-                queryPhase, fetchPhase, indicesQueryCache, aggParsers);
+                queryPhase, fetchPhase);
     }
 
     @Override
diff --git a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
index 2c8a4c7..303203c 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
@@ -18,12 +18,6 @@
  */
 package org.elasticsearch.test;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.Query;
@@ -49,7 +43,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -547,10 +540,6 @@ public class TestSearchContext extends SearchContext {
         return null;
     }
 
-    @Override
-    public FetchPhase fetchPhase() {
-        return null;
-    }
 
     @Override
     public MappedFieldType smartNameFieldType(String name) {
